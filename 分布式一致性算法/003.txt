对上层应用程序的侵入性更小

对于应用程序,尤其是上层的业务系统来说,在系统开发初期,开发人员并没有为系统的高可用性做好充分的考虑。事实上,绝大部分的系统一开始都是从一个只需要支撑较小的负载,并且只需要保证大体可用的原型开始的,往往并没有在代码层面为分布式一致性协议的实现留有余地。当系统提供的服务日趋成熟,并且得到一定规模的用户认可之后,系统的可用性就会变得越来越重要了于是,集群中副本复制和Master选举等一系列提高分布式系统可用性的措施,就会被加入到一个已有的系统中去。

在这种情况下,尽管这些措施都可以通过一个封装了分布式一致性协议的客户端库来完成,但相比之下,使用一个分布式锁服务的接口方式对上层应用程序的侵入性会更小,并且更易于保持系统已有的程序结构和网络通信模式。


便于提供数据的发布与订阅

几乎在所有使用Chubby进行Master选举的应用场景中,都需要一种广播结果的机制,用来向所有的客户端公布当前的Master服务器,这就意味着Chubby应该允许其客户端在服务器上进行少量数据的存储与读取一也就是对小文件的读写操作虽然这个特性也能够通过一个分布式命名服务来实现,但是根据实际的经验来看,分布式锁服务本身也非常适合提供这个功能,这一方面能够大大减少客户端依赖的外部服务,另一方面,数据的发布与订阅功能和锁服务在分布式一致性特性上是相通的。

开发人员对基于锁的接口更为熟悉

对于绝大部分的开发人员来说,在平常的编程过程中,他们对基于锁的接口都已经非常熟悉了,因此,Chubby为其提供了一套近乎和单机锁机制一致的分布式锁服务接口,这远比提供一个一致性协议的库来得更为友好。

更便捷地构建更可靠的服务

通常一个分布式一致性算法都需要使用Quorum机制来进行数据项值的选定Quorum机制是分布式系统中实现数据一致性的一个比较特殊的策略,它指的是在一个由若干个机器组成的集群中,在一个数据项值的选定过程中,要求集群中存在过半的机器达成一致,因此Quorum机制也被称作“过半机制在Chubby中通常使用5台服务器来组成一个集群单元(cell),根据Quorum机制,只要整个集群中有3台服务器是正常运行的,那么整个集群就可以对外提供正常的服务。相反的，如果仅提供一个分布式一致性协议的客户端库,那么这些高可用性的系统部署都将交给开发人员自己来处理,这无疑提高了成本。

因此,Chubby被设计成一个需要访向中心化节点的分布式锁服务。同时,在Chubby的设计过程中,提出了以下几个设计目标。

提供一个完整的、独立的分布式锁服务,而非仅仅是一个一致性协议的客户端库

在上面的内容中我们已经讲到,提供一个独立的锁服务的最大好处在于,Chubby对于使用它的应用程序的侵入性非常低,应用程序不需要修改已有程序的拮构即可使用分布式一致性特性。例如,对于“Master选举同时将Master信息登记井广播的场景,应用程序只需要向Chubby请求一个镇,井且在获得锁之后向相应的锁文件写入Master信息即可,其余的客户端就可以通过读取这个锁文件来获取Master信息。

提供粗粒度的镇服务

Chubby镇服务针对的应用场景是客户端获得镇之后会进行长时间持有(数小时或数天),而非用于短暂获取锁的场景。针对这种应用场景,当锁服务短暂失效时(例如服务器宕机),Chubby需要保持所有锁的持有状态,以避免持有锁的客户端出现问题。这和细粒度锁的设计方式有很大的区别,细粒度锁通常设计为锁服务一旦失效就释放所有锁,因为细粒度锁的持有时间很短,相比而言放弃锁带来的代价较小。

在提供锁服务的同时提供对小文件的读写功能

Chubby提供对小文件的读写服务,以使得被选举出来的Master可以在不依赖额外服务的情况下,非常方便地向所有客户端发布自己的状态信息。具体的,当一个客户端成功获取到一个Chubby文件锁而成为Master之后,就可以继续向这个文件里写入Master信息,其他客户端就可以通过读取这个文件得知当前的Master信息。

高可用、高可靠

在Chubby的架构设计中,允许运维人员通过部署多台机器(一般是5台机器)来组成一个Chubby集群,从而保证集群的高可用。基于对Paxos算法的实现,对于一个由5台机器组成的Chubby集群来说,只要保证存在3台正常运行的机器,整个集群对外服务就能保持可用。

另外,由于Chubby支持通过小文件读写服务的方式来进行Master选举结果的发布与订阅,因此在Chubby的实际应用过程中,必须能够支撑成百上千个Chubby客户端对同一个文件进行监视和读取。

提供事件通知机制

在实际使用过程中,Chubby客户端需要实时地感知到Master的变化情况,当然这可以通过让客户端反复的轮询来实现,但是在客户端规模不断增大的情况下,客户端主动轮询的实时性效果并不理想,且对服务器性能和网络带宽压力都非常大。因此,Chubby需要有能力将服务端的数据变化情况(例如文件内容变更)以事件的形式通知到所有订阅的客户端。

3.1.4Chubby技术架构
接下来我们一起看看，Chubby是如何来实现一个高可用的分布式锁服务的。

系统结构
Chubby的整个系统结构主要由服务端和客户端两部分组成，客户端通过RPC调用与服务端进行通信，如图3-l所示。

图3-1.Chubby服务端与客户端结构示意图

一个典型的Chubby集群,或称为Chubbycell通常由5台服务器组成,这些副本服务器采用Paxos协议,通过投票的方式来选举产生一个获得过半投票的服务器作为Master。一旦某台服务器成为了Master,Chubby就会保证在一段时期内不会再有其他服务器成为Master——这段时期被称为Master租期(Masterlease)。在运行过程中,Master服务器会通过不断续租的方式来延长Master租期,而如果Master服务器出现故障,那么余下的服务器就会进行新一轮的Master选举,最终产生新的Master服务器,开始新的Master租期


集群中的每个服务器都维护着一份服务端数据库的副本,但在实际运行过程中,只有Master服务器才能对数据库进行写操作。而其他服务器都是使用Paxos协议从Master服务器上同步数据库数据的更新。

现在，我们再来看下Chubby的客户端是如何定位到Master服务器的。Chubby客户端通过向记录有Chubby服务端机器列表的DNS来请求获取所有的Chubby服务器列表，然后透个发起请求询问该服务器是否是Master.在这个询问过程中，那些非Master的服务器，则会将当前Master所在的服务器标识反馈给客户端，这样客户端就能够非常快速地定位到Master服务器了。

一旦客户端定位到Master服务器之后，只要该Master正常运行，那么客户端就会将所有的请求都发送到该Master服务器上。针对写请求，ChubbyMaster会采用一致性协议将其广播给集群中所有的副本服务器，并且在过半的服务器接受了该写请求之后，再响应给客户端正确的应答。而对于读请求，则不需要在集群内部进行广播处理，直接由Master服务器单独处理即可。

在Chubby运行过程中，服务器难免会发生故障。如果当前的Master服务器崩清了，那么集群中的其他服务器会在Master租期到期后，重新开启新一轮的Master选举。通常，进行一次Master选举大概需要花费几秒钟的时间。而如果是集群中任意一台非Master服务器崩溃，那么整个集群是不会停止工作的，这个崩溃的服务器会在恢复之后自动加入到Chubby集群中去。新加入的服务器首先需要同步Chubby最新的数据库数据，完成数据同步之后，新的服务器就可以加入到正常的Paxos运作流程中与其他服务器副本一起协同工作。

如果集群中的一个服务器发生崩溃并在几小时后仍无法恢复正常，那么就需要加入新的机器，并同时更新DNS列表。Chubby服务器的更换方式非常简单，只需要启动Chubby服务端程序，然后更新DNS上的机器列表（即使用新机器的IP地址替换老机器的IP地址）即可。在Chubby运行过程中，Master服务器会周期性地轮询DNS列表，因此其很快就会感知到服务器地址列表的变更，然后Master就会将集群数据库中的地址列表做相应的变更，集群内部的其他副本服务器通过复制方式就可以获取到最新的服务器地址列表了。

目录与文件

Chubby对外提供了一套与Unix文件系统非常相近但是更简单的访问接口。Chubby的数据结构可以看作是一个由文件和目录组成的树，其中每一个节点都可以表示为一个使用斜杠分割的字符串，典型的节点路径表示如下：

/1s/foo/wombat/pouch

其中,s是所有Chubby节点所共有的前缀,代表着锁服务,是LockService的缩写;foo则指定了Chubby集群的名字,从DNS可以查询到由一个或多个服务器组成该Chubby集群,剩余部分的路径/wombat/pouch则是一个真正包含业务含义的节点名字,由Chubby服务器内部解析并定位到数据节点。

Chubby的命名空间,包括文件和目录,我们称之为节点(nodes,在本书后面的内容中,我们以数据节点来泛指Chubby的文件或目录)在同一个Chubby集群数据库中,每一个节点都是全局唯一的,和Unix系统一样,每个目录都可以包含一系列的子文件和子目录列表,而每个文件中则会包含文件内容,当然,Chubby井非模拟一个完整的文件系统,因此没有符号链接和硬连接的概念。

由于Chubby的命名结构组成了一个近似标准文件系统的视图,因此Chubby的客户端应用程序也可以通过自定义的文件系统访问接口来访问Chubby服务端数据,比如可以使用GFS的文件系统访问接口,这就大大减少了用户使用Chubby的成本。

Chubby上的每个数据节点都分为持久节点和临时节点两大类,其中持久节点需要显式地调用接口AP来进行删除,而临时节点则会在其对应的客户端会话失效后被自动删除。也就是说,临时节点的生命周期和客户端会话绑定,如果该临时节点对应的文件没有被任何客户端打开的话,那么它就会被删除掉。因此,临时节点通常可以用来进行客户端会话有效性的判断依据。

另外,Chubby上的每个数据节点都包含了少量的元数据信息,其中包括用于权限控制的访控制列表(ACL)信息。同时,每个节点的元数据中还包括4个单调递增的64位编号,分别如下。

实例编号：实例编号用于标识Chubby创建该数据节点的顺序，节点的创建顺序不同.其实例编号也不同，因此，通过实例编号，即使针对两个名字相同的数据节点，客户端也能够非常方便地识别出是否是同一个数据节点——因为创建时间晚的数据节点，其实例编号必定大于任意先前创建的同名节点。

文件内容编号（只针对文件）：文件内容编号用于标识文件内容的变化情况，该编号会在文件内容被写入时增加。

锁编号：锁编号用于标识节点销状态变更情况，该编号会在节点锁从自由（free)状态转换到被持有（held)状态时增加。

ACL编号：ACL编号用于标识节点的ACL信息变更情况，该编号会在节点的ACL配置信息被写入时增加。

同时,Chubby还会标识一个64位的文件内容校验码,以便客户端能够识别出文件是否变更。

锁与锁序列器

在分布式系统中,锁是一个非常复杂的问题,由于网络通信的不确定性,导致在分布式系统中锁机制变得非常复杂,消息的延迟或是乱序都有可能会引起锁的失效。一个典型的分布式锁错乱案例是,一个客户端C获取到了互斥锁L,并且在锁L的保护下发出请求R,但请求R迟迟没有到达服务端(可能出现网络延时或反复重发等),这时应用程序会认为该客户端进程已经失败,于是便会为另一个客户端C2分配锁L,然后再重新发起之前的请求R,并成功地应用到了服务器上。此时,不幸的事情发生了,客户端C发起的请求R在经过一波三折之后也到达了服务端,此时,它有可能会在不受任何锁控制的情况下被服务端处理,从而覆盖了客户端C2的操作,于是导致系统数据出现不一致。当然,诸如此类消息接收顺序紊乱引起的数据不一致问题已经在人们对分布式计算的长期研究过程中得到了很好的解决,典型的解决方案包括虚拟时间和虚拟同步。这两个分布式系统中典型的解决方案并不是本书的重点,感兴趣的读者可以在互联网上了解更多相关的参考资料。

在Chubby中,任意一个数据节点都可以充当一个读写锁来使用:一种是单个客户端以排他(写)模式持有这个锁,另一种则是任意数目的客户端以共享(读)模式持有这个锁。同时,在Chubby的锁机制中需要注意的一点是,Chubby舍弃了严格的强制锁,客户端可以在没有获取任何锁的情况下访问Chubby的文件,也就是说,持有锁F既不是访问文件F的必要条件,也不会阻止其他客户端访问文件F。

在Chubby中,主要采用锁延迟和锁序列器两种策略来解决上面我们提到的由于消息延迟和重排序引起的分布式锁问题。其中延迟是一种比较简单的策略,使用Chubby的应用几乎不需要进行任何的代码修改具体的,如果一个客户端以正常的方式主动释放了一个锁,那么Chubby服务端将会允许其他客户端能够立即获取到该锁。而如果一个锁是因为客户端的异常情况(如客户端无响应)而被释放的话,那么Chubby服务器会为该锁保留一定的时间,我们称之为“锁延迟(lock--delay),在这段时间内,其他客户端无法获取这个锁锁延迟措施能够很好地防止一些客户端由于网络闪断等原因而与服务器暂时断开的场景出现总的来说,该方案尽管不完美,但是锁延时能够有效地保护在出现消息延时情况下发生的数据不一致现象。

Chubby提供的另一种方式是使用锁序列器,当然该策略需要Chubby的上层应用配合在代码中加入相应的修改逻辑。任何时候,锁的持有者都可以向Chubby请求一个锁序列器,其包括锁的名字、锁模式(排他或共享模式),以及锁序号。当客户端应用程序在进行一些需要锁机制保护的操作时,可以将该锁序列器一并发送给服务端。Chubby服务端接收到这样的请求后,会首先检测该序列器是否有效,以及检查客户端是否处于恰当的锁模式;如果没有通过检查,那么服务端就会拒绝该客户端请求。

Chubby中的事件通知机制

为了避免大量客户端轮询Chubby服务端状态所带来的压力,Chubby提供了事件通知机制。Chubby的客户端可以向服务端注册事件通知,当触发这些事件的时候,服务端就会向客户端发送对应的事件通知。在Chubby的事件通知机制中,消息通知都是通过异步的方式发送给客户端的,常见的Chubby事件如下。

文件内容变更
例如,BigTable集群使用Chubby锁来确定集群中的哪台BigTable机器是Master获得锁的BigTableMaster会将自身信息写入Chubby上对应的文件中BigTable集群中的其他客户端可以通过监视这个Chubby文件的变化来确定新的BigTableMaster机器。

节点删除
当Chubby上指定节点被删除的时候,会产生“节点删除”事件,这通常在临时节点中比较常见,可以利用该特性来间接判断该临时节点对应的客户端会话是否有效。

子节点新增、删除
当Chubby上指定节点的子节点新增或是减少时,会产生“子节点新增、删除”事件。

Master服务器转移
当Chubby服务器发生Master转移时,会以事件的形式通知客户端。

Chubby中的缓存

为了提高Chubby的性能,同时也是为了减少客户端和服务端之间频紫的读请求对服务端的压力,Chubby除了提供事件通知机制之外,还在客户端中实现了级存,会在客户端对文件内容和元数据信息进行缓存。使用缓存机制在提高系统整体性能的同时,也为系统带来了一定的复杂性,其中最主要的问题就是应该如何保证缓存的一致性。在Chubby中,通过租期机制来保证缓存的一致性。

Chubby缓存的生命周期和Master租期机制紧密相关,Master会维护每个客户端的数据缓存情况,并通过向客户端发送过期信息的方式来保证客户端数据的一致性。在这种机制下,Chubby就能够保证客户端要么能够从缓存中访问到一致的数据,要么访问出错,而一定不会访问到不一致的数据。具体的,每个客户端的缓存都有一个租期,一旦该租期到期,客户端就需要向服务端续订租期以继续维持缓存的有效性。当文件数据或元数据信息被修改时,Chubby服务端首先会阻塞该修改操作,然后由Master向所有可能缓存了该数据的客户端发送缓存过期信号,以使其缓存失效,等到Master在接收到所有相关客户端针对该过期信号的应答(应答包括两类,一类是客户端明确要求更新缓存,另一类则是客户端允许缓存租期过期)后,再继续进行之前的修改操作。

通过上面这个缓存机制的介绍,相信读者都已经明白了,Chubby的缓存数据保证了强一致性尽管要保证严格的数据一致性对于性能的开销和系统的吞吐影响很大,但由于弱一致性模型在实际使用过程中极容易出现问题,因此Chubby在设计之初就决定了选择强一致性模型。