<==========================200start==============================>

lexclusive_lock
lock
图6-14.排他锁的 ZooKeepr节点示意图
获取锁
在需要获取排他锁时,所有的客户端都会试图通过调用 create()接口,在
exclusivelock节点下创建临时子节点/ exclusive lock/lock在前面几节中我们也介绍了,
ZooKeeper会保证在所有的客户端中,最终只有一个客户端能够创建成功,那么就可以
认为该客户端获取了锁。同时,所有没有获取到锁的客户端就需要到/exclusivelock节
点上注册一个子节点变更的 Watcher监听,以便实时监听到lock节点的变更情况。
释放锁
在“定义锁”部分,我们已经提到,exclusive locklock是一个临时节点,因此在以下
两种情况下,都有可能释放锁。
·当前获取锁的客户端机器发生机,那么 ZooKeeper上的这个临时节点就会被移除。
正常执行完业务逻辑后,客户端就会主动将自己创建的临时节点删除。
无论在什么情况下移除了lock节点, ZooKeeper都会通知所有在/exclusivelock节点上注
册了子节点变更 Watcher监听的客户端。这些客户端在接收到通知后,再次重新发起分布
式锁获取,即重复“获取锁”过程。整个排他锁的获取和释放流程,可以用图6-15来表示。
是否已经其他事
等锁
ock
时节点
是否创建成功
占用锁
完成事务逻
释锁
事务中断
图6-15.排他锁的流程图
6.1典型应用场景及实现注189

<==========================200end ==============================>
<==========================201start==============================>

共享锁
共享锁(Shared Locks,简称S锁),又称为读锁,同样是一种基本的锁类型。如果事务
T1对数据对象O1加上了共享锁,那么当前事务只能对O进行读取操作,其他事务也只
能对这个数据对象加共享锁直到该数据对象上的所有共享锁都被释放。
共享锁和排他锁最根本的区别在于,加上排他锁后,数据对象只对一个事务可见,而加
上共享锁后,数据对所有事务都可见。下面我们就来看看如何借助 ZooKeeper来实现共
享锁。
定义锁
和排他锁一样,同样是通过 ZooKeeper上的数据节点来表示一个锁,是一个类似于
“/shared lock/ Hostname]-请求类型一序号”的临时顺序节点,例如sharedlock
192.168.0.-0000,那么,这个节点就代表了一个共享锁,如图6-16所示。
/shared_lock
001
host-r000005
图6-16.共享锁的 ZooKeeper节点示意图
获取锁
在需要获取共享锁时,所有客户端都会到/sharedlock这个节点下面创建一个临时顺序
节点,如果当前是读请求,那么就创建例如sharedlock/192.168.0.1-00000节
点;如果是写请求,那么就创建例如/sharedlock/192.168.-000节点
判断读写顺序
根据共享锁的定义,不同的事务都可以同时对同一个数据对象进行读取操作,而更新操
作必须在当前没有任何事务进行读写操作的情况下进行。基于这个原则,我们来看看如
何通过 ZooKeeper的节点来确定分布式读写顺序,大致可以分为如下4个步骤。
1.创建完节点后,获取shared lock节点下的所有子节点,并对该节点注册子节点变
更的 Watcher监听。
190第6章 ZooKeeper的典型应用场景

<==========================201end ==============================>
<==========================202start==============================>

2.确定自己的节点序号在所有子节点中的顺序。
3.对于读请求:
如果没有比自己序号小的子节点,或是所有比自己序号小的子节点都是读请求,
那么表明自己已经成功获取到了共享锁,同时开始执行读取逻辑。
如果比自己序号小的子节点中有写请求,那么就需要进入等待。
对于写请求:
如果自己不是序号最小的子节点,那么就需要进入等待。
4.接收到 Watcher通知后,重复步骤1
释放锁
释放锁的逻辑和排他锁是一致的,这里不再赘述。整个共享锁的获取和释放流程,可以
用图6-17来表示。
创建
获取锁
sharedJockhost1-1-求类型序号
节点
同时注册 Watcher
创建ock
临时节点
等特 Watcher通知
获取sharedlock
子节点列表
是否是写求
是否获取
共摩锁
号是否最小等锁
占用锁
Watcher
没有比自己序号小的子节点1完成读写操作
或者:
2.新有比自己序号小的子节点
2.事务中断
都是读请求
释放锁
图6-17.共享锁的流程图
6.1典型应用场景及实现注191

<==========================202end ==============================>
<==========================203start==============================>

羊群效应
上面讲解的这个共享锁实现,大体上能够满足一般的分布式集群竞争锁的需求,并且性
能都还可以——这里说的一般场景是指集群规模不是特别大,一般是在10台机器以内。
但是如果机器规模扩大之后,会有什么问题呢?我们着重来看上面“判断读写顺序”过
程的步骤3,结合图6-18给出的实例,看看实际运行中的情况。
shared tock
1921680.1-r-000
---
192160-000
--------
192168058000
-----
1921606-00
192167007
图6-18.共享锁的实例
针对图6-18中的实际情况,我们看看会发生什么事情。
1.192.168.0.1这台机器首先进行读操作,完成读操作后将节点/192.168.0.1-R-
000000删除。
2.余下的4台机器均收到了这个节点被移除的通知,然后重新从/sharedlock节点上
获取一份新的子节点列表。
3.每个机器判断自己的读写顺序。其中192.168.0.2这台机器检测到自己已经是序号
最小的机器了,于是开始进行写操作,而余下的其他机器发现没有轮到自己进行
读取或更新操作,于是继续等待。
4.继续
上面这个过程就是共享锁在实际运行中最主要的步骤了,我们着重看下上面步骤3中提
到的:“而余下的其他机器发现没有轮到自己进行读取或更新操作,于是继续等待。”很
明显,我们看到,192.168.0.1这个客户端在移除自己的共享锁后, ZooKeeper发送了子
节点变更 Watcher通知给所有机器,然而这个通知除了给192.168.0.2这台机器产生实际
影响外,对于余下的其他所有机器都没有任何作用。
相信读者也已经意识到了,在这整个分布式锁的竞争过程中,大量的“Watcher通知”
192第6章 ZooKeeper的典型应用场景

<==========================203end ==============================>
<==========================204start==============================>

和“子节点列表获取”两个操作重复运行,并且绝大多数的运行结果都是判断出自己并
非是序号最小的节点,从而继续等待下一次通知这个看起来显然不怎么科学。客户
端无端地接收到过多和自己并不相关的事件通知,如果在集群规模比较大的情况下,不
仅会对 ZooKeeper服务器造成巨大的性能影响和网络冲击,更为严重的是,如果同一时
间有多个节点对应的客户端完成事务或是事务中断引起节点消失, ZooKeeper服务器就
会在短时间内向其余客户端发送大量的事件通知这就是所谓的羊群效应。
上面这个 ZooKeeper分布式共享锁实现中出现羊群效应的根源在于,没有找准客户端真
正的关注点。我们再来回顾一下上面的分布式锁竞争过程,它的核心逻辑在于:判断自
己是否是所有子节点中序号最小的。于是很容易可以联想到,每个节点对应的客户端
只需要关注比自己序号小的那个相关节点的变更情况就可以了而不需要关注全局
的子列表变更情况。
改进后的分布式锁实现
现在我们来看看如何改进上面的分布式锁实现。首先,我们需要肯定的一点是,上面提
到的共享锁实现,从整体思路上来说完全正确。这里主要的改动在于:每个锁竞争者,
只需要关注/sharedlock节点下序号比自己小的那个节点是否存在即可,具体实现如下。
1.客户端调用 create()方法创建一个类似于/sharedlock/[Hostname]-请求类型
序号”的临时顺序节点。
2.客户端调用 getChildren()接口来获取所有已经创建的子节点列表,注意,这
里不注册任何 Watcher
3.如果无法获取共享锁,那么就调用 exist)来对比自己小的那个节点注册 Watcher
注意,这里“比自己小的节点”只是一个笼统的说法,具体对于读请求和写请求不
一样。
读请求:向比自己序号小的最后一个写请求节点注册 Watcher监听
写请求:向比自己序号小的最后一个节点注册 Watcher监听。
4.等待 Watcher通知,继续进入步骤2
改进后的分布式锁流程如图6-19所示。
6.1典型应用场景及实现注193

<==========================204end ==============================>
<==========================205start==============================>

取锁
创建
sharedlockhost-求类型号
临时节点
hara tr
等待 Watcher通知
略 shared
子节点列表
否是写请求
是否获
善待锁
共享锁
号是否最小
对比自己小的节
注册 Watcher
占用锁
有比自己湾号小的子节点
或者
2.所有比自己号小的子节点
1.完成读写操作
是读请求
2事务中断
watcher
释放锁
图6-19.改进后的共享锁流程图
注意
看到这里,相信很多读者都会觉得改进后的分布式锁实现相对来说比较麻烦。确实如此,
如同在多线程并发编程实践中,我们会去尽量缩小锁的范围对于分布式锁实现的改
进其实也是同样的思路。那么对于开发人员来说,是否必须按照改进后的思路来设计实
现自己的分布式锁呢?答案是否定的。在具体的实际开发过程中,我们提倡根据具体的
业务场景和集群规模来选择适合自己的分布式锁实现:在集群规模不大、网络资源丰富
的情况下,第一种分布式锁实现方式是简单实用的选择;而如果集群规模达到一定程度,
并且希望能够精细化地控制分布式锁机制,那么不妨试试改进版的分布式锁实现。
6.1.8分布式队列
业界有不少分布式队列产品,不过绝大多数都是类似于 ActiveMQ、 Metamorphosis, Kafka
和 HornetQ等的消息中间件(或称为消息队列)在本节中,我们主要介绍基于 ZooKeeper
实现的分布式队列。分布式队列,简单地讲分为两大类,一种是常规的先入先出队列,
另一种则是要等到队列元素集聚之后才统一安排执行的 Barrier模型。
194第6章 ZooKeeper的典型应用场景

<==========================205end ==============================>
<==========================206start==============================>

FF:先入先出
FFO(First Input First Output,先入先出)的算法思想,以其简单明了的特点,广泛应
用于计算机科学的各个方面。而FIFO队列也是一种非常典型且应用广泛的按序执行的
队列模型:先进入队列的请求操作先完成后,才会开始处理后面的请求。
使用 ZooKeeper实现FIFO队列,和6.1.7节中提到的共享锁的实现非常类似。fFO队
列就类似于一个全写的共享锁模型,大体的设计思路其实非常简单:所有客户端都会到
queue fifo这个节点下面创建一个临时顺序节点,例如queue fifo192.168.0
1000000,如图6-20所示。
Iqueue_
图6-20.fifo的 ZooKeeper节点示意图
创建完节点之后,根据如下4个步骤来确定执行顺序。
1.通过调用getChildren()接口来获取/ queue_fo节点下的所有子节点,即获取
队列中所有的元素。
2.确定自己的节点序号在所有子节点中的顺序。
3.如果自己不是序号最小的子节点,那么就需要进入等待,同时向比自己序号小的
最后一个节点注册 Watcher监听。
4.接收到 Watcher通知后,重复步骤1。
整个FIFO队列的工作流程,可以用图6-21来表示。
6.1典型应用场景及实现注195

<==========================206end ==============================>
<==========================207start==============================>

cueue aifo host11-号
创
节点
临时节点
等待 Watcher知
获queueffo
子节点列
号是否小
对比自己小的节
点注册 Watcher
待锁
N
Watcher
成作
占用锁
锁
图6-21.FF的流程图
Barrier:分布式屏障
Barrier原意是指障碍物、屏障,而在分布式系统中,特指系统之间的一个协调条件,规
定了一个队列的元素必须都集聚后才能统一进行安排,否则一直等待。这往往出现在那
些大规模分布式并行计算的应用场景上:最终的合并计算需要基于很多并行计算的子结
果来进行。这些队列其实是在FIFO队列的基础上进行了增强,大致的设计思想如下:
开始时,/ Iqueue barrier节点是一个已经存在的默认节点,并且将其节点的数据内容赋
值为一个数字n来代表 Barrier值,例如n=0表示只有当queue barrier节点下的子节点
个数达到10后,才会打开 Barrier。之后,所有的客户端都会到queue barrier节点下创
建一个临时节点,例如queuebarrier/192.68.0.1,如图6-22所示
lqueue_barier
host1
host2
host5
图6-22. Barrier的 ZooKeepr节点示意图
创建完节点之后,根据如下5个步骤来确定执行顺序。
1.通过调用 getData()接口获取/ queue barrier节点的数据内容:10
2.通过调用get Children()接口获取/queue barrier节点下的所有子节点,即获取
队列中的所有元素,同时注册对子节点列表变更的 Watcher监听。
196第6章 ZooKeeper的典型应用场景

<==========================207end ==============================>
<==========================208start==============================>

3.统计子节点的个数。
4.如果子节点个数还不足10个,那么就需要进入等待。
5.接收到 Watcher通知后,重复步骤2
整个 Barrier队列的工作流程,可以用图6-23来表示。
创建
创建
queue barrierhost1节点
临时节点
等待 Watcher通知
同时注册 Watcher
获取/ queue fifo
子节点列表
等待锁
子节点个数
N是否集齐
业务处理
图6-23. Barrier的流程图
小结
本节通过对数据发布/订阅、负载均衡、命名服务和分布式协调通知等一系列典型应用
场景的展开讲解,向读者初步展示了 ZooKeeper在解决分布式问题上的强大作用。基于
ZooKeeper对分布式数据一致性的保证及其提供的一系列分布式特性,开发人员能够构
建出自己的分布式系统。
从理论上了解了 ZooKeeper的典型应用场景之后,在下一节中,我们将结合Hadoop、
HBase和 Kafka等广泛使用的开源系统,来讲解 ZooKeeper在大型分布式系统中的实际
应用。
6.6J2《ZooKeeper在大型分布式系统中的应用
在6.1节中,我们已经从理论上详细地讲解了 ZooKeeper的典型应用场景及其实现细节。
而在实际工业实践中,由于 ZooKeeper便捷的使用方式、卓越的运行性能以及良好的稳
6.2 ZooKeeper在大型分布式系统中的应用197

<==========================208end ==============================>
<==========================209start==============================>

定性,已经被广泛地应用在越来越多的大型分布式系统中,用来解决诸如配置管理、分
布式通知/协调、集群管理和 Master选举等一系列分布式问题,其中最著名的就是 Hadoop、
HBase和 Kafka等开源系统。在本节中,我们将围绕这些大型分布式系统的技术原理,
向读者介绍 ZooKeeper在其中的应用场景和具体的实现方式,帮助读者更好地理解
ZooKeeper的分布式应用场景。
6.2.1 Hadoop
Hadoop是 Apache开源的一个大型分布式计算框架,由 Lucene创始人 Doug Cutting牵
头创建,其定义了一种能够开发和运行处理海量数据的软件规范,用来实现一个在大规
模集群中对海量数据进行分布式计算的软件平台 Hadoop的核心是hFS和 MapReduce,
分别提供了对海量数据的存储和计算能力,自0.23.0版本开始, Hadoop又引入了全新
一代 MapReduce框架YARN。
在海量数据存储及处理领域, Hadoop是目前业界公认的最成熟也是最卓越的开源解决
方案。本书不会去过多地介绍Hadoop技术本身,感兴趣的读者可以访问Hadoop的官
方网站3了解更多关于这一分布式计算框架的内容。本书主要讨论 ZooKeeper在 Hadoop
中的使用场景。
在 Hadoop中, ZooKeeper主要用于实现HA(High Availability),这部分逻辑主要集中
在 Hadoop Common的A模块中,hdF的 NameNode与yar的 ResourceManager
都是基于此HA模块来实现自己的HA功能的。同时,在YARN中又特别提供了
ZooKeeper来存储应用的运行状态。本书将以 Cloudera的5.0发布版本为例,围绕yarN
中 ZooKeeper的使用场景来讲解。
YARN介绍
YARN是 Hadoop为了提高计算节点 Master(t)的扩展性,同时为了支持多计算模型和
提供资源的细粒度调度而引入的全新一代分布式调度框架。其上可以支持 MapReduce
计算引擎,也支持其他的一些计算引擎,如 Tez Spark Storm、 Imlala和 Open MPI等
其架构体系如图6-24所示。
注3: 3: hadoop http: /hadoop.apache.org/官方网站:htp:adop.apache.g
198第6章 ZooKeeper的典型应用场景

<==========================209end ==============================>
<==========================210start==============================>

Client
MapReduce Status
Job Submi
Resource R
图6-24.YARN架构体系4
从图6-24中可以看出,YARN主要由 ResourceManager(rm) NodeManager(nm)
ApplicationMaster(am)和 Container四部分组成其中最为核心的就是 ResourceManager
它作为全局的资源管理器,负责整个系统的资源管理和分配。关于YARN的更多介绍,
读者可以访问YARN的官方网站进行查阅。
ResourceManager单点问题
看完YARN的架构体系之后,相信细心的读者也已经看出了上述架构体系中存在的一个
明显的缺陷: ResourceManager的单点问题。 ResourceManager是YARN中非常复杂的一
个组件,负责集群中所有资源的统一管理和分配,同时接收来自各个节点( NodeManager)
的资源汇报信息,并把这些信息按照一定的策略分配给各个应用程序(Application
Manager),其内部维护了各个应用程序的 ApplictionMaster信息、 NodeManager信息以
及资源使用信息等。因此, ResourceManager的工作状况直接决定了整个YARN框架是
否可以正常运转。
ResourceManager HA
为了解决 ResourceManager的这个单点问题,YARN设计了一套 Active/Standby模式的
ResourceManager HA架构,如图6-25所示。
注4:本图来自YARN官方网站
注5:YARN官方网站:ittp: 5:  yarn http: //hadoop.apache.orgl/current/hadoop-yarn/hadoop-yarn-sil/YARN. html..aache.og/docs/curen/hadoop-yarn/hadoop-yar-site/yrn.html
6.2 ZooKeeper在大型分布式系统中的应用199

<==========================210end ==============================>
<==========================211start==============================>

Client
Client
Client
Client Failover
RM
RM
Elector
ZooKeeper
Elector
Active
Standby
Client Failover
NM NM
NM
图6-25.YARN的HA架构
从图6-25中可以看出,在运行期间,会有多个 ResourceManager并存,并且其中只有一
个 Resource Manager处于 Active状态,另外的一些(允许一个或者多个)则是处于 Standby
状态,当 Active节点无法正常工作(如机器挂掉或重启等)时,其余处于 Standby状态
的节点则会通过竞争选举产生新的 Active节点。
主备切换
下面我们就来看看YARN是如何实现多个 ResourceManager之间的主备切换的。
ResourceManager使用基于 ZooKeeper实现的 ActiveStandby Elector组件来确定 ResourceManager
的状态: Active Standby具体做法如下。
1.创建锁节点。
在 EZooKeeper上会有一个类于arn-edr- election/pseudo-clusteryarn-rm-的锁节点,
所有的 ResourceManager在启动的时候,都会去竞争写一个Lock子节点:
yar- -leader-election-/pseudo-cluster-yrn-m-ActiveStandbyElectorLock,同时需要注意的
是,该子节点的类型是临时节点。在前面章节的讲解中,我们已经明确了, ZooKeeper
能够为我们保证最终只有一个 ResourceManager能够创建成功创建成功的那个
ResourceManager就切换为 Active状态,没有成功的那些 ResourceManager则切换为
Standby状态。
2.注册 Watcher监听。
所有 Standby状态的 ResourceManager都会向arn- leader-election-/pseudo--yarn-rm-
cluster/ActiveStandby Elector Lock节点注册一个节点变更的 Watcher监听,利用临
时节点的特性,能够快速感知到 Active状态的 ResourceManager的运行情况。
200第6章 ZooKeeper的典型应用场景

<==========================211end ==============================>
<==========================212start==============================>

3.主备切换。
当 Active状态的 ResourceManager出现诸如重启或挂掉的异常情况时,其在
ZooKeeper上创建的Lock节点也会随之被删除此时其余各个 Standby状态的
ResourceManager都会接收到来自 ZooKeeper务端的 Watcher事件通知,然后会
重复进行步骤1的操作。
以上就是利用 ZooKeeper来实现 ResourceManager的主备切换的过程。 ActiveStandbyElector
组件位于 HadoopCommon-工程的orgapache. hadoop.ha包中,其封装了 ResourceManager和
ZooKeeper之间的通信与交互过程,图6-26中展示了 ActiveStandby Elector的概要类图
StatCallback
StringCallback Watcher
ActiveStandbyElector
WatcherWithClientRef
essResut(int re. String path, Obje
图6-26. ActiveStandby Elector概要类图
HDFS中的 NameNode和 ResourceManager模块都是使用该组件来实现各自的HA的,
感兴趣的读者可以结合其源代码做进一步的详细了解。
Fencing(隔离)
在分布式环境中,经常会出现诸如单机“假死的情况所谓的“假死”是指机器由于
网络闪断或是其自身由于负载过高(常见的有GC占用时间过长或CPU的负载过高等)
而导致无法正常地对外进行及时响应。在上述主备切换过程中,我们假设RM集群由
ResourceManagerl和 ResourceManager22两台机器组成,且 ResourceManagerl为 Active
状态, ResourceManager22为 Standby状态。某一时刻, ResourceManagerl发生了“假死”
现象,此时 ZooKeeper认为 ResourceManagerl挂了,根据上述主备切换逻辑,
ResourceManager22就会成为 Active状态。但是在随后, ResourceManagerl恢复了正常,
其依然认为自己还处于 Active状态。这就是我们常说的分布式“脑裂”( Brain-Split-)现
象,即存在了多个处于 Active状态的 ResourceManager各司其职。那么该如何解决这样
的问题呢?
6.62《ZooKeeper201《在大型分布式系统中的应用201

<==========================212end ==============================>
<==========================213start==============================>

YARN中引入了 Fencing机制,借助 ZooKeeper数据节点的ACL权限控制机制来实现不
同RM之间的隔离。具体做法其实非常简单,在上文的“主备切换”部分中我们讲到,
多个RM之间通过竞争创建锁节点来实现主备状态的确定。这个地方需要改进的一点是,
创建的根节点必须携带 ZooKeeper的ACL信息,目的是为了独占该根节点,以防止其
他RM对该节点进行更新。
经过上述改进后,我们再回过头来看,在主备切换过程中, Fencing机制是如何避免“脑
裂”现象出现的。延续上述提到的实例,R出现假死后, ZooKeeper就会将其创建
的锁节点移除掉,此时RM2会创建相应的锁节点,并切换为 Active状态RM1恢复之
后,会试图去更新 ZooKeeper的相关数据但是此时发现其没有权限更新 ZooKeeper的
相关节点数据,也就是说,RM1发现 ZooKeeper的相关节点不是自己创建的,于是就
自动切换为 Standby状态,这样就避免了“脑裂”现象的出现。
ResourceManager状态存储
在 ResourceManager中, RMStateStore能够存储一些RM的内部状态信息,包括
Application以及它们的 Attempts信息、 Delegation Token及 Version Information等需要
注意的是, RMStateStore中的绝大多数状态信息都是不需要持久化存储的,因为很容易
从上下文信息中将其重构出来,如资源的使用情况。在存储的设计方案中,提供了三种
可能的实现,分别如下。
基于内存实现,一般是用于日常开发测试。
·基于文件系统的实现,如HDFS
基于ZooKeeper的实现。
由于这些状态信息的数据量都不是特别大因此 Hadoop官方建议基于 ZooKeeper来实
现状态信息的存储。在 ZooKeeper上, ResourceMan的状态信息都被存储在/rmstore
这个根节点下面,其数据节点的组织结构如图6-27所示。
通过图6-27我们可以大致了解 RMStateStore状态信息在 ZooKeeper上的存储结构,其中
RMAppRoot节点下存储的是与各个 Application相关的信息, RMDTSecretManagerRoot存储
的是与安全相关的 Token等信息。每个 Active状态的 ResourceManager在初始化阶段都会从
ZooKeeper上读取到这些状态信息,并根据这些状态信息继续进行相应的处理。
202第 ZooKeeper6章的典型应用场景

<==========================213end ==============================>
<==========================214start==============================>

rmstore
ZKRMStateroot
RMAppRoot
IRMVersionNode
RMdtsecretManagerkoo
application_1401 application_1401
5242443780001524244378000
RMDTSequential RMDTMaster RMDelegation
Number
KeysRoot TokensRoot
(Delegationkey_(DelegationKey_
图6-27. RMStateStore的 ZooKeeper数据节点结构示意图
小结
ZooKeeperHadoop一开始是的子项目,因此很多设计之初的原始需求都是为了解决
HadoopHadoop系统中碰到的一系列分布式问题。虽然的架构几经变迁后,ZooKeeper
在 Hadoop的使用场景也有所变化,但其出色的分布式协调功能依然是 Hadoop解决单
点和状态信息存储的重要组件。
6.2.2 HBase
HBase,全称 Hadoop Database,是 Google Bigtable的开源实现,是一个基于 Hadoop文
件系统设计的面向海量数据的高可靠性、高性能、面向列、可伸缩的分布式存储系统,
利用 HBase技术可以在廉价的PC服务器上搭建起大规模结构化的存储集群。
与大部分分布式 NoSQL数据库不同的是, HBase针对数据写入具有强一致性的特性,
甚至包括索引列也都实现了强一致性,因此受到了很多互联网企业的青睐。根据公开报
道的数据, Facebook和阿里集团都分别拥有数千台的 HBase服务器,存储和使用了数以
PB计的在线数据。面对如此海量的数据以及如此大规模的服务器集群,如何更好地进
行分布式状态协调成为了整个 HBase系统正常运转的关键所在。
HBase在实现上严格遵守了 Google BigTable论文的设计思想。 BigTable使用 Chubby来
负责分布式状态的协调。在3.1节中我们已经讲解了 Chubby,这是 Google实现的一种
基于 Paxos算法的分布式锁服务,而 HBase则采用了开源的 ZooKeeper服务来完成对整
个系统的分布式协调工作。图6-28中展示了整个 HBase架构及其与 ZooKeeper之间的
结构关系。
6.2 ZooKeeper在大型分布式系统中的应用203

<==========================214end ==============================>
<==========================215start==============================>

DFS
00000
00000
000000
DataNode
图6-28. HBase整体架构示意图
从图6-28中可以看到,在 HBase的整个架构体系中, ZooKeeper是串联起 HBase集群
与 Client的关键所在。有趣的是,在200年以 HBase前的代码中,还看不到 ZooKeeper
的影子,因为当时HBase的定位是离线数据库。随着HBase逐步向在线分布式存储方
向发展,出现了一系列难以解决的问题,例如开发者发现如果有 RegionServer服务器挂
掉时,系统无法及时得知信息,客户端也无法知晓,因此服务难以快速迁移至其他
RegionServer服务器上类似问题都是因为缺少相应的分布式协调组件,于是后来
ZooKeeper被加入到 HBase的技术体系中直到今天, ZooKeeper依然是 HBase的核心
组件,而且 ZooKeeper在 HBase中的应用场景范围也己经得到了进一步的拓展。下面我
们从系统冗错、 RootRegion管理、 Region状态管理、分布式 SplitLog任务管理和
Replication管理五大方面来讲解 ZooKeeper在 HBase中的应用场景
系统冗错
当 HBase启动的时候,每个 RegionServerZ服务器都会到的/ hbase/rs节点下创
建一个信息节点(下文中,我们称该节点为“r状态节点”),例如/hbase/rs/[Hostname],
同时, HMaster会对这个节点注册监听。当某个 RegionServer挂掉的时候, ZooKeeper
会因为在一段时间内无法接收其心跳信息(即 Session失效),而删除掉该 RegionServer
服务器对应的rs状态节点。与此同时, HMaster则会接收到 ZooKeeper的 NodeDelete
通知,从而感知到某个节点断开,并立即开始错工作在 HBase的实现中,HMaster
会将该 RegionServer所处理的数据分片(Region)重新路由到其他节点上,并记录到
Meta信息中供客户端查询。
204第6 ZooKeeper章的典型应用场景

<==========================215end ==============================>
<==========================216start==============================>

讲到这里,可能有的读者会发问: HBase为什么不直接让 HMaster来负责进行
RegionServer的监控呢? HBase之所以不使用 HMaster直接通过心跳机制等来管理
RegionServer状态,是因为在这种方式下,随着系统容量的不断增加, HMaster的管理
负担会越来越重,另外它自身也有挂掉的可能,因此数据还需要有持久化的必要。在这
种情况下, ZooKeeper就成为了理想的选择。
RootRegion管理
对于 HBase集群来说,数据存储的位置信息是记录在元数据分片,也就是RootRegion
上的。每次客户端发起新的请求,需要知道数据的位置,就会去查询 RootRegion,而
RootRegion自身的位置则是记录在 ZooKeeper上的(默认情况下,是记录在 ZooKeeper
的/hbase/root-region-server--节点中).当 RootRegion发生变化,比如 Region的手工移动、
Balance或者是 RootRegion所在服务器发生了故障等时,就能够通过 ZooKeeper来感知
到这一变化并做出一系列相应的容灾措施,从而保障客户端总是能拿到正确的
RootRegion信息。
Region状态管理
RegionHBase是中数据的物理切片,每个 Region中记录了全局数据的一小部分,并且
不同的 Region之间的数据是相互不重复的但对于一个分布式系统来说, Region是会
经常发生变更的,这些变更的原因来自于系统故障、负载均衡、配置修改、 Region分裂
与合并等。一旦 Region发生移动,它必然会经历 Offline和重新 Online的过程
在 Offline期间数据是不能被访问的,并且 Region的这个状态变化必须让全局知晓,否
则可能会出现某些事务性的异常。而对于 HBase集群来说, Region的数量可能会多达
10万级别,甚至更多,因此这样规模的 Region状态管理也只有依靠 ZooKeeper这样的
系统才能做到。
分布式 SplitLog任务管理
当某台 RegionServer服务器挂掉时,由于总有一部分新写入的数据还没有持久化到 HFile
中,因此在迁移该 RegionServer的服务时,一个重要的工作就是从HLog中恢复这部分
还在内存中的数据,而这部分工作最关键的一步就是 SplitLog,即 HMaster需要遍历该
RegionServer服务器的HLog,并按 Region切分成小块移动到新的地址下,并进行数据
的 Replay
由于单个 RegionServer的日志量相对庞大(可能有数千个 Region,上GB的日志),而用
户又往往希望系统能够快速完成日志的恢复工作因此一个可行的方案是将这个处理
6.2 ZooKeeper在大型分布式系统中的应用205

<==========================216end ==============================>
<==========================217start==============================>

Hlog的任务分配给多台 RegionServer服务器来共同处理,而这就又需要一个持久化组
件来辅助 HMaster完成任务的分配。当前的做法是, HMaster会在 ZooKeeper上创建一
个 splitlog的节点(默认情况下,是/ basesplitlog节点),将“哪个RegionServer处理哪
个 Region”这样的信息以列表的形式存放到该节点上,然后由各个 RegionServer服务器
自行到该节点上去领取任务并在任务执行成功或失败后再更新该节点的信息,以通知
HMaster继续进行后面的步骤 ZooKeeper在这里担负起了分布式集群中相互通知和信
息持久化的角色。
Replication管理
ReplicationBase是实现中主备集群间的实时同步的重要模块。有了 Replication,HBase
就能实现实时的主备同步,从而拥有了容灾和分流等关系型数据库才拥有的功能,从而
大大加强了 HBase的可用性,同时也拓展了其应用场景。和传统关系型数据库的
Replication功能所不同的是, HBase作为分布式系统,它的 Replication是多对多的,且
每个节点随时都有可能挂掉,因此在这样的场景下做 Replication要比普通数据库复杂得
多。
HBase同样借助 ZooKeeper来完成 Replication功能。做法是在 ZooKeeper上记录一个
replication节点(默认情况下,是/hase/replication节点),然后把不同的 RegionServer
服务器对应的HLog文件名称记录到相应的节点上, HMaster集群会将新增的数据推送
给 Slave集群,并同时将推送信息记录到 ZooKeeper上(我们将这个信息称为“断点记
录”),然后再重复以上过程。当服务器挂掉时,由于 ZooKeeper上经保存了断点信息,
因此只要有 HMaster能够根据这些断点信息来协调用来推送HLog数据的主节点服务器,
就可以继续复制了。
ZooKeeper部署
下面我们再来看下 HBase中是如何进行 ZooKeeper部署的。 HBase的启动脚本
(hbaseenv-sh)中可以选择是由HBase启动其自带的默认 ZooKeeper,还是使用一个已
有的外部 ZooKeeper集群。一般的建议是使用第二种方式,因为这样就可以使得多个
HBase集群复用同一套 ZooKeeper集群,从而大大节省机器成本。当然,如果一个
ZooKeeper集群需要被几个 HBase复用的话,那么务必为每一个 HBase集群明确指明对
应的 ZooKeeper根节点配置(对应的配置项是 zookeeper znode. parent),以确保各个 HBase
集群间互不干扰。而对于 HBase的客户端来说,只需要指明 ZooKeeper的集群地址以及
对应的 HBase根节点配置即可,不需要任何其他配置。当 HBase集群启动的时候,会
ZooKeeper上逐个添加相应的初始化节点,并在 HMaster以及 RegionServer进程中进
行相应节点的 Watcher注册。
206第6章 ZooKeeper的典型应用场景

<==========================217end ==============================>
<==========================218start==============================>

小结
以上就是一些 HBase系统中依赖 ZooKeeper完成分布式协调功能的典型场景但事实上,
HBase对于 ZooKeeper的依赖还不止这些,比如 HMaster依赖 ZooKeeper来完成
ActiveMaster的选举、 BackupMaster的实时接管、 Table的 enable/disable状态记录,以
及 HBase中几乎所有的元数据存储都是放在 ZooKeeper上的。有趣的是, HBase甚至还
通过 ZooKeeper来实现 DrainingServer这样的增强功能(相当于降级标志)。事实上,由
于 ZooKeeper出色的分布式协调能力以及良好的通知机制, HBase在各版本的演进过程
中越来越多地增加了 ZooKeeper的应用场景,从趋势上来看两者的交集越来越多。HBase
中所有对 ZooKeeper的操作都封装在了org. apache. hadoop.ase e. zookeeper这个包中,感
兴趣的读者可以自行研究。
6.2.3 Kafka
KafkaLinkedIn是知名社交网络公司于2010年12月份开源的分布式消息系统,主要由
Scala语言开发,于2012年成为 Apache的顶级项目,目前被广泛应用在包括 Twitter
Netflix和 Tumblr等在内的一系列大型互联网站点上。
Kafka主要用于实现低延迟的发送和收集大量的事件和日志数据这些数据通常都是
活跃的数据。所谓活跃数据,在互联网大型的Web网站应用中非常常见,通常是指网站
的PV数和用户访问记录等。这些数据通常以日志的形式记录下来,然后由一个专门的
系统来进行日志的收集与统计。
Kafka是一个吞吐量极高的分布式消息系统其整体设计是典型的发布与订阅模式系统。
在 Kafka集群中,没有“中心主节点”的概念,集群中所有的服务器都是对等的,因此,
可以在不做任何配置更改的情况下实现服务器的添加与删除,同样,消息的生产者和消
费者也能够做到随意重启和机器的上下线 Kafka服务器及消息生产者和消费者之间的
部署关系如图6-29所示。
注6: 6:  kafka http://kafka. apache.org/.官方网站:htp:afa.apache.org
6.2 ZooKeeper在大型分布式系统中的应用207

<==========================218end ==============================>
<==========================219start==============================>

生产者
生产者
producer
producer
BROKERI
BROKER 2
BROKER 3
topicl/partl
topicl/partl
topicl/partl
/part2
/part2
/part2
topic2/partl
topic2/partl
topic2/partl
消费者
consumer
消费者
图6-29. Kafka消息系统生产者和消费者部署关系生7
术语介绍
尽管 Kafka是一个近似符合JMS规范的消息中间件实现,但是为了让读者能够更好地理
解本节余下部分的内容,这里首先对Kafa中的一些术语进行简单的介绍。
·消息生产者,即Producer,是消息产生的源头,负责生成消息并发送到 Kafka服务
器上。
·消息消费者,即 Consumer,是消息的使用方,负责消费Kafka服务器上的消息。
主题,即Topic,由用户定义并配置在 Kafka服务端,用于建立生产者和消费者之
间的订阅关系:生产者发送消息到指定 Topic下,消费者从这个 Topic下消费消息。
·消息分区,即 Partition,一个 Topic下面会分为多个分区,例如“kafka--test”这个
Topic可以分为10个分区,分别由两台服务器提供,那么通常可以配置为让每台服
务器提供5个分区,假设服务器ID分别为01,则所有分区为0-0、0-1、0-2、
0-3、0-4和1-0、1-1、1-2、1-3、1-4消息分区机制和分区的数量与消费者的负载
均衡机制有很大关系,后面将会重点展开讲解。
Broker,即 Kafka的服务器,用于存储消息,在消息中间件中通常被称为 Broker
消费者分组,即 Group,用于归组同类消费者。在 Kafka中,多个消费者可以共同
消费一个 Topic下的消息,每个消费者消费其中的部分消息,这些消费者就组成了
一个分组,拥有同一个分组名称,通常也被称为消费者集群。
Offset,消息存储在 Kafka的 Broker上消费者拉取消息数据的过程中需要知道消
息在文件中的偏移量,这个偏移量就是所谓的 Offset
注7:图片来自 Kafka官方论文 Kafka: Distributed Messaging System for Log Processing的插图。
208第6章 ZooKeeper的典型应用场景

<==========================219end ==============================>
<==========================220start==============================>

Broker注册
KafkaBroker是一个分布式的消息系统,这也体现在其 Producer Consumer的布式
部署上。虽然 Broker是分布式部署并且相互之间是独立运行的,但还是需要有一个注册
系统能够将整个集群中的 Broker服务器都管理起来。在Kaka的设计中,选择了使用
ZooKeeper来进行所有 Broker的管理。
在 ZooKeeper上会有一个专门用来进行 Broker服务器列表记录的节点,下文中我们称之
为“Broker节点”,其节点路径为/ brokers/ids
每个 Broker服务器在启动时,都会到 ZooKeeper上进行注册,即到 Broker节点下创建
属于自己的节点,其节点路径为/brokerids/[0N
从上面的节点路径中,我们可以看出,在 Kafka中,我们使用一个全局唯一的数字来指
代每一个 Broker服务器,可以称其为“Broker ID”,不同的 Broker必须使用不同的 Broker
ID进行注册,例如/broker/ids/和/broker/ids/2分别代表了两个 Broker服务器。创建完
Broker节点后,每个Broker就会将自己的IP地址和端口等信息写入到该节点中去。
请注意, Broker创建的节点是一个临时节点,也就是说,一旦这个Broker服务器宕机
或是下线后,那么对应的 Broker节点也就被删除了。因此我们可以通过 ZooKeeper上
Broker节点的变化情况来动态表征 Broker服务器的可用性。
Topic注册
在 Kafka中,会将同一个 Topic的消息分成多个分区并将其分布到多个 Broker上,而这
些分区信息以及与 Broker的对应关系也都是由 ZooKeeper维护的,由专门的节点来记录,
其节点路径为/brokers/topics。下文中我们将这个节点称为“Topic节点”。 Kafka中的每
一个 Topic,都会以/brokerstopics/ topic的形式记录在这个节点下,例如/brokers/topics
ogin和/brokers/topics/search等。
Broker服务器在启动后,会到对应的 Topic节点下注册自己的 Broker ID,并写入针对该
Topic的分区总数。例如,/brokers/topics/ogin/3→2这个节点表明 Broker ID为3的一
个 Broker服务器,对于“login”这个Topic的消息,提供了2个分区进行消息存储。同
样,这个分区数节点也是一个临时节点。
生产者负载均衡
在上面的内容中,我们讲解了 KafkaB是分布式部署服务器的,会对同一个
的消息进行分区并将其分布到不同的 Broker服务器上。因此,生产者需要将消息合理地
6.2 ZooKeeper在大型分布式系统中的应用209

<==========================220end ==============================>
<==========================221start==============================>

发送到这些分布式的 Broker上这就面临一个问题:如何进行生产者的负载均衡。对
于生产者的负载均衡, Kafka支持传统的四层负载均衡,同时也支持使用 ZooKeeper方
式来实现负载均衡,这里我们首先来看使用四层负载均衡的方案。
四层负载均衡
四层负载均衡方案在设计上比较简单,一般就是根据生产者的P地址和端口来为其确
定一个相关联的 Broker。通常一个生产者只会对应单个 Broker,然后该生产者生成的所
有消息都发送给这个 Broker。从设计上,我们可以很容易发现这种方式的优缺点:好处
是整体逻辑简单,不需要引入其他三方系统,同时每个生产者也不需要同其他系统建立
额外的TCP链接,只需要和 Broker维护单个TCP链接即可。
但这种方案的弊端也是显而易见的,事实上该方案无法做到真正的负载均衡。因为在系
统实际运行过程中,每个生产者生成的消息量,以及每个 Broker的消息存储量都是不一
样的,如果有些生产者产生的消息远多于其他生产者的话,那么会导致不同的 Broker
接收到的消息总数非常不均匀。另一方面,生产者也无法实时感知到 Broker的新增与删
除,因此,这种负载均衡方式无法做到动态的负载均衡。
使用 ZooKeeper进行负载均衡
在 Kafka中,客户端使用了基于 ZooKeeper的负载均衡策略来解决生产者的负载均衡问
题。在前面内容中也已经提到,每当一个 Broker启动时,会首先完成 Broker注册过程,
并注册一些诸如“有哪些可订阅的 Topic的元数据信息。生产者就能够通过这个节点
的变化来动态地感知到 Broker服务器列表的变更。在实现上, Kafka的生产者会对
ZooKeeper上的“Broker的新增与减少”、“Topic的新增与减少”和“Broker与 Topic关
联关系的变化”等事件注册 Watcher监听,这样就可以实现一种动态的负载均衡机制了。
此外,在这种模式下,还能够允许开发人员控制生产者根据一定的规则(例如根据消费
者的消费行为)来进行数据分区,而不仅仅是随机算法而已 Kafka将这种特定的分
区策略称为“语义分区”。显然, ZooKeeper在整个生产者负载均衡的过程中扮演了非常
重要的角色,通过 ZooKeeper的 Watcher通知能够让生产者动态地获取 Broker和 Topic
的变化情况。
消费者负载均衡
与生产者类似, Kafka中的消费者同样需要进行负载均衡来实现多个消费者合理地从对
应的 Broker服务器上接收消息。 Kafka有消费者分组的概念,每个消费者分组中都包含
了若干个消费者,每一条消息都只会发送给分组中的一个消费者,不同的消费者分组消
费自己特定 Topic下面的消息,互不干扰,也不需要互相进行协调。因此消费者的负载
210第6章 ZooKeeper的典型应用场景

<==========================221end ==============================>
<==========================222start==============================>

均衡也可以看作是同一个消费者分组内部的消息消费策略。
消息分区与消费者关系
对于每个消费者分组, Kafka都会为其分配一个全局唯一的 Group ID,同一个消费者分
组内部的所有消费者都共享该ID同时, Kafka也会为每个消费者分配一个 ID,
通常采用“Hostname:uuid”的形式来表示。在 Kafka的设计中,规定了每个消息分区
有且只能同时有一个消费者进行消息的消费,因此,需要在 ZooKeeper上记录下消息分
区与消费者之间的对应关系。每个消费者一旦确定了对一个消息分区的消费权利,那么
需要将其 Consumer ID写入到对应消息分区的临时节点上,例如/consumers/group_id
owners/ topic]broker_id--partitionid],其中“[broker id-partition- id]”就是一个消息
分区的标识,节点内容就是消费该分区上消息的消费者的 Consumer ID
消息消费进度 Offset记录
在消费者对指定消息分区进行消息消费的过程中,需要定时地将分区消息的消费进度,
即 Offset记录到 ZooKeeper上去,以便在该消费者进行重启或是其他消费者重新接管该
消息分区的消息消费后,能够从之前的进度开始继续进行消息的消费。 Offset在
ZooKeeper上的记录由一个专门的节点负责,其节点路径为/consumers/[group_id
setx7Ioqi7Ibroker《id-partionudOfIset/rokerid-partitioni,其节点内容就是offset值。
消费者注册
下面我们再来看看消费者服务器在初始化启动时加入消费者分组的过程。
1.注册到消费者分组。
每个消费者服务器在启动的时候,都会到 ZooKeeper的指定节点下创建一个属于
自己的消费者节点,例如/consumers/group_id/is/ consumer_id]
完成节点创建后,消费者就会将自己订阅的 Topic信息写入该节点。注意,该节
点也是一个临时节点,也就是说,一旦消费者服务器出现故障或是下线后,其对
应的消费者节点就会被删除掉。
2.对消费者分组中消费者的变化注册监听。
每个消费者都需要关注所属消费者分组中消费者服务器的变化情况,即对
consumers/group_id/ids节点注册子节点变化的 Watcher监听。一旦发现消费者
新增或减少,就会触发消费者的负载均衡。
6.2 ZooKeeper在大型分布式系统中的应用211

<==========================222end ==============================>
<==========================223start==============================>

3.对 Broker服务器的变化注册监听。
消费者需要对/brkr/[N中的节点进行监听的注册,如果发现 Broker服务器
列表发生变化,那么就根据具体情况来决定是否需要进行消费者的负载均衡。
4.进行消费者负载均衡。
所谓消费者负载均衡,是指为了能够让同一个 Topic下不同分区的消息尽量均衡
地被多个消费者消费而进行的一个消费者与消息分区分配的过程。通常,对于一
个消费者分组,如果组内的消费者服务器发生变更或 Broker服务器发生变更,会
触发消费者负载均衡。
负载均衡
KafkaZooKeeper借助上记录的 Broker和消费者信息,采用了一套特殊的消费者负载均
衡算法。由于该算法和 ZooKeeper本身关系并不是特别大,因此这里只是结合官方文档
来对该算法进行简单的陈述,不做详细讲解。
我们将一个消费者分组的每个消费者记为C1c2,,Cg,那么对于一个消费者C1,
其对应的消息分区分配策略如下。
1.设置P为指定 Topic所有的消息分区。
2.设置CG为同一个消费者分组中的所有消费者。
3.对P进行排序,使分布在同一个Broker服务器上的分区尽量靠在一起。
4.对C进行排序。
5.设置i为C1在C中位置的索引值,同时设置N=size(pr)/size(Cg)
6.将编号为ixN~(i+1)N-1的消息分区分配给消费者C
7.重新更新 ZooKeeper上消息分区与消费者C的关系。
关于 Kafka消费者的负载均衡算法,读者可以访问其官方网站进行更深入的了解。
小结
Kafka从设计之初就是一个大规模的分布式消息中间件,其服务端存在多个 Broker,同
时为了达到负载均衡,将每个 Topic的消息分成了多个分区,并分布在不同的 Broker上,
多个生产者和消费者能够同时发送和接收消息。 Kafka使用 ZooKeeper作为其分布式协
212第6章 ZooKeeper的典型应用场景

<==========================223end ==============================>
<==========================224start==============================>

调框架,很好地将消息生产、消息存储和消息消费的过程有机地结合起来。同时借助
ZooKeeper, Kafka能够在保持包括生产者、消费者和 Broker在内的所有组件无状态的
情况下,建立起生产者和消费者之间的订阅关系,并实现了生产者和消费者的负载均衡。
6.6J3《ZooKeeper在阿里巴巴的实践与应用
随着2010年底 ZooKeeper正式从 Hadoop子项目中剥离出来,成为了Apache的顶级项
目之后,越来越多的开源项目和商业公司在自己的生产环境中引入了 ZooKeeper,并基
于该分布式协调框架来实现自己的上层业务系统,一时间, ZooKeeper成为了最热门的
分布式开发利器。
自2011年上半年起,阿里巴巴中间件团队的几位技术专家,率先将 ZooKeeper引入到
了阿里巴巴集团,并先后基于其开发了一系列分布式系统,其中就包括知名的分布式消
息中间件 Metamorphosis和PAAS解决方案TAE系统。经过近3年的开发与运维,目前
中间件团队运维的 ZooKeeper集群规模,已经从最初的3台服务器,增长到了7个集群
27台服务器;客户端规模也已经从最初不到100个客户端,增长到了1万多个客户端,
高峰时期甚至覆盖全网1/3的机器。同时,也滋生出了众多上层业务系统,其中包括消
息中间件 Metamorphosis、RPC服务框架 Dubbo、 MySQL复制组件 Canal和同步组件
Otter等一大批知名的开源系统。
在本节中,我们将围绕阿里巴巴的这些基于 ZooKeeper构建的开源系统,来讲解
ZooKeeper在阿里巴巴的实践与应用
6.3.1案例一消息中间件: Metamorphosis
Metamorphosis是阿里巴巴中间件团队的 killme22008和wq163于2012年3月开源的一个
Java消息中间件,目前项目主页地址为 Java, https: //github. com/killme2008/Metamorphosis,:gubcom/kilm20/metamorphosis,由
开源爱好者及项目的创始人 killme22008和wq163持续维护。关于消息中间件,相信读者
应该都听说过JMS规范,以及一些典型的开源实现,如 ActiveMQ和 HornetQ等
Metamorphosis也是其中之一。
Metamorphosis是一个高性能、高可用、可扩展的分布式消息中间件,其思路起源于
LinkedIn的 Kafka,但并不是 Kafka的一个简单复制。 Metamorphosis具有消息存储顺序
写、吞吐量大和支持本地XA事务等特性适用于大吞吐量、顺序消息、消息广播和日
志数据传输等分布式应用场景,目前在淘宝和支付宝都有着广泛的应用,其系统整体部
署结构如图6-30所示。
6.3 ZooKeeper在阿里巴巴的实践与应用213

<==========================224end ==============================>
<==========================225start==============================>

Producer
oducer
同步制
Staver
异步
Broker
异步制
Staver
图6-30. Metamorphosis整体部署结构
和传统的消息中间件采用推(Push)模型所不同的是, Metamorphosis是基于拉(ull)
模型构建的,由消费者主动从 Metamorphosis服务器拉取数据并解析成消息来进行消费,
同时大量依赖 ZooKeeper来实现负载均衡和 Offset的存储。
生产者的负载均衡
和 Kafka系统一样, Metamorphosis假定生产者、 Broker和消费者都是分布式的集群系
统。生产者可以是一个集群,多台机器上的生产者可以向相同的 Topic发送消息。而服
务器 Broker通常也是一个集群,多台 Broker组成一个集群对外提供一系列的 Topic消
息服务,生产者按照一定的路由规则向集群里某台 Broker发送消息,消费者按照一定的
路由规则拉取某台 Broker上的消息。每个 Broker都可以配置一个 Topic的多个分区,
但是在生产者看来,会将一个 Topic在所有 Broker上的所有分区组成一个完整的分区列
表来使用。
在创建生产者的时候,客户端会从 ZooKeeper获取已经配置好的 Topic对应的 Broker
和分区列表,生产者在发送消息的时候必须选择一台 Broker上的一个分区来发送消息,
默认的策略是一个轮询的路由规则,如图6-31所示。
Partition
Parifion
图6-31. Metamorphosis生产者消息分区发送示意图
214第6章 ZooKeeper的典型应用场景

<==========================225end ==============================>
<==========================226start==============================>

生产者在通过 ZooKeeper获取分区列表之后,会按照 Broker Id和 Partition的顺序排列
组织成一个有序的分区列表,发送的时候按照从头到尾循环往复的方式选择一个分区来
发送消息。考虑到我们的 Broker服务器软硬件配置基本一致,因此默认的轮询策略已然
足够。
在 Broker因为重启或者故障等因素无法提供服务时, Producer能够通过 ZooKeeper感知
到这个变化,同时将失效的分区从列表中移除,从而做到 Fail Over需要注意的是,因
为从故障到生产者感知到这个变化有一定的延迟,因此可能在那一瞬间会有部分的消息
发送失败。
消费者的负载均衡
消费者的负载均衡则会相对复杂一些,我们这里讨论的是单个分组内的消费者集群的负
载均衡,不同分组的负载均衡互不干扰。消费者的负载均衡跟 Topic的分区数目和消费
者的个数紧密相关,我们分几个场景来讨论。
消费者数和 Topic分区数一致
如果单个分组内的消费者数目和 Topic总的分区数目相同,那么每个消费者负责消费一
个分区中的消息,一一对应,如图6-32所示。
Partition 1
Partition 2
Partition 3
图6-32.消费者数和 Topic分区数一致情况下的分区消息消费示意图
消费者数大于 Topic分区数
如果单个分组内的消费者数目比 Topic总的分区数目多,则多出来的消费者不参与消费,
如图6-33所示。
Partition 1
Partition2
图6-33.消费者数大于 Topic分区数情况下的分区消息消费示意图
6.63《ZooKeeper215《在阿里巴巴的实践与应用215

<==========================226end ==============================>
<==========================227start==============================>

消费者数小于 Topic分区数
如果分组内的消费者数目比 Topic总的分区数目小,则有部分消费者需要额外承担消息
的消费任务,具体如图6-34所示。
Partition 1
Partiton 2
Partiton 4
图6-34.消费者数小于 Topic分区数情况下的分区消息消费示意图
当分区数目(n)大于单个Group的消费者数目(m)的时候,则有n%m个消费者需要
额外承担1/n的消费任务,我们假设n无限大,那么这种策略还是能够达到负载均衡的
目的的。
综上所述,单个分组内的消费者集群的负载均衡策略如下。
每个分区针对同一个Group只能挂载一个消费者,即每个分区至多同时允许被一个
消费者进行消费。
·如果同一个Group的消费者数目大于分区数目,则多出来的消费者将不参与消费。
如果同一个Group的消费者数目小于分区数目,则有部分消费者需要额外承担消费
任务。
Metamorphosis的客户端会自动处理消费者的负载均衡,将消费者列表和分区列表分别
排序,然后按照上述规则做合理的挂载。
从上述内容来看,合理地设置分区数目至关重要。如果分区数目太小,则有部分消费者
可能闲置;如果分区数目太大,则对服务器的性能有影响。
在某个消费者发生故障或者发生重启等情况时,其他消费者会感知到这一变化(通过
ZooKeeper的“节点变化”通知),然后重新进行负载均衡,以保证所有的分区都有消费
者进行消费。
消息消费位点 Offset存储
为了保证生产者和消费者在进行消息发送与接收过程中的可靠性和顺序性,同时也是为
了尽可能地保证避免出现消息的重复发送和接收, Metamorphosis会将消息的消费记录
Offset记录到 ZooKeeper上去,以尽可能地确保在消费者进行负载均衡的时候,能够正
216第6章 ZooKeeper的典型应用场景

<==========================227end ==============================>
<==========================228start==============================>

确地识别出指定分区的消息进度。
6.3.2案例二RPC服务框架: Dubbo
Dubbo是阿里巴巴于2011年10月正式开源的一个由Java语言编写的分布式服务框架,
致力于提供高性能和透明化的远程服务调用方案和基于服务框架展开的完整SOA服务
治理方案。目前项目主页地址为 https://github. com/alibabaldubbo.
Dubbo的核心部分包含以下三块。
远程通信:提供对多种基于长连接的NO框架抽象封装,包括多种线程模型、序列
化,以及“请求一响应”模式的信息交换方式。
集群容错:提供基于接口方法的远程过程透明调用,包括对多协议的支持,以及对
软负载均衡、失败容错、地址路由和动态配置等集群特性的支持。
自动发现:提供基于注册中心的目录服务,使服务消费方能动态地查找服务提供方,
使地址透明,使服务提供方可以平滑地增加或减少机器。
此外, Dubbo框架还包括负责服务对象序列化的 Serialize组件、网络传输组件 Transport、
协议层 Protocol以及服务注册中心 Registry等,其整体模块组成和协作方式如图6-35所
示。在本节中,我们将主要关注 Dubbo中基 ZooKeeper实现的服务注册中心
图6-35. Dubbo整体模块协作示意图
6.3 ZooKeeper在阿里巴巴的实践与应用217

<==========================228end ==============================>
<==========================229start==============================>

注册中心是RPC框架最核心的模块之一,用于服务的注册和订阅。在 Dubbo的实现中,
对注册中心模块进行了抽象封装,因此可以基于其提供的外部接口来实现各种不同类型
的注册中心,例如数据库、 ZooKeeper和 Redis等。在本书前面部分我们已经多次提到,
ZooKeeper是一个树形结构的目录服务,支持变更推送,因此非常适合作为 Dubbo服务
的注册中心,下面我们着重来看基于 ZooKeeper实现的 Dubbo注册中心
在 Dubbo注册中心的整体架构设计中, ZooKeeper上服务的节点设计如图6-36所示。
dubbo
Service
consur
url1020153102880
10201410
subscribe register subscribe
Provider register
Moni
图6-36.基于 ZooKeeper实现的注册中心节点结构示意图
dubbo:这是 Dubbo在 ZooKeeper上创建的根节点。
dubbo/com.foo. BarService:这是服务节点,代表了 Dubbo的一个服务。
dubbo/com,foo. BarService/providers:这是服务提供者的根节点,其子节点代表了每
一个服务的真正提供者。
dubbo/com.foo. BarService/consumers:这是服务消费者的根节点,其子节点代表了
每一个服务的真正消费者。
结合图6-36,我们以“com.foo. BarService”这个服务为例,来说明 Dubbo基于 ZooKeeper
实现的注册中心的工作流程。
服务提供者
服务提供者在初始化启动的时候,会首先在 ZooKeeper的/dubbo/com.foo. BarService
providers节点下创建一个子节点,并写入自己的URL地址,这就代表了“com.foo.
BarService”这个服务的一个提供者。
服务消费者
服务消费者会在启动的时候,读取并订阅 ZooKeeper上/dubbol/comfoo. BarService
218第6章 ZooKeeper的典型应用场景

<==========================229end ==============================>
<==========================230start==============================>

providers节点下的所有子节点,并解析出所有提供者的URL地址来作为该服务地
址列表,然后开始发起正常调用。
同时,服务消费者还会在 ZooKeeper的/dubb/com.foo. BarServiceconsumers节点下
创建一个临时节点,并写入自己的URL地址,这就代表了“comfo.BarService
这个服务的一个消费者。
监控中心
监控中心是 Dubbo中服务治理体系的重要一部分,其需要知道一个服务的所有提
供者和订阅者,及其变化情况。因此,监控中心在启动的时候,会通过 ZooKeeper
的/dubbo/cmfoo. BarService节点来获取所有提供者和消费者的URL地址,并注册
Watcher来监听其子节点变化。
另外需要注意的是,所有提供者在ZooKeeper上创建的节点都是临时节点,利用的是临
时节点的生命周期和客户端会话相关的特性因此一旦提供者所在的机器出现故障导致
该提供者无法对外提供服务时,该临时节点就会自动从 ZooKeeper上删除,这样服务的
消费者和监控中心都能感知到服务提供者的变化。
在 ZooKeeper节点结构设计上,以服务名和类型作为节点路径,符合 Dubbo订阅和通知
的需求,这样保证了以服务为粒度的变更通知,通知范围易于控制,即使在服务的提供
者和消费者变更频繁的情况下,也不会对 ZooKeeper造成太大的性能影响
6.3.3案例三基于 Binlog的增量订阅和消费组件:
Canal
Canal是阿里巴巴于2013年1月正式开源的一个由纯Java语言编写的基于 MySQL数据
库 Binlog https: //github. com /alibabal实现的增量订阅和消费组件。目前项目主页地址为https:giub.om/alibaba/
canal,由项目主要负责人,同时也是资深的开源爱好者 agapple持续维护。
项目名 Canal取自“管道”的英文单词,寓意数据的流转,是一个定位为基于 MySQL
数据库的 Binlog增量日志来实现数据库镜像、实时备份和增量数据消费的通用组件。
早期的数据库同步业务,大多都是使用 MySQL数据库的触发器机制(即 Trigger)来获
取数据库的增量变更。不过从2010年开始,阿里系下属各公司开始逐步尝试基于数据
库的日志解析来获取增量变更,并在此基础上实现数据的同步,由此衍生出了数据库的
增量订阅和消费业务 Canal项目也由此诞生了。
Canal的工作原理相对比较简单,其核心思想就是模拟 MySQL Slave的交互协议,将自
6.3 ZooKeeper在阿里巴巴的实践与应用219

<==========================230end ==============================>
<==========================231start==============================>

己伪装成一个 MySQL的 Slave机器,然后不断地向 Master服务器发送Dump请求 Master
收到Dump请求后,就会开始推送相应的 Binary Log给该 Slave(也就是 Canal) Canal
收到 Binary Log,解析出相应的 Binary Log对象后就可以进行二次消费了,其基本工作
原理如图6-37所示。
Master
I'mSlave
Mysou
000
Canal
Binary Log
图6-37. Canal基本工作原理示意图
Canal Server主备切换设计
在 Canal的设计中,基于对容灾的考虑,往往会配置两个或更多个 Canal Server来负责
一个 MySQL数据库实例的数据增量复制另一方面,为了减少 Canal Server的ump
请求对 MySQL Master所带来的性能影响,要求不同的 Canal Server上的 instance在同
一时刻只能有一个处于 Running状态,其他的 instance都处于 Standby状态,这就使得
Canal必须具备主备自动切换的能力。在 Canal中,整个主备切换过程控制主要是依赖
于 ZooKeeper来完成的,如图6-38所示。
A
B
Canal Server
/Canal Server
2.允许启动
1试动
1试动
检测发现A机器不
可用,通知启动
ZooKeeper
记录当前 Running状态的机器信息
记录当前可用的机器列表信意
图6-38. Canal Server主备切换机制
1.尝试启动。
每个 Canal Server在启动某个 Canal instance的时候都会首先向 ZooKeeper进行一
次尝试启动判断。具体的做法是向ZooKeeper创建一个相同的临时节点,哪个 Canal
Server创建成功了,那么就让哪个 Server启动。
220第6章 ZooKeeper的典型应用场景

<==========================231end ==============================>
<==========================232start==============================>

以“example”这个instance为例来说明,所有的 Canal Server在启动的时候,都
会去创建oter/canaldestinations/example/running节点,并且无论有多少个 Canal
Server同时并发启动, ZooKeeper都会保证最终只有一个 Canal Server能够成功创
建该节点。
2.启动 instance
假设最终IP地址为10.20.144.51的 Canal Server成功创建了该节点,那么它就会
将自己的机器信息写入到该节点中去:
{"active":true,"address":"10.20.144.1:111,"cid":1}
并同时启动 instance。而其他 Canal Server由于没有成功创建节点,于是就会将自
己的状态置为 Standby,同时对 otter/canaldestinations/example/running节点注册
Watcher监听,以监听该节点的变化情况。
3.主备切换。
Canal Server在运行过程中,难免会发生一些异常情况导致其无法正常工作,这个
时候就需要进行主备切换了。基于 ZooKeeper临时节点的特性,当原本处于
Running状态的 Canal Server因为挂掉或网络等原因断开了与 ZooKeeper的连接,
那么 lotter/canaldestinations/example/running节点就会在一段时间后消失。
由于之前处于 Standby状态的所有 Canal Server已经对该节点进行了监听,因此它
们在接收到 ZooKeeper发送过来的节点消失通知后,会重复进行步骤1以此实
现主备切换。
下面我们再来看看在主备切换设计过程中最容易碰到的一个问题,就是“假死”所谓
假死状态是指, Canal Server所在服务器的网络出现闪断,导致 ZooKeeper认为其会话
失效,从而释放了 Running节点但此时 Canal Server对应的JM并未退出,其工作
状态是正常的。
在 Canal的设计中,为了保护假死状态的 Canal Server,避免因瞬间 Running节点失效导
致 instance重新分布带来的资源消耗,所以设计了一个策略:
状态为 Standby的 Canal Server在收到 Running节点释放的通知后,会延迟一段时
间抢占 Running节点,而原本处于 Running状态的 instance,即 Running节点的拥
有者可以不需要等待延迟,直接取得 Running节点。
这样就可以尽可能地保证假死状态下一些无谓的资源释放和重新分配了。目前延迟时间
6.3 ZooKeeper在阿里巴巴的实践与应用221

<==========================232end ==============================>
<==========================233start==============================>

的默认值为5秒,即 Running节点针对假死状态的保护期为5秒。
Canal Client的HA设计
Canal Client在进行数据消费前,首先当然需要找到当前正在提供服务的 Canal Server,
即 Master。在上面“主备切换”部分中我们已经讲到,针对每一个数据复制实例,例如
example,都会在/otter/canaldestinations/example/running节点中记录下当前正在运行的
Canal Server.因此, Canal Client只需要连接 ZooKeeper,并从对应的节点上读取 Canal
Server信息即可。
1.从 ZooKeeper中读取出当前处于 Running状态的 Server
Canal Client在启动的时候,会首先从oter/naldestinationsexample/running节点
上读取出当前处于 Running状态的 Server。同时,客户端也会将自己的信息注册
到 ZooKeeper/ ottercanaldestinations/example/100l/running节点上,其中“1001
代表了该客户端的唯一标识,其节点内容如下:
{"active": true, "address": "10.12.48.171 50544", "clientId": 1001}
2.注册Running节点数据变化的监听。
由于 Canal Server存在挂掉的风险,因此 Canal Client还会对/ ottercanall
destinations/example/running节点注册一个节点变化的监听,这样一旦发生 Server
的主备切换, Client就可以随时感知到。
3.连接对应的 Running Server进行数据消费。
数据消费位点记录
由于存在 Canal Client的重启或其他变化,为了避免数据消费的重复性和顺序错乱,Canal
必须对数据消费的位点进行实时记录。数据消费成功后, Canal Server会在 ZooKeeper
中记录下当前最后一次消费成功的 Binary Log位点,一旦发生重启,只需要从这
最后一个位点继续进行消费即可。具体的做法是在 ZooKeeper的/ otter/canaldestinations
example/100/cursor节点中记录下客户端消费的详细位点信息:
{"@type": "com. alibaba. otter. canal protocol. position. LogPosition","identity":
{"slaveId": -1, "sourceAddress": {"address": "10.20.144. 15", "port": 3306)}}, "posti
on": {"included": false, "journalName""mysql-bin.002253", "position": 2574756,"t
imestamp:13636822000
222第 ZooKeeper6章的典型应用场景

<==========================233end ==============================>
<==========================234start==============================>

6.3.4案例四分布式数据库同步系统:Otter
Otter是阿里巴巴于2013年8月正式开源的一个由纯Java语言编写的分布式数据库同步
系统,主要用于异地双A机房的数据库数据同步,致力于解决长距离机房的数据同步及
双A机房架构下的数据一致性问题。目前项目主页地址为 A. https: //github. com/alibabalotter,
由项目主要负责人,同时也是资深的开源爱好者 agapple持续维护。
项目名 Otter取自“水獭”的英文单词,寓意数据搬运工,是一个定位为基于数据库增
量日志解析,在本机房或异地机房的 MySQLOracle数据库之间进行准实时同步的分布
式数据库同步系统。 Otter的第一个版本可以追溯到2004年,初衷是为了解决阿里巴巴
中美机房之间的数据同步问题,从4.0版本开始开源,并逐渐演变成一个通用的分布式
数据库同步系统。其基本架构如图6-39所示。
(ZooKeeper
置推送状态反情分布式协调
Antrate cong statlste Arbetrat cong satisilo
cana0
图6-39. Otter基本架构示意图
从图6-39中,我们可以看出,在 Otter中也是使用 ZooKeeper来实现一些与分布式协调
相关的功能,下面我们将从 Otter的分布式SEDA模型调度和面向全球机房服务的
ZooKeeper集群搭建两方面来讲解 Otter中的 ZooKeeper使用。
分布式SEDA模型调度
为了更好地提高整个系统的扩展性和灵活性,在 Otter中将整个数据同步流程抽象为类
似于ETL的处理模型,具体分为四个阶段(Stage)
Select:数据接入。
Extract:数据提取。
主8:seda(Staged Event-Driven- Architecture)是阶段事件驱动架构的简称,也称为阶段式服务器模
型。读者可以到哈佛大学的网站上查看更多关于SEDA的内容http:weecs.SEDA《httpwnecshnuru7edu/
~mdw/proj/sedal
6.3 ZooKeeper在阿里巴巴的实践与应用223

<==========================234end ==============================>
<==========================235start==============================>

Transform:数据转换。
Load:数据载。
其中 Select阶段是为了解决数据来源的差异性比如可以接入来自 Canal的增量数据,
也可以接入其他系统的数据源。 Extract/Transform/load阶段则类似于数据仓库的tl
模型,具体可分为数据Join、数据转化和数据Load等过程。同时,为了保证系统的高
可用性,SEDA的每个阶段都会有多个节点进行协同处理。如图6-40所示是该SEDA模
型的示意图。
Stage 1
ZooKeeper
/xxcx/stage/s1
RequestA
RequestB
fxox/stage/s
equestc
RequestD
图6-40.分布式SEDA模型调度示意图
整个模型分为 Stage管理和 Schedule调度两部分。
Stage管理
Stage管理主要就是维护一组工作线程,在接收到 Schedule的 Event任务信号后,分配
一个工作线程来进行任务处理,并在任务处理完成后,反馈信息到 Schedulc
Schedule调度
Schedule调度主要是指基于 ZooKeeper来管理 Stage之间的任务消息传递,其具体实现
逻辑如下。
1.创建节点。
Otter首先会为每个 Stage在 ZooKeeper上创建一个节点,例如seda/lstage/s1,其
中s即为该 Stage的名称,每个任务事件都会对应于该节点下的一个子节点,例
224第6章 ZooKeeper的典型应用场景

<==========================235end ==============================>
<==========================236start==============================>

/seda/stage/s 1/RequestA.
2.任务分配。
当s的上一级 Stage完成 RequestA任务后,就会通知“Schedule调度器”其已完
成了该请求。根据预先定义的 Stage流程, Schedule调度器便会在 Stage s1的目录
下创建一个 RequestA的子节点,告知s1有一个新的请求需要其处理以此完成
一次任务的分配。
3.任务通知。
每个 Stage都会有一个 Schedule监听线程,利用 ZooKeeper的 Watcher机制来关注
ZooKeeper中对应 Stage节点的子节点变化,比如关注s就是关注seda/stage/s
的子节点的变化情况。此时,如果步骤2中调度器在s的节点下创建了一个
RequestA,那么 ZooKeeper就会通过 Watcher制通知到该 Schedule线程,然后
ScheduleStage就会通知进行任务处理以此完成一次任务的通知。
4.任务完成。
当s完成了 Request任务后,会删除s1目录下的 RequestA任务,代表处理完成,
然后继续步骤2,分配下一个 Stage的任务。
在上面的步骤3中,还有一个需要注意的细节是在真正的生产环境部署中,往往都会
由多台机器共同组成一个 Stage来处理 Request,此就涉及多个机器节点之间的分布式
协调。
如果s1有多个节点协同处理,每个节点都会有该 Stage的一个 Shedule线程,其在s1
目录变化时都会收到通知。在这种情况下,往往可以采取抢占式的模式,尝试在 RequestA
目录下创建一个lock节点,谁创建成功就可以代表当前谁抢到了任务,而没抢到该任务
的节点,便会关注该lock节点的变化(因为旦该lock节点消失,那么代表当前抢到
任务的节点可能出现了异常退出,没有完成任务),然后继续抢占模型。
中美跨机房 ZooKeeper集群的部署
由于 Otter主要用于异地双A机房的数据库同步,致力于解决长距离机房的数据同步及
双A机房架构下的数据一致性问题,因此其本身就有面向中美机房服务的需求,也就会
有每个机房都要对 ZooKeeper进行读写操作的需求。于是,希望可以部署一个面向全球
机房服务的 ZooKeeper集群,保证读写数据一致性。
这里就需要使用 ZooKeeper的 Observer功能了。从3.3.0版本开始, ZooKeeper新增了
6.3 ZooKeeper在阿里巴巴的实践与应用225

<==========================236end ==============================>
<==========================237start==============================>

Observer模式,该角色提供只读服务,且不参与事务请求的投票,主要用来提升整个
ZooKeeper集群对非事务请求的处理能力。
因此,借助 ZooKeeper的 Observer特性,Oter将 ZooKeeper集群进行了三地部署。
·杭州机房部署Leader/Follower集群,为了保障系统高可用,可以部署3个机房。每
个机房的部署实例可为1/1/1或者3/2/2的模式。
美国机房部署 Observer集群,为了保证系统高可用,可以部署2个机房,每个机房
的部署实例可以为1/1
·青岛机房部署 Observer集群。
图6-41所示是 ZooKeeper集群三地部署示意图。
中国青岛
美
中国杭州
图6-41. ZooKeeper集群三地部署示意图
当美国机房的客户端发起一个非事务请求时,就直接从部署在美国机房的Observer
ZooKeeper读取数据即可,这将大大减少中美机房之间网络延迟对 ZooKeeper操作的影
响。而如果是事务请求,那么美国机房的 Observer就会将该事务请求转发到杭州机房的
Leader/Follower集群上进行投票处理,然后再通知美国机房的 Observer,最后再由美国
机房的 Observer负责响应客户端。
上面这个部署结构,不仅大大提升了 ZooKeeper集群对美国机房客户端的非事务请求处
理能力,同时,由于对事务请求的投票处理都是在杭州机房内部完成,因此也大大提升
了集群对事务请求的处理能力。
6.3.5案例五轻量级分布式通用搜索平台:终搜
终搜(Terminator)是阿里早期的一款产品,最早应用在淘江湖,基于 Lucene、solr、
226第6章 oKeeper的典型应用场景

<==========================237end ==============================>
<==========================238start==============================>

ZooKeeper和 Hadoop等开源技术构建,全方位支持各种检索需求,是一款实时性高、
接入成本低、支持个性化检索定制的分布式全文检索系统。历经发展,终搜目前已成为
服务于阿里集团内部各大业务线的通用搜索平台,截止2014年4月,已经有200多个
不同规模、不同查询特征的应用接入使用。
终搜系统主要由前端业务查询处理、后台索引构建、数据存储和后台管理四大部分组成,
其整体架构如图6-42所示。
图6-42.终搜系统整体架构
CenterNode
该节点收集和监控整个集群平台所有检索节点机器和引擎 SolrCore的状态,并且根
据这些状态信息来决定业务对应的引擎是否需要进行发布、变更、删除、容灾恢复
和在线扩容等操作。
CoreNode
该节点负责从 CenterNode的任务池领取任务指令,并对相应的业务引擎 SolrCore
进行创建、变更和删除等动作,同时在引擎 SolrCore对象正常创建后提供检索服务。
JobNode
该节点接收 CoreNode提交的业务对应的全量任务指令,并根据 TaskNode当时的空
闲程度将任务分配给最空闲的 TaskNode节点进行全量索引构建任务。
TaskNode
该节点接收来自 JobNodeJ节点分配的全量任务,根据提交的任务配置项启
6.3 ZooKeeper在阿里巴巴的实践与应用227

<==========================238end ==============================>
<==========================239start==============================>

动全量任务,将HDFS上对应业务的全量源数据构建成 Lucene的索引文件,构建
索引完毕后再回流到HDFS
TriggerNode
该节点根据每个业务所配置的时间表达式定时触发业务方的 ClientNode客户端的
增量和全量任务。
ClientNode
该节点是业务方发起查询请求的节点,如果本节点从 ZooKeeper上抢到执行导入的
锁,那么该节点将会接收到 Trigger Node的定时触发指令,然后会根据分库分表规
则将数据库的源数据通过增量和全量模式导入到HDFS
ManagerNode
该节点是整个引擎平台的后台管理节点,负责所有接入业务的发布、扩容和配置变
更等指令的触发,并提供整个引擎平台所有业务状态信息的可视化查询。
ZooKeeper
该节点负责整个引擎平台所有的 CoreNode角色协调,以及所有 ClientNode间的分
布式导入锁的控制。
Hadoop
该平台负责整个引擎平台所有业务引擎所需要的源数据和索引数据的存储。
终搜系统大量依赖 ZooKeeper来实现分布式协调和分布式锁功能,接下来我们就从元数
据管理、中心节点架构、应用配置文件管理和全量任务执行等方面来讲解 ZooKeeper在
终搜中的使用。
元数据管理
为了对所有业务实例的生命周期进行全局的管理,必须对所有业务实例元数据信息进行
结构化的管理。通过各种技术调研,最终选择了 ZooKeeper来进行元数据管理准确
地讲,在终搜中并不是直接简单地拿 ZooKeeper来做这件事,而是开发了一个封装了
ZooKeeper内核的中心节点()集群来负责引擎状态数据收集和搜索业务实
例元数据保存。具体来讲就是 CenterNode内部关于搜索业务实例持久化的工作统一交
给了 ZooKeeper之所以选择 ZooKeeper主要考虑以下两个因素。
228第6章 ZooKeeper的典型应用场景

<==========================239end ==============================>
<==========================240start==============================>

·元数据信息属于目录型的轻量级数据,而 ZooKeeper对目录型的轻量级数据的存储
有天然的优势。
·元数据的信息非常重要,需要副本容灾,而 ZooKeeper正是用来解决分布式数据多
副本存储及数据一致性问题的。
下面我们就来看看如何利用 ZooKeeper对业务实例进行元数据信息的持久化,核心的实
现思路是让 CenterNode掌控整个搜索集群平台所有业务的客户端机器视图和机器状态
等信息,同时监控各个 CoreNode节点(在这里我们将承载搜索实例的节点称为 CoreNode)
的健康状态, CenterNode节点主要收集的内容如下。
机器状态信息收集,包括:
机器操作系统版本;
机器磁盘使用率;
机器内存使用率;
机器 CPU Load情况;
JVM版本信息;
JVM内存使用率。
检索服务状态收集,包括:
一索引构建时间和容量大小;
一每秒响应请求次数;
一索引数据总量;
请求平均响应时间。
这些状态信息收集后需要和具体的 CoreNode一一对应起来,在 CenterNode内存中
CoreNode状态信息的视图关系如图6-43所示。
在 CenterNode中,主要包括两种数据结构。
NameSpaceFile中的静态Core
CorenodeDescrptor中的动态 DynCore
6.3 ZooKeeper在阿里巴巴的实践与应用229

<==========================240end ==============================>
<==========================241start==============================>

NameSpaceFile
Cots-0Core-1Core-2
(DynCore-a
(DynCore-0
(DynCore-2
(DynCore-1
(DynCore-2
DynCore-I
Cor emode
Descaptor
Deserptor
Centertode
7SolrCore-
SolrCore-2
SolCore-2
CoreNode A
CoreNode B
CoreNodeC
图6-43. CoreNode状态信息的视图关系
DynCore不在本书讨论范围内,这里主要介绍下 NameSpaceFile中的静态Core
NameSpaceFile是在创建搜索业务时就会在 CenterNode中生成的一个元数据结构,是搜
索业务在 CenterNode中的一个管理抽象和业务抽象,主要内容包括该业务 Shard的数量、
副本数量以及涉及的配置文件名称等。例如,某个业务存在3个 Shard分片,那么就会
在 NameSpaceFile中存在3个Core的抽象这些信息一旦发布基本都不大会改变,除非
出现扩容情况。
CenterNodeNa对于每个业务的管理和操作都是基于进行的,例如扩容和容
灾等。同时,这些信息是需要持久化存储的,所以在这里使用 ZooKeeper来做持久化,
其在 ZooKeeper上的数据节点结构如下:
/tsearcher/centernode/namespace/search4A/seq
/tsearcher/centernode/namespace/search4B/seq
/tsearcher/centernode/namespace/search4C/seq
其中每个seq节点中保存的都是一个序列化的 NameSpaceFile数据
Leader/Follower模式的中心节点架构
在上文中我们已经提到,中心节点(CenterNode)在整个终搜平台中起到了中心调度的
作用,是终搜系统完成信息收集、汇总和分发的中转节点,是把整个系统串联在一起的
一个重要组成部分。因此,中心节点是整个终搜的核心,如果中心节点机器宕机导致无
230第6章 ZooKeeper的典型应用场景

<==========================241end ==============================>
<==========================242start==============================>

法对外服务的话,那么终搜所有业务机器的状态信息将全部丢失。于是,如何处理好中
心节点的容灾问题成为了终搜中最关键也是最棘手的一环。
旧版本终搜的中心节点采用的是类似于HDF的 NameNode处理方式:使用两台机器来
保证中心节点的稳定性,一台用来部署中心节点的组件,另一台用来同步中心节点的数
据文件到本地,实现中心节点中元数据文件的远程备份,该节点称为 ImageNode
ImageNode对中心节点进行数据冗余备份,负责对中心节点中业务元数据信息
(NameSpaceFile信息)的定期快照,如图6-44所示。
seac节点
Search节点
searcht节点
中心节点
同步
Search节点
search节点
图6-44.中心节点的 ImageNode模式主备架构
利用HDFS的 Image Node解决中心节点单点失败的方式虽然可以在一定程度上恢复宕机
之前的业务元数据信息,但是还是会存在一些问题。
·该方案必须通过人工手动处理的方式寻找并复制在远程 ImageNode机器中保存的
快照文件,然后手工重启中心节点无法自动化完成在宕机之后的数据复制和机
器重启,从而自动完成中心节点的恢复。
·在中心节点失败期间,无法收集机器的状态信息,也无法对业务进行操作,系统不
可用时间完全取决于人工恢复中心节点的时间长短。
中心节点和ImageNode之间的异步化的数据同步,在一些极端情况下会出现数据丢
失的情况。
正是由于以上三个问题的存在,使得虽然可以在中心节点失败后利用 ImageNode中保存
的快照文件对业务进行恢复,但还是会存在一些不足之处,所以考虑采用 ZooKeeper多
机器副本原理改造中心节点,使得中心节点能具备多副本概念,当其中一台主节点宕机
的时候,能够自动地从其余从节点中选举出主节点来,再重新提供服务,如图6-45所
示。
6.3 ooKeeper在阿里巴巴的实践与应用231

<==========================242end ==============================>
<==========================243start==============================>

中心节点(L)
Zeckeepes(
点
中心节点(F)
中心节点(F)
图6-45.基于 ZooKeeper的 LeaderFollower模式的中心节点架构示意图
选主机制
从图6-45中我们可以看到,终搜的中心节点都是基于 ZooKeeper架构的事实上,中
心节点就是在 ZooKeeper基础上进行二次开发的,其中 Leader选举完全使用 ZooKeeper
的底层实现,这样就能很好地在多台中心节点中选举出一台主节点来。
当某一个中心节点失败时,如果该中心节点是 Leader节点,那么就从其他Follower节
点中重新进行选举(这些都是依靠底层ZooKeeper原生支持的),选举出来的 Leader节
点充当主节点作用;如果挂掉的中心节点不是 Leader节点,则不用进行选举,同时所有
和这台失败的机器连接的 Search节点会自动重连到其他中心节点机器,对于后续的读写
请求,则依然交给 Leader节点进行处理,这对于用户来说是透明的,可以说是完成了一
个平滑的恢复。
CenterNode基于 Zookeeper版本的二次开发工作
CenterNode是基于3.4.5版本的 ZooKeeper进行二次开发的,其核心改造点如图6-46所示。
图6-46.终搜基于 ZooKeeper进行二次开发
232第6章 ZooKeeper的典型应用场景

<==========================243end ==============================>
<==========================244start==============================>

从图6-46中,可以看到,中心节点对 ZooKeeper进行的二次开发,主要集中在 CenterNodePeer
和 CenterNodeCnxn这两个类上。
CenterNodePeer
CenterNodePeer类继承自 ZooKeeper中原生的 QuorumPeer类 QuorumPeer
是一个线程类,继承自 Thread,主要负责检测 ZooKeeper服务器状态并触发 Leader
选举。一个QuorumPeer代表了一个 ZooKeeper节点,或者说一个 ZooKeeper进
程。
QuorumPeer线程启动之后,首先会进行 Leader选举在运行期间, QuorumPeer
共有4种可能的状态,分别是 LOOKING、 FOLLOWIN LEADING和 OBSERVING
启动时的初始状态是 LOOKING,表示正在寻找确定新的 Leader服务器。在
ZooKeeper中, Leader选举的默认算法是基于TCP实现的 Fast LeaderElection关于
ZooKeeper的 Leader选举的具体细节,可以参考本书7.6节。
当某一台 ZooKeeper服务器被选举成为 Leader节点后,会调用被 CenterNodePeer重
写了的 setLeader方法,来初始化 CenterNodeCnx服务,这样就完成了正常的调
用逻辑。同样道理,被选举成为 Follower节点或是 Observer节点的 CenterNode,也会调
用对应的Set方法来完成相关逻辑。
当出现因为某些机器宕机了而造成集群需要重新选举 Leader的情况时,首先会调
用对应的Set方法,通过传递NULL参数的方式来标识当前服务要重新选举 Leader,
服务需要暂停, CenterNodeCnxn就会处理一系列的逻辑故障从而恢复逻辑。
所以,在 CenterNodePeer类中,终搜只是重写了 QuorumPeer的 setLeader
(Leader leader) set Follower(Follower follwer)以及 setobserver
(bserver observer)这3个方法,加上终搜服务对应的处理逻辑,就能完成
CenterNode基于 ZooKeeper的二次开发。
CeterNodeCnxn
CeterNodeCnxn类主要就是中心节点对外提供服务的入口类,所有 Search节点
的请求都会先发送到 CenterNodeCnxn类,然后 CenterNode会根据自己是否是
Leader节点来对请求做出相应的处理逻辑。
请求处理
中心节点的请求处理也是参考 ZooKeeper的请求实现的,即 Leader节点负责请求处理,
6.3 ZooKeeper在阿里巴巴的实践与应用233

<==========================244end ==============================>
<==========================245start==============================>

Follower节点负责转发。具体当用户发起一个业务创建请求的时候,处理过程如下:
中心节点收到业务请求,首先会检查自身是否是 Leader角色,如果是 Leader角色,
则进行正常的业务处理;否则把该请求发送到 LeaderLeader节点上去,然后等待
节点返回操作结果,如果 Leader节点长时间未响应或者请求失败,则给请求方返
回异常信息,否则返回正常的业务响应。
应用配置文件管理
在终搜构建索引的过程中,会使用到的关键配置包括 schema.xml和 solrconfig.xml两个
文件,分别定义了索引结构和查询入口,是串联应用和索引之间的桥梁,因此需要为每
个应用定制特有的配置。在终搜中,使用 ZooKeeper对这些配置文件进行了管理,基本
步骤如下。
1.配置初始化
例如,对于某应用App1,首先会在本地根据该应用的结构化特征数据和查询特性
配置好 schemaxml和 solrconfig.xml两个配置文件,然后将这两份配置分别写入
ZooKeeper指定数据节点:
/terminator/terminator-node/[Hostname]/search4App1-0/schema.xml
terminator/terminator--node/[Hostname//4appl-/solrconfig.xml
其中的“[Hostname]”是指该应用的数据内容所在的终搜机器。
2.动态更新配置。
上述配置文件初始化完毕后,应用Appl会到 ZooKeeper指定节点(即上述两个节
点)上获取相关配置,同时注册对这两个节点的“数据变更”Watcher监听这
样,一旦配置文件发生变化,应用就可以实时获取到最新的配置了。
使用 ZooKeeper来实现应用配置文件的管理,能够做到配置的实时性和全局的一致性,
同时解除了应用系统和终搜后台索引系统的耦合,但同时受限于 ZooKeeper数据节点数
据大小的限制,配置文件的配置需要非常精简。
选举机器执行全量任务
在6.1.6节中我们已经讲到,在分布式系统中,有些特别耗费资源(包括网络、CPU和
内存等)的任务,通常只需要选举集群中的一台机器来执行,然后再将执行结果同步给
234第6章 ZooKeeper的典型应用场景

<==========================245end ==============================>
<==========================246start==============================>

集群中的其他机器,这样能够大大提高集群对外的整体服务能力在终搜中,数据的
定时全量DUMP就是这样一个典型的任务。
通常应用会被部署在多台机器上,如果每台机器都进行增量和全量数据导入,那么会存
在多份重复数据,如果只让其中一台机器进行导入操作,那么该机器出现宕机后,导
任务将会终止。因为,基于对容灾的考虑,我们需要解决如下问题:在保证全局执行导
入的机器只有一台的同时,还要在该台机器出现宕机后,保证将有其他机器能够继续执行
下一次的增量和全量导入任务。而解决该问题最好的实现方式便是利用 ZooKeeper的分布
式锁。
1.注册节点。
我们还是以应用Appl为例,在应用启动初始化的时候,会检查 ZooKeeper的指定
节点(该节点是临时节点,下文中我们称该节点为“Master节点”)是否存在:
/terminator/terminator-node/search4Appl-0/full-dump/master
·如果节点不存在,那么就创建该临时节点同时将自己所在的服务器IP地址
写入该节点。
如果节点已经存在,或者是在上述创建过程中出现“被其他机器抢先创建导
致节点创建失败”的现象,那么就对已经存在的节点注册“节点变更”的
Watcher监听。
2.执行任务。
在应用集群开始执行定时全量任务时,会首先访问 ZooKeeper上的 Master节点,
读取出节点的IP信息,如果该IP信息和自身服务器地址一致,则说明自己有执
行全量任务的权限;如果和自身服务器地址不一致,则不进行全量任务。
3. Master选举
在整个系统运行过程中,会出现 Master节点上IP信息对应的服务器出现问题导致
Master节点也随之消失的情况。由于我们在步骤1中已经注册了对该 Master节点
的“节点变更”Watcher监听,因此所有其他机器都会收到通知,于是再次按照步
骤1的逻辑进行节点注册。
服务路由
应用在使用终搜的过程中,初始化阶段需要找到“查询服务”的提供方,我们称这个过
程为服务路由。在传统的方案中,可以使用域名的方式来实现通过分配不同的域名,
6.3 ZooKeeper在阿里巴巴的实践与应用235

<==========================246end ==============================>
<==========================247start==============================>

为其配置不同的IP,而在终搜中是使用 ZooKeeper来完成服务器路由的功能。在一个应
用申请接入的过程中,终搜后台会为其分配查询服务的分组,对应一个集群的机器,并
将这个集群的机器配置到指定节点:
/terminator/terminator-node/search4Appl-O/query-group
应用服务器在启动的过程中,会首先从 ZooKeeper集群上读取出查询服务的分组信息,
并同时对该节点注册“数据变化”通知。另外,客户端还会将从 ZooKeeper上获取到的
数据信息持久化存储到本地文件系统中,以便在出现多次尝试连接 ZooKeeper服务器失
败时,能够使用这份本地的信息。通过这种方式,每个应用就可以动态获取查询服务的
分组信息,完成服务路由,同时也便于终搜的运维人员进行全局运维,提高了实时性。
索引分区
在传统的关系型数据库中,随着数据库数据量的不断增加,单台数据库的存储空间查询
性能已经不能满足业务需求,这时候就需要进行分库操作。在终搜中也同样面临这样的
问题,主要体现在索引上,不断增长的索引量成为了制约查询性能的瓶颈,针对这个问
题,约定俗成的解决方案通常就是将索引进行分区终搜基于 ZooKeeper配置来进行
索引分区。
在终搜中,每次完成全量索引构建后,都会将当前应用的索引分区同步到 ZooKeeper上,
如图6-47所示。
图6-47.终搜索引分区示意图
从图6-47中我们可以看到,该应用的全量索引被分成了0、1和2三个分区,同时每个
分区里面又分配了两台机器来存储索引副本。应用在启动的时候,会首先到 ZooKeeper
节点上获取相应的索引分区,以及每个分区索引副本的服务器地址。
236第6章 ZooKeeper的典型应用场景

<==========================247end ==============================>
<==========================248start==============================>

垂直扩容
所谓垂直扩容,是指为每一个索引分区添加更多的机器以保证分区数据的安全性。假如
一次垂直扩容,添加了一台IP3的机器,那么垂直扩容后 ZooKeeper上的分区如图6-48
所示。
图6-48.索引垂直扩容示意图
水平扩容
水平扩容和垂直扩容非常相近,只是水平扩容是对分区的扩容,因此改动的是 ZooKeeper
上对应的分区节点,如图6-49所示。
4Appt
IP1
2
图6-49.索引水平扩容示意图
图6-49中就是在原来索引分区的基础上,进行了分区扩容,添加了新的分区:3
6.3 ZooKeeper在阿里巴巴的实践与应用237

<==========================248end ==============================>
<==========================249start==============================>

6.3.6案例六实时计算引擎:JStorm
随着互联网大数据技术的不断发展,人们对数据实时性的要求越来越高,传统 Hadoop
的 Map Reduce技术已经逐渐无法满足这些需求,因此实时计算成为了眼下大数据领域
最热门的研究方向之一,出现了诸如 Storm和 JStorm这样的实时计算引擎。 Storm是
Twitter开源的一个高容错的分布式实时计算系统,而JStorm是阿里巴巴集团中间件团
队在 Storm基础上改造和优化的一个分布式实时计算引擎,使用Java语言编写,于2013
年9月正式开源。相较于 Storm, JStorm在功能上更强大,在稳定性和性能上有更卓
越的表现,目前广泛应用于日志分析、消息转化器和统计分析器等一系列无状态的实时
计算系统上。
JStormHadoop是一个类似于 MapReduce的分布式任务调度系统,用户按照指定的接口
编写一个任务程序,然后将这个任务程序提交给 JStorm系统,JStorm会负责7×24小时
运行并调度该任务。在运行过程中如果某个任务执行器( Worker)发生意外情况或其他
故障,调度器会立即分配一个新的 Worker替换这个失效的 Worker来继续执行任务。
JStorm是一个典型的分布式调度系统,其系统整体架构如图6-50所示。
图6-50. JStorm整体架构图
其核心部分由 Nimbus、 Supervisor WorkerTask和 ZooKeeper五部分组成
● Nimbus是任务的中央调度器。
Supervisor作为 Worker的代理角色,负责管理 Worker的生命周期
Worker是Task的容器。
Task对应每一个任务的真正执行体。
● ZooKeeper是整个系统中的协调者。
注9: 9: jstorm : https: //github. com/alibaba/jstorm.的开源站点:htp:github.com/alibabajstorm
238第6章 ZooKeeper的典型应用场景

<==========================249end ==============================>
<==========================250start==============================>

无论是 Storm还是 JStorm,都高度依赖 ZooKeeper来实现诸如同步心跳、同步任务配置
和调度器选举等功能,可以说,如果脱离了 ZooKeeper,这两个实时计算系统都无法正
常工作。
同步心跳
在 JStorm中,需要在集群内部实时同步三种心跳检测。
Worker向 Supervisor汇报心跳。
Supervisor向 Nimbus汇报心跳。
task向 Nimbus汇报心跳
其中后两种心跳检测机制都是通过 ZooKeeper来实现的。
在 JStorm的实现中, Supervisor每隔10秒就会将自己拥有的资源数同步到 ZooKeeper
的/supervisors节点上, Nimbus就可以通过查询这些节点来检测有哪些机器是活着的,
并且能够清楚地知道这些机器上有哪些资源。
而每个Task同样会每隔10秒就将自己的心跳和运行状态同步到 ZooKeeper的/asks节
点上,这样 Nimbus就能够检测到哪些Task是活着的。同时,一旦检测到某个Task的
心跳超时,则会触发 Nimbus对该Task执行 Reassign动作(重新分配任务)。
同步任务配置
在上文中已经提到, JStorm是一个类似于 Hadoop MapReduce的分布式任务调度系统,
用户按照指定的接口编写一个任务程序,然后将这个任务程序提交给 JStorm系统,由
JStorm来负责运行并调度该任务,因此同步任务配置是 JStorm的一大核心功能。整个
同步任务配置过程大体可以分为提交任务和同步 Topology状态两大环节。
提交任务
提交任务的过程如下。
1.客户端提交一个JAR包到 Nimbus
2. Nimbus扫描 ZooKeeper上的/supervisors节点,来获取本集群中的所有资源信息。
3. Nimbus还会扫描 ZooKeeper上的/ assignments节点,来获取已经分配的任务的资
源占用情况。
6.3 ZooKeeper在阿里巴巴的实践与应用239

<==========================250end ==============================>
<==========================251start==============================>

4. Nimbus根据平衡算法,将Task分配到每台机器上,同时确定Task绑定的端口和
资源占用情况(CPUSlotCPUSIot《MemorySIotDiskSlotJ和DiskSlot)。
5.完成任务分配后, Nimbus会将任务的分配结果写入 ZooKeeper的/ assignments节
点。
6. Nimbus还需要设置 Topology的状态为 Active,做法就是在 ZooKeeper上的/topolog
节点下找到以该 Topology的 topology-id命名的对应子节点,并将其设置为 Active
7.重新分配任务。每个Supervisor都会监听 ZooKeeper上的/assignments节点,当检
测到节点发生变更时,就会立即获取本机的任务配置,然后启动或杀死对应的
Worker.
同步 Topology状态
JStorm提供了一系列的命令来控制 Storm服务,这里以客户端的 deactivate命令为例来
说明 JStorm是如何借助 ZooKeeper来同步 Topology状态的。
1.客户端发出 deactivate命令
2. Nimbus在接收到该命令后,会设置 ZooKeeper中的/StormBase节点对应的
Topology的状态为 deactivate
3.同时, Worker进程会对 Zookeeper中的StormBase节点注册监听,当节点发生变
更时,立即设置 Worker的状态为 deactivate
4. Worker内部的Task每执行一个 batch操作后,就会检查 Worker的状态,如果状
态变更为 deactivate,那么ask就会立即将自己置为挂起状态。
调度器选举
Storm相比, JStorm中增加了调度器的HA机制,用于实现调度器的动态选举。每一
个 Nimbus在启动的时候,都会试图到 ZooKeeper上创建一个临时节点/nimbusmaster
在创建的过程中,如果发现该节点已经存在,则表示 Nimbus的 Master已经存在,那么
当前 Nimbus就会在 ZooKeeper的/nimbusslave节点下创建一个临时子节点,并将自己
的机器名和端口号写入到该节点中,同时注册对/nimbus master节点的监听。
在运行过程中,该 Nimbus(这里指创建nimbus slave节点对应的机器)还会启动一个
Follower线程,用于:
240第6章 ZooKeeper的典型应用场景

<==========================251end ==============================>
<==========================252start==============================>

·反复扫描/nimbusmaster是否存在。
·如果nimbus master节点存在,则同步/ nimbusmaster Topology到本机中
·如果/nimbus master节点已经消失,则会触发调度器的重新选举,具体流程和上面
提到的初始化流程是一致的,简单地讲就是集群中所有机器都去创建
nimbusmaster节点,如果节点创建成功,那么该机器就是 Master,创建失败,那
么就是 Slave
ZooKeeper使用优化
JStormStorm是从中改造而来的,在使用 ZooKeeper方面也进行了大量的改进与优化。
减少对 Zookeeper的全量扫描
在 Storm中,判断一个Task是否存活的方法非常复杂,首先会通过扫描/StormBase节点
来获取 TopgyTopology列表,将存活的提取出来:然后扫描/assignments节点,获取
每一个Task的任务配置,然后以此来判断Tsk是否存活。相信读者很容易发现,在整
个过程中,几乎扫描了整个 ZooKeeper上的数据节点,这显然增加了 ZooKeeper的压力
而在 JStorm中,判断一个Task是否存活的方法只需要扫描/中该 Topology的节点,
通过对心跳时间进行判断即可。
减少无用的 Watcher操作
·在JStorm中, Nimbus取消了对/supervisors节点的 Watcher操作,因为增加或减少
Supervisor没有必要触发 Rebalance动作,而 Storm的设计却画蛇添足地触发了
Rebalance动作,直到0.9.0版本后, Storm官方才取消了该 Rebalance动作。
·在JStorm中, Supervisor取消了对/StormBase节点的 Watcher操作, Supervisor只
需监听/assignments节点即可,没有必要重复性地监听/StormBase节点举个例子,
假如有200台机器,那么后者至少额外增加了200多次的 Watcher通知。
延长心跳设置
Task的心跳频率,由原来的3秒改为了10秒。这个 JStorm改动使得对 ZooKeeper
的压力减轻了许多:
在 JStorm中,每一台机器上通常会运行20多个 Worker,假设当集群的规模上升
到200台时,整个集群可能运行着5000个以上的Task,这样就会造成对 ZooKeeper
每秒至少1600次的心跳请求。同时,每一个Task心跳包大小为200多个字节,
因此,将Task的心跳频率延长到10秒,可以明显减轻对 ZooKeeper的压力
6.3 ZooKeeper在阿里巴巴的实践与应用241

<==========================252end ==============================>
<==========================253start==============================>

增加 ZooKeeper的 Timeout重连次数。
在 Strom中,当失去与 ZooKeeper连接的时候会进行5次重连操作。但在实际运
行过程中, ZooKeeper很容易在某个瞬间处于无应答的状态,一旦Storm连续5
次请求连接 ZooKeeper失败后, Nimbus、 Supervisor和 Worker就会自动退出,而
如果 Nimbus自动退出,就很容易导致集群丧失中央调度器功能。而在大部分的
情况下, ZooKeeper只是短暂地处于无应答状态,一段时间后就会恢复正常。因
此,增加重试次数,可以明显降低 Supervisor, Nimbus和 Worker的自动退出概率。
小结
ZooKeeper是一个高可用的分布式数据管理与系统协调框架。基于对ZAB算法的实现,
该框架很好地保证了分布式环境中数据的一致性。也正是基于这样的特性,使得
ZooKeeper成为了解决分布式一致性问题的利器。随着近年来互联网系统规模的不断扩
大,大数据时代飞速到来,越来越多的分布式系统将 ZooKeeper作为核心组件使用,如
Hadoop、 Hbase和 Kafka等因此,正确地理解 ZooKeeper的应用场景,对于研发人员
来说,显得尤为重要。
本章首先从数据发布/订阅、负载均衡、命名服务、分布式通知/协调、集群管理、 Master
选举、分布式锁和分布式队列等这些分布式系统中常见的应用场景展开,从理论上向读
者讲解了 ZooKeeper的最佳实践,同时结合 Hadoop、 HBase和 Kafka等这些大型分布式
系统以及阿里巴巴的一系列开源系统,向读者展现了如何借助 ZooKeeper解决实际生产
中的分布式问题。
242第6章 ZooKeeper的典型应用场景

<==========================253end ==============================>
<==========================254start==============================>

第7章
ZooKeeper技术内幕
好了,到现在为止,在学习了前面几章的内容之后,相信读者已经能够在应用中很好地
使用 ZooKeeper了。尤其在数据发布/订阅、负载均衡、命名服务、分布式协调通知、
集群管理、 Master选举、分布式锁以及分布式队列等分布式场景中,能够很好地利用
ZooKeeper来解决实际的分布式问题了。
当然,相信读者也一定对 ZooKeeper内部如何做到分布式数据一致性而感到好奇。在本
章中,我们将从系统模型、序列化与协议、客户端工作原理、会话、服务端工作原理以
及数据存储等方面来向读者揭示 ZooKeeper的技术内幕,帮助读者更深入地了解
ZooKeeper这一分布式协调框架。
7.1系统模型
在本节中,我们首先将从数据模型、节点特性、版本、 Watcher和ACL五方面来讲述
ZooKeeper的系统模型。
7.1.1数据模型
ZooKeeper的视图结构和标准的Unix文件系统非常类似,但没有引入传统文件系统中目
录和文件等相关概念,而是使用了其特有的“数据节点”概念,我们称之为 ZNode ZNode
是 ZooKeeper数据的最小单元,每个 ZNode上都可以保存数据,同时还可以挂载子节
点,因此构成了一个层次化的命名空间,我们称之为树。
树
首先我们来看图7-1所 ZooKeeper示的数据节点示意图,从而对 ZooKeeper上的数据节
243

<==========================254end ==============================>
<==========================255start==============================>

点有一个大体上的认识。在 ZooKeeper中,每一个数据节点都被称为一个 ZNode,所有
ZNode按层次化结构进行组织,形成一棵树。 ZNode的节点路径标识方式和Unix文件
系统路径非常相似,都是由一系列使用斜杠()进行分割的路径表示,开发人员可以向
这个节点中写入数据,也可以在节点下面创建子节点。
/app2
/appl/ /app1/c2 /appl/c3
图7-1. Zookeeper数据模型
事务D
在《事务处理:概念与技术》一书中提到,事务是对物理和抽象的应用状态上的操作集
合。在现在的计算机科学中,狭义上的事务通常指的是数据库事务,一般包含了一系列
对数据库有序的读写操作,这些数据库事务具有所谓的ACD特性,即原子性(Atomic)、
一致性(Consistency)、隔离性(Isolation)和持久性(Durability)
在 EZooKeeper中,事务是指能够改变 ZooKeeper服务器状态的操作,我们也称之为事务
操作或更新操作,一般包括数据节点创建与删除、数据节点内容更新和客户端会话创建
与失效等操作。对于每一个事务请求, ZooKeeper都会为其分配一个全局唯一的事务ID,
用ZXID来表示,通常是一个64位的数字每一个ZXID对应一次更新操作,从这些
ZXID中可以间接地识别出 ZooKeeper处理这些更新操作请求的全局顺序。
7.1.2节点特性
在上一节中,我们已经了解到, ZooKeeper的命名空间是由一系列数据节点组成的,在
本节中,我们将对数据节点做详细讲解。
节点类型
在 ZooKeeper中,每个数据节点都是有生命周期的,其生命周期的长短取决于数据节点
的节点类型。在 ZooKeeper中,节点类型可以分为持久节点(PERSISTENT)、临时节点
(EPHEMERAL)和顺序节点(SEQUENTIAL三大类,具体在节点创建过程中,通过
组合使用,可以生成以下四种组合型节点类型:
244第7章 ZooKeeper技术内幕

<==========================255end ==============================>
<==========================256start==============================>

持久节点(PERSISTENT)
持久节点是 ZooKeeper中最常见的一种节点类型。所谓持久节点,是指该数据节点被创
建后,就会一直存在于 ZooKeeper服务器上直到有删除操作来主动清除这个节点。
持久顺序节点(PERSISTENT_SEQUENTIAL)
持久顺序节点的基本特性和持久节点是一致的,额外的特性表现在顺序性上。在
ZooKeeper中,每个父节点都会为它的第一级子节点维护一份顺序,用于记录下每个子
节点创建的先后顺序。基于这个顺序特性,在创建子节点的时候,可以设置这个标记,
那么在创建节点过程中, ZooKeeper会自动为给定节点名加上一个数字后缀,作为一个
新的、完整的节点名。另外需要注意的是,这个数字后缀的上限是整型的最大值。
临时节点(EPHEMERAL)
和持久节点不同的是,临时节点的生命周期和客户端的会话绑定在一起,也就是说,如
果客户端会话失效,那么这个节点就会被自动清理掉。注意,这里提到的是客户端会话
失效,而非TCP连接断开。关于 ZooKeeper客户端会话和连接,将在7.4节中做详细讲
解。另外, ZooKeeper规定了不能基于临时节点来创建子节点,即临时节点只能作为叶
子节点。
临时顺序节点(EPHEMERAL_SEQUENTIAL)
临时顺序节点的基本特性和临时节点也是一致的,同样是在临时节点的基础上,添加了
顺序的特性。
状态信息
在7.1.1节中,我们提到可以针对 ZooKeeper上的数据节点进行数据的写入和子节点的
创建。事实上,每个数据节点除了存储了数据内容之外,还存储了数据节点本身的一些
状态信息。在5.2.2节中,我们介绍了如何使用get命令来获取一个数据节点的内容,
如图7-2所示。
Thost (CONNECTE get /YINSHT MONTTOR. AL TVE. CRECK
Thu Noy0310:36:24cst2011
wed oct2320:54:20cst2013
p2xid0x5013at163
cversion ss43as1
dataversion 8543951
ephemeralowner0x0
nunch idren
caThost:2181(CONNECTED)
图7-2.命令行获取节点信息
7.1系统模型245

<==========================256end ==============================>
<==========================257start==============================>

从图7-2所示的返回结果中,我们可以看到第一行是当前数据节点的数据内容,从第
二行开始就是节点的状态信息了,这其实就是数据节点的Stat对象的格式化输出,
图7-3展示了 ZooKeeper中stat类的数据结构。
Stat
rs ton: long
ersion: long
ersion:long
ephemeralowner: long
-dataLength: int
-numchildren: int
pDid: long
tring tag
+deserialize(nputArchive a_ String tagr vold
图7-3.Stat类图
从图7-3中可以看到,Stat类中包含了 ZooKeeper上一个数据节点的所有状态信息,
包括事务ID、版本信息和子节点个数等,表-1中对所有这些属性进行了说明。
表7-1.stat对象状态属性说明
状态属性
说明
czxid
即 Created ZXID,表示该数据节点被创建时的事务ID
mzxid
即 Modified ZXID,表示该节点最后一次被更新时的事务ID
ctime
即 Created Time,表示节点被创建的时间
mtime
即 Modified Time,表示该节点最后一次被更新的时间
version
数据节点的版本号。关于 ZooKeeper中版本相关的内容,将在7.1.3节
中做详细讲解
cversion
子节点的版本号
aversion
节点的ACL版本号
ephemeralOwner
创建该临时节点的会话的 sessionID。如果该节点是持久节点,那么这
个属性值为0
dataLength
数据内容的长度
numChildren
当前节点的子节点个数
pzxid
表示该节点的子节点列表最后一次被修改时的事务D注意,只有子
节点列表变更了才会变更 pzxid,子节点内容变更不会影响 pzxid
7.1.3版本保证分布式数据原子性操作
ZooKeeper中为数据节点引入了版本的概念每个数据节点都具有三种类型的版本信息,
对数据节点的任何更新操作都会引起版本号的变化,表7-2中对这三类版本信息分别进
行了说明。
246第7章 ZooKeeper技术内幕

<==========================257end ==============================>
<==========================258start==============================>

表7-2.数据节点版本类型说明
ers本类型
说明
version
当前数据节点数据内容的版本号
cversion
当前数据节点子节点的版本号
aversion
当前数据节点ACL变更版本号
ZooKeeper中的版本概念和传统意义上的软件版本有很大的区别,它表示的是对数据节
点的数据内容、子节点列表,或是节点AcL信息的修改次数,我们以其中的 version
这种版本类型为例来说明。在一个数据节点/zk-book被创建完毕之后,节点的 version
值是0,表示的含义是“当前节点自从创建之后,被更新过0次”。如果现在对该节点的
数据内容进行更新操作,那么随后, version的值就会变成1同时需要注意的是,在
上文中提到的关于 version的说明,其表示的是对数据节点数据内容的变更次数,强
调的是变更次数,因此即使前后两次变更并没有使得数据内容的值发生变化,version
的值依然会变更。
在上面的介绍中,我们基本了解了 ZooKeeper中的版本概念那么版本究竟用来干嘛呢?
在讲解版本的作用之前,我们首先来看下分布式领域中最常见的一个概念锁。
一个多线程应用,尤其是分布式系统,在运行过程中往往需要保证数据访问的排他性。
例如在最常见的车站售票系统上,在对系统中车票“剩余量”的更新处理中,我们希望
在针对某个时间点的数据进行更新操作时(这可能是一个极短的时间间隔,例如几秒或
几毫秒,甚至是几纳秒,在计算机科学的有些应用场景中,几纳秒可能也算不上太短的
时间间隔),数据不会因为其他人或系统的操作再次发生变化。也就是说,车站的售票
员在卖票的过程中,必须要保证在自己的操作过程中,其他售票员不会同时也在出售这
个车次的车票。
为保证上面这个场景的正常运作,一种可能的做法或许是这样,车站某售票窗口的售票
员突然向其他售票员大喊一声:“现在你们不要出售杭州到北京的XX次车票!”然后
当他售票完毕后,再次通知大家:“该车次已经可以售票啦!”
当然在现实生活中,不会依靠这么原始的人工方式来实现数据访问的排他性,但这个例
子给我们的启发是:在并发环境中,我们需要通过一些机制来保证这些数据在某个操作
过程中不会被外界修改,我们称这样的机制为“锁”。在数据库技术中,通常提到的“悲
观锁”和“乐观锁”就是这种机制的典型实现。
悲观锁,又被称作悲观并发控制(Pessimistic Concurrency Control,CC),是数据库中
一种非常典型且非常严格的并发控制策略悲观锁具有强烈的独占和排他特性,能够有
效地避免不同事务对同一数据并发更新而造成的数据一致性问题。在悲观锁的实现原理
7.1系统模型247

<==========================258end ==============================>
<==========================259start==============================>

中,如果一个事务(假定事务A)正在对数据进行处理,那么在整个处理过程中,都会
将数据处于锁定状态,在这期间,其他事务将无法对这个数据进行更新操作,直到事务
A完成对该数据的处理,释放了对应的锁之后其他事务才能够重新竞争来对数据进行
更新操作。也就是说,对于一份独立的数据,系统只分配了一把唯一的钥匙,谁获得了
这把钥匙,谁就有权力更新这份数据。一般我们认为,在实际生产应用中,悲观锁策略
适合解决那些对于数据更新竞争十分激烈的场景在这类场景中,通常采用简单粗暴
的悲观锁机制来解决并发控制问题。
乐观锁,又被称作乐观并发控制(Optimistic Concurrency Control,occ),也是一种常
见的并发控制策略。相对于悲观锁而言,乐观锁机制显得更加宽松与友好。从上面对悲
观锁的讲解中我们可以看到,悲观锁假定不同事务之间的处理一定会出现互相干扰,从
而需要在一个事务从头到尾的过程中都对数据进行加锁处理。而乐观锁则正好相反,它
假定多个事务在处理过程中不会彼此影响,因此在事务处理的绝大部分时间里不需要进
行加锁处理。当然,既然有并发,就一定会存在数据更新冲突的可能。在乐观锁机制中,
在更新请求提交之前,每个事务都会首先检查当前事务读取数据后,是否有其他事务对
该数据进行了修改。如果其他事务有更新的话,那么正在提交的事务就需要回滚。乐观
锁通常适合使用在数据并发竞争不大、事务冲突较少的应用场景中。
从上面的讲解中,我们其实可以把一个乐观锁控制的事务分成如下三个阶段:数据读取、
写入校验和数据写入,其中写入校验阶段是整个乐观锁控制的关键所在。在写入校验阶
段,事务会检查数据在读取阶段后是否有其他事务对数据进行过更新,以确保数据更新
的一致性。那么,如何来进行写入校验呢?我们首先可以来看下JDK中最典型的乐观
锁实现CAS。在5.3.5节中,我们已经对CAS理论有过阐述,简单地讲就是“对于
值V,每次更新前都会比对其值是否是预期值A,只有符合预期,才会将V原子化地更
新到新值B”,其中是否符合预期便是乐观锁中的“写入校验”阶段。
好了,现在我们再回过头来看看ZooKeeper中版本的作用。事实上,在 ZooKeeper中,
version属性正是用来实现乐观锁机制中的“写入校验”的。在5.3.5节中,我们已经
详细地讲解了如何正确地使用 version属性来实现乐观锁机制,在这里我们重点看下
ZooKeeper的内部实现。在 ZooKeeper服务器的 PrepRequestP rocessor处理器类中,
在处理每一个数据更新(setDataRequest)请求时,会进行如清单7-1所示的版本检
清单7-1. setData请求版本检查
399 version setDataRequest.getVersion (
400 int currentVersion= nodeRecord.stat. getVersion()
401
if (version !=-1 & version ! currentVersion){
402
throw new KeeperException. BadVersionExcep(path);
248第 ZooKeeper7章技术内幕

<==========================259end ==============================>
<==========================260start==============================>

403
404 version= currentVersion+1
从上面的执行逻辑中,我们可以看出,在进行一次 setDataRequest请求处理时,首先进
行了版本检查: ZooKeeper会从 setDataRequest请求中获取到当前请求的版本,
同时从数据记录 nodeRecord中获取到当前服务器上该数据的最新版本 currentVersion
如果 version为“-1”,那么说明客户端并不要求使用乐观锁,可以忽略版本比对;如果
version不是“-1”,那么就比对 version和 currentVers,如果两个版本不匹配,
那么将会抛出 BadVersionException异常
7.1.4 Watche数据变更的通知
在6.1.1节中,我们已经提到, ZooKeeper提供了分布式数据的发布/订阅功能。一个典
型的发布/订阅模型系统定义了一种一对多的订阅关系,能够让多个订阅者同时监听某一
个主题对象,当这个主题对象自身状态变化时,会通知所有订阅者,使它们能够做出相
应的处理。在 ZooKeeper中,引入了 Watcher机制来实现这种分布式的通知功能。
ZooKeeper允许客户端向服务端注册一个 Watcher监听,当服务端的一些指定事件触发
了这个 Watcher,那么就会向指定客户端发送一个事件通知来实现分布式的通知功能。
整个 Watcher注册与通知过程如图7-4所示。
ZooKeeper
1.注册
3通知
Client
2存WatchManager
图7-4. Watcher机制概述
从图7-4中,我们可以看到, ZooKeeper的 Watcher机制主要包括客户端线程、客户端
WatchManager和 ZooKeeper服务器三部分。在具体工作流程上,简单地讲,客户端在
向 ZooKeeper服务器注册 Watcher的同时,会将 Watcher对象存储在客户端的
WatchManager中.当 ZooKeeper服务器端触发 Watcher事件后,会向客户端发送通知,
客户端线程从 WatchManager中取出对应的 Watcher对象来执行回调逻辑
7.1系统模型|249

<==========================260end ==============================>
<==========================261start==============================>

Watcher接口
在 ZooKeeper中,接口类 Watcher用于表示一个标准的事件处理器,其定义了事件通
知相关的逻辑,包含 KeeperState和 EventType两个枚举类,分别代表了通知状态
和事件类型,同时定义了事件的回调方法: processWatchedEvent event
Watcher事件
同一个事件类型在不同的通知状态中代表的含义有所不同,表7-3列举了常见的通知状
态和事件类型。
表7-3. Watcher通知状态与事件类型一览
KeeperState
EventType
触发条件说明
None
客户端与服务器成功建
(-1)
立会话
NodeCreated
Watcher监听的对应数
(1)
据节点被创建
NodeDeleted
Watcher监听的对应数
SyncConnected (2)
据节点被删除
此时客户端和服务
(3)
NodeDataChanged
Watcher监听的对应数器处于连接状态
(3)
据节点的数据内容发生变
更
Watcher监听的对应数
NodeChildrenChanged
(4)
据节点的子节点列表发生
变更
Disconnected None
客户端与 ZooKeeper服此时客户端和服务
(⊙)
(-1)
务器断开连接
器处于断开连接状态
生
此时客户端会话失
Expired
None
效,通常同时也会收到
(-112)
(-1)
会话超时
SessionExpiredEx
ception异常
通常有两种情况:
AuthFailed None
使用错误的 scheme进通常同时也会收到
(4)
(-1)
行权限检查
AuthFailedExcept
SASL权限检查失败ion异常
Unknown
(-1)
从3.1.0版本开始已
AeSyncCennected
废弃
(1)
表7-3中列 ZooKeeper举了中最常见的几个通知状态和事件类型。其中,针对
NodeDataChanged事件,在5.3.5节中也有提到,此处说的变更包括节点的数据内容
和数据的版本号 dataVersion。因此,即使使用相同的数据内容来更新,还是会触发
这个事件通知,因为对于 ZooKeeper来说,无论数据内容是否变更,一旦有客户端调用
了数据更新的接口,且更新成功,就会更新 dataVersion值。
250第7章 ZooKeeper技术内幕

<==========================261end ==============================>
<==========================262start==============================>

NodeChildrenChanged事件会在数据节点的子节点列表发生变更的时候被触发,这
里说的子节点列表变化特指子节点个数和组成情况的变更,即新增子节点或删除子节点,
而子节点内容的变化是不会触发这个事件的。
对于 AuthFailed这个事件,需要注意的地方是,它的触发条件并不是简简单单因为
当前客户端会话没有权限,而是授权失败。我们首先通过清单7-2和清单7-3所示的两
个例子来看看 AuthFailed这个事件
清单7-2.使用正确的 Scheme进行授权
zkClient= new ZooKeeper( SERVERLIST,3000, new Sample_ AuthFailed1())
zkClient. addAuthInfo"digest","taokeeper:true".getBytes())
zkClient. create("/zk-book",". getBytes(),acls, CreateMode. EPHEMERAL)
zkclient_error new Zookeeper( SERVER_LIST, 3000, new Sample_AuthFailed1() )
zkClient_error. addAuthInfo( "digest", "taokeeper:error". getBytes())
zkClient_error.getData( "/zk-book", true, null
清单7-3.使用错误的 Scheme进行授权
zkClient= new ZooKeeper( SERVERLIST,3000, new SampleAuthFailed2())
zkClient. addAuthInfo"digest","taokeeper:true". getBytes())
zkClient. create( "/zk-book", "".getBytes(), acls, CreateMode. EPHEMERAL
zkClient_error new ZooKeeper( SERVER_LIST, 3000, new Sample_AuthFailed2() )
zkClient_error. addAuthInfo( "digest2", "taokeeper: error". getBytes())
zkClient_error. getData( "/zk-book", true, null )
上面两个示例程序都创建了一个受到权限控制的数据节点,然后使用了不同的权限
Scheme进行权限检查。在第一个示例程序中,使用了正确的权限 Scheme: digest
而第二个示例程序中使用了错误的 Scheme: digest2另外,无论哪个程序,都使用
了错误的Auth: taokeeper: error,因此在运行第一个程序的时候,会抛出
NoAuthException异常,而第二个程序运行后,抛出的是 AuthFailedException
异常,同时,会收到对应的 Watcher事件通知:(AuthFailed,none)关于这两个示
例的完整程序,可以到本书对应的源代码包中获取,包名为book. chapter007.714
回调方法 process()
processWatcher方法是接口中的一个回调方法,当 ZooKeeper向客户端发送一个
Watcher事件通知时,客户端就会对相应的 process方法进行回调,从而实现对事件的
处理。 process方法的定义如下:
abstract public void process(WatchedEvent event);
这个回调方法的定义非常简单,我们重点看下方法的参数定义: WatchedEvent
7.1系统模型251

<==========================262end ==============================>
<==========================263start==============================>

WatchedEvent包含了每一个事件的三个基本属性:通知状态(keeperState)、事
件类型(eventType)和节点路径(path),其数据结构如图7-5所示。 ZooKeeper使
用 WatchedEvent对象来封装服务端事件并传递给 Watcher,从而方便回调方法
process对服务端事件进行处理。
WatchedEvent
-keeperState:KeeperState
eventType: EventType
path: String
+getwrapper(): WatcherEvent
图7-5. WatchedEvent类图
提到 WatchedEvent,不得不讲下 WatcherEvent实体笼统地讲,两者表示的是同
一个事物,都是对一个服务端事件的封装。不同的是, WatchedEvent是一个逻辑事
件,用于服务端和客户端程序执行过程中所需的逻辑对象,而 WatcherEvent因为实
现了序列化接口,因此可以用于网络传输,其数据结构如图7-6所示。
WatcherEvent
-type:int
-state:int
-path:String
+serialize(OutputArchive a_ String tag): void
图7-6. WatcherEvent类图
服务端在生成 WatchedEvent事件之后,会调用 getWrapper方法将自己包装成一个
可序列化的 WatcherEvent事件,以便通过网络传输到客户端。客户端在接收到服务
端的这个事件对象后,首先会将 WatcherEvent事件还原成一个 WatchedEvent事
件,并传递给 process方法处理,回调方法 process根据入参就能够解析出完整的
服务端事件了。
需要注意的一点是,无论是 WatchedEvent还是 WatcherEvent,其对 ZooKeeper服
务端事件的封装都是极其简单的。举个例子来说,当zkbok这个节点的数据发生变更
时,服务端会发送给客户端一个“ZNode数据内容变更”事件,客户端只能够接收到如
下信息:
KeeperState: SyncConnected
EventType: NodeDataChanged
Path:/zk-book
252第7章 ZooKeeper技术内幕

<==========================263end ==============================>
<==========================264start==============================>

从上面展示的信息中,我们可以看到,客户端无法直接从该事件中获取到对应数据节点
的原始数据内容以及变更后的新数据内容,而是需要客户端再次主动去重新获取数
据这也是 ZooKeeper Watcher机制的一个非常重要的特性。
工作机制
ZooKeeper的 Watcher机制,总的来说可以概括为以下三个过程:客户端注册 Watcher
服务端处理 Watcher和客户端回调 Watcher,其部各组件之间的关系如图7-7所示。
ZooKeeper
-w atchManager: ZKWatchManager
clientWatchM anager
+materialize(: SeteWatcher>
WatchRegistr
watcher: Watcher
+ge
int): Maps
register( ot re) void
etWatches(int rc)
ClientCnxn
Pack
q
ListcPacket
LinkedListPackep>
and Thread
entThread: Even Thread
submRequesto
.ereate60 void
sendThread resdResponse0
图7-7. Watcher相关uml
客户端注册 Watcher
在5.3.1节中,我们提到在创建一个 ZooKeeper客户端对象实例时,可以向构造方法中
传入一个默认的 Watcher:
public ZooKeeper(String connectstring, int sessionTimeout, Watcherwatcher);
这个 Watcher将作为整个 ZooKeeper会话期间的默认 Watcher,会一直被保存在客户端
ZKWatchManager的 defaultwatcher中另外, ZooKeeper客户端也可以通过
7.1系统模型253

<==========================264end ==============================>
<==========================265start==============================>

getData getchildren和 exist三个接口来向 ZooKeeper服务器注册 Watcher,无
论使用哪种方式,注册 Watcher的工作原理都是一致的,这里我们以 getData这个接
口为例来说明。 getData接口用于获取指定节点的数据内容,主要有两个方法:
public byte[] getData(String path, boolean watch, Stat stat)
public byte[] getData(final String path, Watcher watcher, Stat stat)
在这两个接口上都可以进行 Watcher的注册,第一个接口通过一个 boolean参数来标
识是否使用上文中提到的默认 Watcher来进行注册,具体的注册逻辑和第二个接口是一致
的。
在向 getData接口注册 Watcher后,客户端首先会对当前客户端请求 request进行标记,
将其设置为“使用Watcher监听”,同时会封装一个 Watcher的注册信息 WatchRegistration
对象,用于暂时保存数据节点的路径和 Watcher的对应关系,具体的逻辑代码如下:
public Stat getData(final String path, Watcher watcher, Statstat)
{
WatchRegistration wcb null;
if (watcher ! null){
wcb= new DataWatchRegistration(watcher, clientPath);
}
request. setwatch(watcher ! null);
ReplyHeader= cnxn. submitRequesth, request, response, wcb);
在 ZooKeeper中, Packet可以被看作一个最小的通信协议单元,用于进行客户端与服
务端之间的网络传输,任何需要传输的对象都需要包装成一个 Packet对象。因此,在
ClientCnxn中 WatchRegistration又会被封装到 Packet中去,然后放入发送队
列中等待客户端发送:
Packet queuePacket(RequestHeader h, ReplyHeader r, Record request,
Record response, AsyncCallback cb, String clientPath,
String serverPath, Object ctx,WatchRegistration watchRegistration){
Packetpacket =null;
synchronized (outgoingQueue)
packet new Packet(, r, request response, watchRegistration);
outgoingQueue. add(packet);
254第7章 ZooKeeper技术内幕

<==========================265end ==============================>
<==========================266start==============================>

随后, ZooKeeper客户端就会向服务端发送这个请求,同时等待请求的返回。完成请求发送
后,会由客户端 SendThread线程的 readResponse方法负责接收来自服务端的响应,
finishPacket方法会从 Packet中取出对应的 Watcher并注册到 ZKWatchManager中去:
private void finishPacket(Packet p){
if (p. watchRegistration ! null)
p. watchRegistration. register(p. replyHeade.getErr());
从上面的内容中,我们已经了解到客户端已经将 Watcher暂时封装在了 WatchRegistration
对象中,现在就需要从这个封装对象中再次提取出 Watcher来:
protected Map<String, Set<watcher>> getWatches(int rc){
return watchManager. datawatches;
public void register(int rc)
if (shouldAddwatch()){
Map<String, Set<watcher>> watches getwatches(rc);
synchronized(watches){
set<watcher> watchers watches.getclientPath);
if(watchersnull)
watchers new HashSet<Watcher>()
watches. put(clientPath, watchers);
watchers. add(watcher);
在 register方法中,客户端会将之前暂时保存的 Watcher对象转交给 ZKWatchManager,
并最终保存到 datawatches中去 ZKWatchManager. datawatches是一个
Map<String,setWatcher>>类型的数据结构,用于将数据节点的路径和 Watcher对象进
行一一映射后管理起来。整个客户端 Watcher的注册流程如图7-8所示。
7.1系统模型255

<==========================266end ==============================>
<==========================267start==============================>

用户端AP
传入 Watcherx对象
1标记request
2封 Watcher装到
WatchRegistration
服务端发送
request
峡应是否成功
Watcheri管到
KWatchManager中进
请求返回
行管
图7-8.客户端 Watcher注册流程图
通过上面的讲解,相信读者已经对客户端的 Watcher注册流程有了一个大概的了解。但
同时我们也可以发现,极端情况下,客户端每调用一次 getData()接口,就会注册上
一个 Watcher,那么这些 Watcher实体都会随着客户端请求被发送到服务端去吗?
答案是否定的。如果客户端注册的所有 Watcher都被传递到服务端的话,那么服务端肯定会
出现内存紧张或其他性能问题了,幸运的是,在 ZooKeeper的设计中充分考虑到了这个问题。
在上面的流程中,我们提到把 WatchRegistration封装到了 Packet对象中去,但事实
上,在底层实际的网络传输序列化过程中,并没有将 WatchRegistration对象完全地序
列化到底层字节数组中去。为了证实这一点,我们可以看下 Packet内部的序列化过程:
public void createBB()
try
ByteArrayOutputStream baos= new ByteArrayOutputStream()
BinaryOutputArchive boa= BinaryOutputArchi. getArchive(baos);
boa.writeInt(-1, "Len") / We'll fill this in later
if(requestHeader=null){
requestHeader. serialize(boa, "header");
if (request instanceof ConnectRequest){
request.serialize(boa, "connect");
1/ append "am-I-allowed-to-be-readonly"flag
boa. writeBool(readonly, "readonly");
else if (request ! null){
request. serialize(boa, "request");
256第 ZooKeeper7章技术内幕

<==========================267end ==============================>
<==========================268start==============================>

从上面的代码片段中,我们可以看到,在 Packet. createBB()方法中,ZooKeeper
只会将 requestHeader和 request两个属性进行序列化,也就是说,尽管
WatchRegistration被封装在了 Packet中,但是并没有被序列化到底层字节数组
中去,因此也就不会进行网络传输了。
服务端处理 Watcher
上面主要讲解了客户端注册 Watcher的过程,并且已经了解了最终客户端并不会将
Watcher对象真正传递到服务端。那么,服务端究竟是如何完成客户端的 Watcher注册,
又是如何来处理这个 Watcher的呢?本节将主要围绕这两个问题展开进行讲解。
ServerCnxn存储
我们首先来看下服务端接收 Watcher并将其存储起来的过程,如图7-9所示是
ZooKeeper服务端处理 Watcher的序列图
户端R
3.getData
bytel]
byten
图7-9.服务端处理 Watcher的序列图
从图7-9中我们可以看到,服务端收到来自客户端的请求之后,在 FinalRequest
Processor. processRequest()中会判断当前请求是否需要注册 Watcher:
case OpCode.getData:{
byte]=zks. get ZKDatabase).getData(getDataRequest. getPath(),stat,
getDataRequest. getwatch)cnxn:null);
rsp new GetDataResponse(b, stat);
break;
从 getData请求的处理逻辑中,我们可以看到,当 getDataRequest. getWatch()
7.1系统模型257

<==========================268end ==============================>
<==========================269start==============================>

为true的时候, ZooKeeper就认为当前客户端请求需要进行 Watcher注册,于是就会将当前
的 ServerCnxn对象和数据节点路径传入 getData方法中去。那么为什么要传入
ServerCnxn呢? ServerCnxnZooKeeper是一个客户端和服务器之间的连接接口,代表
了一个客户端和服务器的连接。 ServerCnxn接口的默认实现是 NIOServerCnxn,同时
从3.4.0版本开始,引入了基于 Netty的实现: NettyServerCnxn无论采用哪种实现方
式,都实现了 Watcher的 process接口,因此我们可以把 ServerCnxn看作是一个 Watcher
对象。数据节点的节点路径和 ServerCnxn最终会被存储在 WatchManager的
watchTable和 watch2Paths2中
WatchManager是 ZooKeeper服务端 Watcher的管理者,其内部管理的 watchTable
和 watchPaths2两个存储结构,分别从两个维度对 Watcher进行存储
watchTableWatcher是从数据节点路径的粒度来托管
watch2Paths2是从 Watcher的粒度来控制事件触发需要触发的数据节点。
同时, WatchManager还负责 Watcher事件的触发,并移除那些已经被触发的 Watcher注
意, WatchManager只是一个统称,在服务端, DataTree中会托管两个 WatchManager,
分别是 dataWatches和 childWatches,分别对应数据变更 Watcher和子节点变更 Watcher
在本例中,因为是getData接口,因此最终会被存储在 datawatches中,其数据结构如
图7-10所示。
WatchManager
-watchTable: HashMap-<String. HashSetWatchep>
-watch2Paths: HashMap<
HashSet<Strings>
+addwatch(String, Watcher): void
removeWatcher(Watcher): void
triggerWatch(String. EventType) Set<watcher
+triggerWatch(String. EventType, SetWatcher>): Set<Watchep
图7-10.7-10《WatchManage数据结构
Watcher触发
在上面的讲解中,我们了解了对于标记了 Watcher注册的请求, ZooKeeper会将其
对应的 ServerCnxn存储到 WatchManager中,下面我们来看看服务端是如何触
发 Watcher的。在表7-3中我们提到, NodeDataCh事件的触发条件是
“Watcher监听的对应数据节点的数据内容发生变更”,其具体实现如下:
public Stat setData(String path, byte data[, int version, long zxid,
long time) throws KeeperException. NoNodeException
Stat= new Stat()
258第7章 ZooKeeper技术内幕

<==========================269end ==============================>
<==========================270start==============================>

DataNode n nodes.get(path);
throw new KeeperException. NoNodeException();
}
byte lastdata[] =null;
synchronized (n){
lastdata n.data;
n. data datai
n. stat. setMtime(time);
n.stat. setMzxid(zxid);
n.stat. setVersion(version);
n. copystat(s);
}
dataWatches. triggerWatch(path, EventType. NodeDataChanged);
return s;
}
在对指定节点进行数据更新后,通过调用 WatchManager的 triggerWatch方
法来触发相关的事件:
public Set<Watcher> triggerWatch(String path, EventType type){
return triggerWatch(path, type, null);
}
public Set<Watcher> triggerwatch(String path, EventType type, Set<Watcher>
supress){
WatchedEvent e new WatchedEvent(type,
KeeperState. SyncConnected, path);
HashSet<Watcher> watchers
synchronized (this){
watchers watchTable. remove(path);
/如果不存在Watcher,直接返回
for (Watcher: watchers){
HashSet<String> paths watch2Paths. get (w);
if(paths=null){
paths. remove(path);
}
}
}
for (Watcher w: watchers){
if(supress=null supress. contains(w)){
continue;
}
.process(e);
return watchers;
7.1系统模型259

<==========================270end ==============================>
<==========================271start==============================>

无论是 dataWatches还是 childWatches管理器, Watcher的触发逻辑都是一
致的,基本步骤如下。
1.封装 WatchedEvent
首先将通知状态(KeeperState)、事件类型(EventType)以及节点路径(path)
封装成一个 WatchedEvent对象。
2.查询 Watcher
根据数据节点的节点路径从 watchTable中取出对 Watcher应的如果没有
找到 Watcher,说明没有任何客户端在该数据节点上注册过 Watcher,直接退
出。而如果找到了这个 Watcher,会将其提取出来,同时会直接从 watchTable
和 watch2Paths2中将其删除从这里我们也可以看出, Watcher在服务端
是一次性的,即触发一次就失效了。
3.调用 process方法来触发 Watcher
在这一步中,会逐个依次地调用从步骤2中找出的所有 Watcher的 process
方法。那么这里的process方法究竟做了些什么呢?在上文中我们已经提
到,对于需要注册 Watcher的请求, ZooKeeper会把当前请求对应的
ServerCnxn作为一个 Watcher进行存储,因此,这里调用的 process方
法,事实上就是 ServerCnxn的对应方法:
public class NIOServerCnxn extends ServerCnxn
synchronized public void process(WatchedEvent event){
ReplyHeader= new ReplyHeader(-1,-1l,0);
//Convert WatchedEvent to a type that can be sent over the wire
atcherEvent e event.getWrapper();
sendResponse(h, e, "notification");
从上面的代码片段中,我们可以看出在 process方法中,主要逻辑如下。
在请求头中标记“-1”,表明当前是一个通知。
将 WatchedEvent包装成 WatcherEvent,以便进行网络传输序列化。
向客户端发送该通知。
从以上几个步骤中可以看到, ServerCnxn的 process方法中的逻辑非常简单,本
260第7章 ZooKeeper技术内幕

<==========================271end ==============================>
<==========================272start==============================>

质上并不是处理客户端 Watcher真正的业务逻辑,而是借助当前客户端连接的
ServerCnxn对象来实现对客户端的 WatchedEve传递,真正的客户端Watcher
回调与业务逻辑执行都在客户端。
客户端回调 Watcher
上面我们已经讲解了服务端是如何进行 Watcher触发的,并且知道了最终服务端会通过
使用 ServerCnxn对应的TCP连接来向客户端发送一个 WatcherEvent事件,下面
我们来看看客户端是如何处理这个事件的。
SendThread接收事件通知
首先我们来看下 ZooKeeper客户端是如何接收这个客户端事件通知的
class SendThread extends Thread
void readResponse(ByteBuffer incomingBuffer) throws IOException
if(replyHdr. getxid()=-1){
//-1 means notification
WatcherEvent event new WatcherEvent()
event. deserialize(bbia, "response");
/1 convert from a server path to a client path
if(chrootPath=null){
string serverPath= event.getPath()
if(serverPath. compareTo(chrootPath)=0)
event.setPath("/");
else if (serverPath. length()> chrootPath. length())
event. setPath(serverPath. substring(chrootPath. length()));
}
WatchedEvent we new WatchedEvent(event);
eventThread. queueEvent(we)
return;
对于一个来自服务端的响应,客户端都是由 SendThread. readResponse
(ByteBuffer incomingBuffer)方法来统一进行处理的,如果响应头
replyHdr中标识了XD为-1,表明这是一个通知类型的响应,对其的处理大体
上分为以下4个主要步骤。
7.1系统模型261

<==========================272end ==============================>
<==========================273start==============================>

1.反序列化。
ZooKeeper客户端接到请求后,首先会将字节流转换成 WatcherEvent对象
2.处理chrootPath
如果客户端设置了 chrootPath属性,那么需要对服务端传过来的完整的节
点路径进行 chrootPath处理,生成客户端的一个相对节点路径。例如客户
端设置了 chrootPath为app1,那么针对服务端传过来的响应包含的节点
路径为/appl/locks,经过 ch root Path处理后,就会变成一个相对路径:/locks
关于 ZooKeeper的 chrootPath,将在7.3.2节中做详细讲解。
3.还原 WatchedEvent
在本节的“回调方法 process()部分”中提到, process接口的参数定义
是 WatchedEvent,因此这里需要将 WatcherEven对象转换成 Watched
Event.
4.回调 Watcher
最后将 WatchedEvent对象交给 EventThread线程,在下一个轮询周期
中进行 Watcher回调。
EventThread处理事件通知
在上面内容中我们讲到,服务端的 Watcher事件通知,最终交给了 EventThread
线程来处理,现在我们就来看看 EventThread的一些核心逻辑。 EventThread
线程是 ZooKeeper客户端中专门用来处理服务端通知事件的线程,其数据结构如
图7-11所示。
EventThread
waitingEvents: LinkedBlockingQueue-objed
queueEvent(WatchedEvent: void
+run(: vold
+processEvent(Object): void
图7-11.7-11a《EventThread数据结构
在上文中,我们讲到 SendThread接收到服务端的通知事件后,会通过调用
EventThread. queueEventEventT方法将事件传给 read线程,其逻辑如下:
public void queueEvent(WatchedEvent event){
if (event. getType() =EventType. None
262第7章 ZooKeeper技术内幕

<==========================273end ==============================>
<==========================274start==============================>

& sessionState = event. getState())
return;
sessionState event.getstate();
/ materialize the watchers based on the event
WatcherSetEventPair pair= new WatcherSetEventPair(
watcher.materialize(event. getState(), event. getType(),
event.getPath())
event);
/ queue the pair(watch set& event) for later processing
waitingEvents.add(pair);
}
queueEvent方法首先会根据该通知事件,从 KWatchManager中取出所有相关
的 Watcher
public Set<Watcher> materialize(Watcher.Event. KeeperState state,
Watcher. Event. EventType type,
String clientPath){
Set<Watcher> result= new HashSet<Watcher>();
switch (type){
case NodeDatachanged:
case NodeCreated:
synchronized (dataWatches){
addTo(datawatches. remove(clientPath), result);
}
synchronized (existWatches){
addTo(existWatches. remove(clientPath), result);
}
break;
return result;
final private void addTo(Set<Watcher> from,Set-Watcher> to){
if (from ! null){
to. addAll(from);
客户端在识别出事件类型 EventType后,会从相应的Watcher存储(即
datawatches existwatches childWatch中的一个或多个,本例中就
是从 datawatches和 existWatches两个存储中获取)中去除对应的 Watcher
注意,此处使用的是 remove接口,因此也表明了客户端的 Watcher机制同样也是
一次性的,即一旦被触发后,该 Watcher就失效了。
获取到相关的所有 Watcher之后,会将其放入 waitingEvents这个队列中去。
7.1系统模型263

<==========================274end ==============================>
<==========================275start==============================>

WaitingEvents是一个待处理 Watcher的队列, Event Thread的run方法会不
断对该队列进行处理:
public void run(){
try
isRunning=true;
while (true){
Object event waitingEvents.take();
if (event = eventOfDeath){
wasKilled=true;
else
processEvent(event);
}
private void processEvent(Object event){
try
if (event instanceof WatcherSetEventPair){
1/ each watcher will process the event
WatcherSetEventPair pair=(WatcherSet) event;
for (Watcher watcher pair. watchers){
try
watcher. process(pair.event);
} catch(Throwable){
从上面的代码片段中我们可以看出, EventThread线程每次都会从 waiting Events
队列中取出一个 Watcher,并进行串行同步处理。注意,此处 process Event方法中的
Watcher才是之前客户端真正注册的 Watcher,用其 process方法就可以实现 Watcher
的回调了。
Watcher特性总结
到目前为止,相信读者已经了解了 ZooKeeper中 Watcher机制的相关接口定义以及
Watcher的各类事件。同时,我们以 ZooKeeper节点的数据内容获取接口为例,从
ZooKeeper客户端进行 Watcher注册、服务端处理 Watcher以及客户端回调 Watcher三方
面分阶段讲解了 ZooKeeper的 Watcher工作机制。
通过上面内容的讲解,我们不难发现 ZooKeeper的 Watcher具有以下几个特性。
一次性
从上面的介绍中可以看到,无论是服务端还是客户端,一旦一个Watcher被触发,
ZooKeeper都会将其从相应的存储中移除。因此,开发人员在 Watcher的使用上要
264第7章 ZooKeeper技术内幕

<==========================275end ==============================>
<==========================276start==============================>

记住的一点是需要反复注册。这样的设计有效地减轻了服务端的压力试想,如果
注册一个 Watcher之后一直有效,那么,针对那些更新非常频繁的节点,服务端会
不断地向客户端发送事件通知,这无论对于网络还是服务端性能的影响都非常大
客户端串行执行
客户端 Watcher回调的过程是一个串行同步的过程这为我们保证了顺序,同时,
需要开发人员注意的一点是,千万不要因为一个 Watcher的处理逻辑影响了整个客
户端的 Watcher回调。
轻量
WatchedEvent是 ZooKeeper整个 Watcher知机制的最小通知单元,这个数据
结构中只包含三部分内容:通知状态、事件类型和节点路径。也就是说,Watcher
通知非常简单,只会告诉客户端发生了事件而不会说明事件的具体内容。例如针
对 NodeDataChanged事件, ZooKeeper的 Watcher只会通知客户端指定数据节点
的数据内容发生了变更,而对于原始数据以及变更后的新数据都无法从这个事件中
直接获取到,而是需要客户端主动重新去获取数据这也是 ZooKeeper的 Watcher
机制的一个非常重要的特性。
另外,客户端向服务端注册 Watcher的时候,并不会把客户端真实的 Watcher对象
传递到服务端,仅仅只是在客户端请求中使用 boolean类型属性进行了标记,同
时服务端也仅仅只是保存了当前连接的 ServerCnxn对象。
如此轻量的 Watcher机制设计,在网络开销和服务端内存开销上都是非常廉价的。
7.1.5ACL保障数据的安全
从前面的介绍中,我们已经了解到, ZooKeeper作为一个分布式协调框架,其内部存储
的都是一些关乎分布式系统运行时状态的元数据,尤其是一些涉及分布式锁、 Master选
举和分布式协调等应用场景的数据,会直接影响基于 ZooKeeper进行构建的分布式系统
的运行状态。因此,如何有效地保障ZooKeeper中数据的安全,从而避免因误操作而带
来的数据随意变更导致的分布式系统异常就显得格外重要了。所幸的是, ZooKeeper提
供了一套完善的ACL(Access Control List)权限控制机制来保障数据的安全。
提到权限控制,我们首先来看看大家都熟悉的、在Unix/Linux文件系统中使用的,也是
目前应用最广泛的权限控制方式GO(User、 Group和 Others)权限控制机制。简
单地讲,UGO就是针对一个文件或目录,对创建者(User)、创建者所在的组(Group)
7.1系统模型265

<==========================276end ==============================>
<==========================277start==============================>

和其他用户(Other)分别配置不同的权限从这里可以看出,UGO其实是一种粗粒度
的文件系统权限控制模式,利用UGO只能对三类用户进行权限控制,即文件的创建者、
创建者所在的组以及其他所有用户,很显然,UGO无法解决下面这个场景:
用户U1创建了文件F1,希望U1所在的用户组G拥有对F1读写和执行的权限,
另一个用户组G2拥有读权限,而另外一个用户U3则没有任何权限
接下去我们来看另外一种典型的权限控制方式: ACL ACL,即访问控制列表,是一种
相对来说比较新颖且更细粒度的权限管理方式,可以针对任意用户和组进行细粒度的权
限控制。目前绝大部分Unix系统都已经支持了ACL方式的权限控制, Linux也从2.6
版本的内核开始支持这个特性。
ACL介绍
在5.3.7节中,我们已经讲解了如何使用 ZooKeeper的CL机制来实现对数据节点的权
限控制,在本节中,我们将重点来看看 ZooKeeper中acl机制的技术内幕。
ZooKeeper的ACL权限控制和Unix/Linux操作系统中的ACL有一些区别,读者可以从
三个方面来理解ACL机制,分别是:权限模式(Scheme)、授权对象(ID)和权限
(Permission),通常使用“scheme:id: permission来标识一个有效的ACL信息。
权限模式:Scheme
权限模式用来确定权限验证过程中使用的检验策略。在 ZooKeeper中,开发人员使用最
多的就是以下四种权限模式。
IP模式通过IP地址粒度来进行权限控制,例如配置了“ip:192.168.0.110”,即表示
权限控制都是针对这个IP地址的。同时,IP模式也支持按照网段的方式进行配置,
例如“ip:192.168.0.1/24”表示针对192.168.0.*这个IP段进行权限控制。
Digest
Digest是最常用的权限控制模式,也更符合我们对于权限控制的认识,其以类似于
“username:password”形式的权限标识来进行权限配置,便于区分不同应用来进行
权限控制。
当我们通过“username:password”形式配置了权限标识后,ZooKeeper会对其先后
进行两次编码处理,分别是SHA-1算法加密和BASE64编码,其具体实现由
266第7章 ZooKeeper技术内幕

<==========================277end ==============================>
<==========================278start==============================>

DigestAuthenticationProvider generateDigest(String
idPassword)函数进行封装,清单7-4所示为使用该函数进行“username password
编码的一个实例。
清单7-4.对“password进行编码
package book. chapter07.$_;
import java. security. NoSuchAlgorithmException;
import org. apache. zookeeper. server. authDigestAuthenticationProvider;
//对“username: password进行编码
public class DigestAuthenticationProviderUsage
public static void main( String args throws NoSuchAlgorithmException
System.out. println( DigestAuthenticationProvidergenerateDigest("foo:z
k-book"));
}
}
运行程序,输出结果如下:
foo: kWN6aNSbjcKWPqjiV7cg0N24raU=
从上面的运行结果中可以看出,“
username: password"
最终会被混淆为一个无法辨
识的字符串。
World
World是一种最开放的权限控制模式,从其名字中也可以看出,事实上这种权限控
制方式几乎没有任何作用,数据节点的访问权限对所有用户开放,即所有用户都可
以在不进行任何权限校验的情况下操作 ZooKeeper上的数据。另外, World模式也
可以看作是一种特殊的 Digest模式,它只有一个权限标识,即“world: anyone
Super
Super模式,顾名思义就是超级用户的意思也是一种特殊的 Digest模式。在Super
模式下,超级用户可以对任意 ZooKeeper上的数据节点进行任何操作。关于Super
模式的用法,本节后面会进行详细的讲解。
授权对象:D
授权对象指的是权限赋予的用户或一个指定实体,例如P地址或是机器等。在不同的
权限模式下,授权对象是不同的,表7-4中列出了各个权限模式和授权对象之间的对应
关系。
7.1系统模型267

<==========================278end ==============================>
<==========================279start==============================>

表7-4.权限模式和授权对象的对应关系
权限模式授权对象
通常是一个IP地址或是IP段,例如“192.68.0.110”或“192.168.0.1/24
Digest
自定义,通常是“username:base64(sha-1-username password))",例如“foo:
kWN6aNSbjcKWPqjiV7cg0N24raU="
World
只有一个D:“anyone
Super
与 Digest模式一致
权限:Permission
权限就是指那些通过权限检查后可以被允许执行的操作。在 ZooKeeper中,所有对数据
的操作权限分为以下五大类:
● CREATE(C):数据节点的创建权限,允许授权对象在该数据节点下创建子节点。
DELETE(D):子节点的删除权限,允许授权对象删除该数据节点的子节点。
●READ(R):数据节点的读取权限,允许授权对象访问该数据节点并读取其数据
内容或子节点列表等。
WRITE(W):数据节点的更新权限,允许授权对象对该数据节点进行更新操作。
ADMIN(A):数据节点的管理权限,允许授权对象对该数据节点进行ACL相关
的设置操作。
权限扩展体系
在上文中,我们已经讲解了 ZooKeeper默认提供的IP、 Digest、 World和 Super这四种
权限模式,在绝大部分的场景下,这四种权限模式已经能够很好地实现权限控制的目的。
同时, ZooKeeper提供了特殊的权限控制插件体系,允许开发人员通过指定方式对
ZooKeeper的权限进行扩展。这些扩展的权限控制方式就像插件一样插入到 ZooKeeper
的权限体系中去,因此在 ZooKeeper的官方文档中,也称该机制为“Pluggable ZooKeeper
Authentication
实现自定义权限控制器
要实现自定义权限控制器非常简单, ZooKeeper定义了一个标准权限控制器需要实现的
接口:org. apache. zookeeper. server. auth AuthenticationProvider,其
接口定义如清单7-5所示。
清单7-5.权限控制器 Authentication Provider接口定义
public interface AuthenticationProvider
String getScheme()
268第7章 ZooKeeper技术内幕

<==========================279end ==============================>
<==========================280start==============================>

KeeperException. Code handleAuthentication(ServerCnxn cnxn, byte authData[]);
boolean matches(String id, String aclExpr);
boolean isAuthenticated()
boolean isValid(String id);
用户可以基于该接口来进行自定义权限控制器的实现。事实上,在前面内容中提到的几
个权限模式,对应的就是 ZooKeeper自带的 DigestAuthenticationProvider和
IPAuthenticationProvider两个权限控制器。
注册自定义权限控制器
完成自定义权限控制器的开发后,接下去就需要将该权限控制器注册到 ZooKeeper服务
器中去了。 ZooKeeper支持通过系统属性和配置文件两种方式来注册自定义的权限控制
器。
系统属性-Dzookeeeper. authProvider.
在 ZooKeeper启动参数中配置类似于如下的系统属性:
-Dzookeeper. authProvider. 1=com. zkbook. CustomAuthenticationProvider
配置文件方式
在zoocfg配置文件中配置类似于如下的配置项:
authProvider.=com. zkbook. CustomAuthentic
对于权限控制器的注册, ZooKeeper采用了延迟加载的策略,即只有在第一次处理包含
权限控制的客户端请求时,才会进行权限控制器的初始化。同时, ZooKeeper还会将所
有的权限控制器都注册到 ProviderRegistry中去。在具体的实现中, ZooKeeper首
先会将 DigestAuthenticationP rovider和 IPAuthentication rovider这两
个默认的控制器初始化,然后通过扫描 zookeeper. authProvider.这一系统属性,
获取到所有用户配置的自定义权限控制器,并完成其初始化。
ACL管理
讲解完 ZooKeeper的ACL及其扩展机制后,我们来看看如何进行ACL管理。
设置ACL
通过 zkCli脚本登录 ZooKeeper服务器后,可以通过两种方式进行ACL的设置。一种是
在数据节点创建的同时进行ACL权限的设置,命令格式如下:
7.1系统模型269

<==========================280end ==============================>
<==========================281start==============================>

create [-s] [-e] path data acl
具体使用如清单7-6所示。
清单7-6.创建数据节点的同时设置ACL
[zk: localhost CONNECTED create-e/zk-book- init digest:foo:
MiGs3Eiy1pP4rVH1Q1NwbP+OUF8=: cdrwa
Created /zk-book
[zk: Localhost CONNECTED 3] getAcl /zk-book
'digest, 'foo: MiGs3Eiy1pP4rVH1Q1NwbP+OUF8=
cdrwa
另一种方式则是使用 setAcl命令单独对已经存在的数据节点进行ACL设置:
setAcl path acl
具体使用如清单7-7所示。
清单7-7.使用 setAcl命令对数据节点设置ACL
[zk: Localhost CONNECTED 0] create -e /zk-book init
Created /zk-book
[zk: localhost CONNECTED 1] setAcl / digest: foo: MiGs3Eiy1pP4rVH1Q1NwbP+OUF8=:
cdrwa
CZxid=0400000042
ctime= Sun Jul1322:14:13cst2014
mZxid=040000042
mtime= Sun Jul1322:14:13cst2014
pZxid =0x400000042
cversion=0
dataVersion=0
aclVersion =1
ephemeralowner =0x1472ff49b020003
dataLength 4
numChildren=0-
[zk: Localhost CONNECTED 3] getAcl /zk-book
'digest, 'foo: MiGs3Eiy1pP4rVH1Q1NwbP+OUF8=
cdrwa
Super模式的用法
根据ACL权限控制的原理,一旦对一个数据节点设置了ACL权限控制,那么其他没有
被授权的 ZooKeeper客户端将无法访问该数据节点,这的确很好地保证了 ZooKeeper的
数据安全。但同时,ACL权限控制也给 ZooKeeper的运维人员带来了一个困扰:如果一
个持久数据节点包含了ACL权限控制,而其创建者客户端已经退出或已不再使用,那
么这些数据节点该如何清理呢?这个时候,就需要在ACL的 Super模式下,使用超级
管理员权限来进行处理了。要使用超级管理员权限,首先需要在 ZooKeeper服务器上开
270第7章 ZooKeeper技术内幕

<==========================281end ==============================>
<==========================282start==============================>

启 Super模式,方法是在 ZooKeeper服务器启动的时候,添加如下系统属性:
-Dzookeeper. DigestAuthenticationProvider superDigest=foo: kWN6aNSbjcKWPqjiV7c
gen24raU=
其中,“foo”代表了一个超级管理员的用户名;“kwn6aNSbjcK6 WPqjiV77cgon24ra=”是
可变的,由 ZooKeeper的系统管理员来进行自主配置,此例中使用的是“foo:zk-book”
的编码。完成对ZooKeeper服务器的 Super式的开启后,就可以在应用程序中使用了,
清单7-8是一个使用超级管理员权限操作 ZooKeeper数据节点的示例程序。
清单7-8.使用超级管理员权限操作 ZooKeeper数据节点
package book. chapter07. $7_
import org. apache. zookeeper. CreateMode;
import org. apache. zookeeper. ZooDefs. Ids;
import org. apache. zookeeper. ZooKeeper;
/使用Super权限模式进行权限控制
public class AuthSample_Super
final static String PATH "/zk-book";
public static void main(String[] args) throws Exception
null
ZooKeeper zookeeperl =new ZooKeeper("domainl. book. zookeeper: 2181",5000,
zookeeper1. addAuthInfodigest","foo:true".getBytes());
zookeeper1.create( PATH, "init".getBytes () Ids. CREATOR_ALL ACL, CreateMode.
EPHEMERAL )
ZooKeeper zookeeper2=new ZooKeeper("domainl.book, zookeeper:2181",50000,
null);
zookeeper2. addAuthInfo("digest", "foo: zk-book".getBytes());
System. out. println(zookeeper2. getData( PATH, false, null ))
ZooKeeper zookeeper3 new ZooKeeper("domain1. book. zookeeper: 2181",50000,
null):
zookeeper3. addAuthInfo("digest " "foo: false". getBytes());
System. out. println(zookeeper3. getData( PATH, false, nult
运行程序,输出结果如下:
[B7b7072
org. apache. zookeeper. KeeperException$NoAuthException:
KeeperErrorCode= NoAuth for/zk-book
从上面的输出结果中,我们可以看出,由于“foo:zk-book”是一个超级管理员账户,因
此能够针对一个受权限控制的数据节点z-book随意进行操作,但是对于“foo: false”这
7.1系统模型271

<==========================282end ==============================>
<==========================283start==============================>

个普通用户,就无法通过权限校验了。
7.2序列化与协议
在前面的章节中,我们对整个 ZooKeeper的系统模型进行了全局性的了解,从本节开始,
我们将深入 ZooKeeper的每个组成部分来讲解其内部的实现原理。
从上面的介绍中,我们已经了解到, ZooKeeper的客户端和服务端之间会进行一系列的
网络通信以实现数据的传输。对于一个网络通信,首先需要解决的就是对数据的序列化
和反序列化处理,在 ZooKeeper中,使用了Jute这一序列化组件来进行数据的序列化和
反序列化操作。同时,为了实现一个高效的网络通信程序,良好的通信协议设计也是至
关重要的。本章将围绕 ZooKeeper的序列化组件ute以及通信协议的设计原理来讲解
ZooKeeper在网络通信底层的一些技术内幕。
7.2.1jute介绍
Jute是 ZooKeeper中的序列化组件,最初也是 Hadoop中的默认序列化组件,其前身是
Hadoop Record中的序列化组件,后来由于 Apache Avro具有出众的跨语言特性、
丰富的数据结构和对 MapReduce的天生支持,并且能非常方便地用于RPC调用,从而
深深吸引了 Hadoop因此 Hadoop从0.21.版本开始,废弃了Record IO,使用了vro
这个序列化框架,同时Jute也从 Hadoop工程中被剥离出来,成为了独立的序列化组件。
ZooKeeper则从第一个正式对外发布的版本(0.0.1版本)开始,就一直使用Jute组件来
进行网络数据传输和本地磁盘数据存储的序列化和反序列化工作,一直使用至今。其实
在前些年, ZooKeeper官方也一直在寻求一种高性能的跨语言序列化组件,期间也多次
提出要替换 ZooKeeper的序列化组件。关于序列化组件的改造还需要追溯到2008年左
右,那时候 ZooKeeper官方就提出要使用类似于 Apache Avro、 Thrift或是 Google的
protobuf这样的组件来替换Jute,但是考虑到新老版本序列化组件的兼容性,官方团队
一直对替换序列化组件工作的推进持保守和观望态度值得一提的是,在替换序列化组
件这件事上, ZooKeeper官方团队曾经也有过类似于下面这样的方案:服务器开启两个
客户端服务端口,让包含新序列化组件的新版客户端连接单独的服务器端口,老版本的
客户端则连接另一个端口。但考虑到其实施的复杂性,这个想法设计一直没有落地。更
为有趣的是, ZooKeeper开发团队曾经甚至考虑将“如何让依赖Jute组件的老版本客户
注1: Apache Avro最初是 Hadoop的子项目,是由 Hadoop之父 Doug Cutting牵头发起开发的跨语言
序列化框架,目前已是 Apache : http: //avro.apache.org.的顶级项目,其官方主页是:htparoapach.org
272第 ZooKeeper7章技术内幕

<==========================283end ==============================>
<==========================284start==============================>

端/服务器和依赖Avro组件的新版本客户端/服务器进行无缝通信”这个问题作为Google
Summer of Code的题目。当然,另一个重要原因是针对Avro早期的发布版本,ZooKeeper
官方做了一个Jute和Avro的性能测试,但是测试结果并不理想,因此也并没有决定使
用Avro时至今日,Jute的序列化能力都不曾是 ZooKeeper的性能瓶颈
总之,因为种种原因以及2009年以后 ZooKeeper快速地被越来越多的系统使用,开发
团队需要将更多的精力放在解决更多优先级更高的需求和Bug修复上,以致于替换ute
序列化组件的工作一度被搁置——于是我们现在看到,在最新版本的 ZooKeeper中,底
层依然使用了Jute这个古老的,并且似乎没有更多其他系统在使用的序列化组件。
在本节接下来的部分,我们将向读者重点介绍Jute这种序列化组件在java语言中的使
用和实现原理。
7.2.2使用Jute进行序列化
下面我们通过一个例子来看看如何使用Jute来完成ava对象的序列化和反序列化。假
设我们有一个实体类 MockReqHeader(代表了一个简单的请求头),其定义如清单7-9
所示。
清单7-9. MockReqHeader实体类定义
public class MockReqHeader implements Record
private long sessionId;
private string typei
public MockReqHeader()
public MockReqHeader( long sessionId, String type){
this. sessionId= sessionId;
this. type type;
}
public long getSessionId()
return sessionId;
}
public void setSessionId( long sessionId){
this. sessionId sessionId;
}
public String getType()
return type;
}
public void setType( String m_){
type =m_;
p
public void serialize( OutputArchive a, String tag throws java.io. IOException
a_. startRecord( this, tag
7.2序列化与协议273

<==========================284end ==============================>
<==========================285start==============================>

a_.writeLong( sessionId, "sessionId");
a_. writeString( type, "type"
a_endRecord(this,tag)
p
public void deserialize( InputArchive a_, String tag throws java.io. IOException
a_.startRecord( tag
sessionId =a_. readLong("sessionId");
type =a_. readString "type")
a_.endRecord( tag )
}
}
上面即为一个非常简单的请求头定义,包含了两个成员变量: sessionId和type接
下来我们看看如何使用Jute来进行序列化和反序列化。
清单7-10. MockReqHeader实体类的序列化和反序列化
/开始序列化
ByteArrayOutputStream baos =new ByteArrayOutputStream()
BinaryOutputArchive boa BinaryOutputArc. getArchive(baos);
new MockReqHeader( 0x34221eccb92a34el "ping"). serialize(boa, "header");
//这里通常是TCP网络传输对象
ByteBuffer bb= ByteBuffer.wrap(bos. toByteArray())
开始反序列化
ByteBufferInputStream bbis new ByteBufferInputStream(bb);
BinaryInputArchive bbia BinaryInputArchive. getArchive(bbis);
MockReqHeader header2 new MockReqHeader()
header2.deserialize(bbia, "header");
bbis. close();
baos.close();
上面这个代码片段演示了如何使用Jute来对 MockReqHeader对象进行序列化和反序
列化,总的来说,大体可以分为4步。
1.实体类需要实现 Record接口的 serialize和 deserialize方法。
2.构建一个序列化器 BinaryOutputArchive
3.序列化。
调用实体类的 serialize方法,将对象序列化到指定tag中去例如在本例中
就将 MockReqHeader对象序列化到 header中去。
4.反序列化。
调用实体类的 deserialize,从指定的tag中反序列化出数据内容。
274第7章 ZooKeeper技术内幕

<==========================285end ==============================>
<==========================286start==============================>

以上就是Jute进行序列化和反序列化的基本过程,读者可以到源代码包book. chapter007
7_22中查看完整的样例程序。
7.2.3深入Jute
从上面的讲解中可以看出,使用Jute来进行Java对象的序列化和反序列化是非常简单
的。接下去我们再通过 Record序列化接口、序列化器和Jute配置文件三方面来深入了
解下ute
Record接口
Jute定义了自己独特 Record的序列化格式, ZooKeeper中所有需要进行网络传输或是本地磁盘
存储的类型定义,都实现了该接口,其结构简单明了,操作灵活可变,是Jute序列化的核心。
Record接口定义了两个最基本的方法,分别是 serialize和 deserialize,分别用于序
列化和反序列化:
package org.apache.jute
import java. io. IOException;
public interface Record
public void serialize(OutputArchive archive, String tag) throws IOException;
public void deserialize(InputArchive archive, String tag) throws IOException;
所有实体类通过实现 Record接口的这两个方法来定义自己将如何被序列化和反序列
化。其中 archive是底层真正的序列化器和反序列化器,并且每个 archive中可以
包含对多个对象的序列化和反序列化,因此两个接口方法中都标记了参数tag,用于向
序列化器和反序列化器标识对象自己的标记。例如在清单7-10所示的代码片段中,将
MockReqHeader对象交付给boa序列化器进行序列化,并标记为header
我们再重点来看清单7-9中对序列化和反序列化接口方法的实现:
public void serialize( OutputArchive a_, String tag throws java.io.IOException
{
a_ startRecord(this,tag);
a_ writeLong( sessionId,"sessionId");
a_. writeString( type, "type)
a_.endRecord this, tag
}
public void deserialize( InputArchive_, String tag) throws java.io. IOException
{
a. startRecord(tag)
sessionId=a_ readLong("sessionId");
type=a. readstring("type");
7.2序列化与协议275

<==========================286end ==============================>
<==========================287start==============================>

a_endRecordtag)
}
我们可以看到,在这个样例实现中, serialize和 deserialize的过程基本上是两
个相反的过程, serialize过程就是将当前对象的各个成员变量以一定的标记(tag)
写入到序列化器中去;而 deserialize过程则正好相反,是从反序列化器中根据指定
的标记(tag)将数据读取出来,并赋值给相应的成员变量。
OutputArchive和 InputArchive
OutputArchive和 InputArchive分别是Jute底层的序列化器和反序列化器接口定
义。在最新版本的Jute中,分别有 BinaryOutputAr/BinaryInputarchive
CsvOutputArchive/CsVInputArchive XmLOutputArchive/XmLInputArchive
三种实现。无论哪种实现,都是基于 Outputstream和 InputStream进行操作。
关于三种序列化/反序列化器的实现,读者可以到 ZooKeeper的org. apache.jute包
下面进行查阅。 BinaryOutputArchive对数据对象的序列化和反序列化,主要用于
进行网路传输和本地磁盘的存储,是 ZooKeeper底层最主要的序列化方式
CsvOutputArchive对数据的序列化,则更多的是方便数据对象的可视化展现,因此
被使用在 toString方法中。最后一种 XmLOutputA,则是为了将数据对象以
XML格式保存和还原,但是目前在 ZooKeeper中基本没有被使用到。
zookeeperjute
很多读者在阅读 ZooKeeper的代码的过程中,都会发现一个有趣的现象,那就是在很多
ZooKeeper类的说明中,都写着“File generated by hadoop record compiler. Do not edit.
这是因为该类并不是 ZooKeeper的开发人员编写的,而是通过Jute组件在编译过程中动
态生成的。在 ZooKeeper的src目录下,有一个 2名叫的文件:
清单7-11. zookeeperjute定义
module org. apache. zookeeper.data
class Id
ustring scheme;
ustring id;
class ACL
int perms
Id id;
}
在这个文件中定义了所有实体类的所属包名、类名以及该类的所有成员变量及其类型。
276第7章 ZooKeeper技术内幕

<==========================287end ==============================>
<==========================288start==============================>

例如清单7-11中的代码片段就分别定义了org.apache. zookeeper.data.id和
org. apache. zookeeper.data.cl两个类。
有了这个定义文件后,在源代码编译阶段,Jute会使用不同的代码生成器来为这些类定
义生成实际编程语言(Java或C/C++)的类文件。以Java语言为例,jute会使用
JavaGenerator来生成相应的类文件,这些类文件都会被存放在 srctjavalgenerated目
录下。需要注意的一点是,使用这种方式生成的类,都会实现 Record接口
7.2.4通信协议
基于TCP/IP协议, ZooKeeper实现了自己的通信协议来完成客户端与服务端、服务端与
服务端之间的网络通信。 ZooKeeper通信协议整体上的设计非常简单,对于请求,主要
包含请求头和请求体,而对于响应,则主要包含响应头和响应体,如图7-12所示。
请求头
请求体
len
响应头
响应体
图7-12.通信协议体
协议解析:请求部分
我们首先来看请求协议的详细设计,图7-13定义了一个“获取节点数据”请求的完整
协议定义。
4-11
12-n
Bit 0ffset 0-3
4-7-1112-15161)
Frotoca.
Part len
xid type len
pati: wate6
图7-13. GetDataRequest请求完整协议定义
接下来,我们将从请求头和请求体两方面分别解析 ZooKeeper请求的协议设计
请求头: RequestHeader
请求头中包含了请求最基本的信息,包括xid和type
module org. apache. zookeeper. proto{
class RequestHeader
int xid;
7.2序列化与协议277

<==========================288end ==============================>
<==========================289start==============================>

int type;
}
xid用于记录客户端请求发起的先后序号,用来确保单个客户端请求的响应顺序。type
代表请求的操作类型,常见的包括创建节点(0OpCode. create:1)、删除节点
(0OpCode create:2)和获取节点数据(0code. getData:4)等,所有这些操作
类型都被定义在类org. apache. zookeeper. ZooDefs. Op Code中根据协议规定,
除非是“会话创建”请求,其他所有的客户端请求中都会带上请求头。
请求体: Request
协议的请求体部分是指请求的主体内容部分包含了请求的所有操作内容。不同的请求
类型,其请求体部分的结构是不同的,下面我们以会话创建、获取节点数据和更新节点
数据这三个典型的请求体为例来对请求体进行详细分析。
ConnectRequest:会话创建
ZooKeeper客户端和服务器在创建会话的时候,会发送 ConnectRequest请求,
该请求体中包含了协议的版本号 protocolVersi、最近一次接收到的服务器
ZXID lastZxidSeen、会话超时时间 timeOut、会话标识 sessionId和会话密
码 passwd,其数据结构定义如下:
module org. apache. zookeeper. proto{
class ConnectRequest
int protocolVersion;
long lastZxidSeen;
int timeOut;
Long sessionId;
buffer passwd;
}
GetDataRequest:获取节点数据
ZooKeeper客户端在向服务器发送获取节点数据请求的时候,会发送 GetDataRequest
请求,该请求体中包含了数据节点的节点路径path和是否注册 Watcher的标识 watch,
其数据结构定义如下:
module org. apache. zookeeper. proto
class GetDataRequest
ustring path;
boolean watch;
278第7章 ZooKeeper技术内幕

<==========================289end ==============================>
<==========================290start==============================>

SetDataRequest:更新节点数据
ZooKeeper客户端在向服务器发送更新节点数据请求的时候,会发送 SetDataRequest请求,
该请求体中包含了数据节点的节点路径path、数据内容data和节点数据的期望版本号
version,其数据结构定义如下:
module org.apache. zookeeper.proto
.
{
class SetDataRequest
ustring path;
buffer data;
int version;
以上介绍了常见的三种典型请求体定义,针对不同的请求类型, ZooKeeper都会定义不
同的请求体,读者可以到org. apache. zookeeper. proto包下自行查看
请求协议实例:获取节点数据
上面我们分别介绍了请求头和请求体的协议定义,现在我们通过一个客户端“获取节点
数据”的具体例子来进一步了解请求协议。
清单7-12.发起一次简单的获取节点数据请求
public class A_simple_get_data_request implements Watcher
public static void main(String args) throws Exception
ZooKeeper zk new ZooKeeper("domain1. book. zookeeper",//
5000,/
new A simple get data request());
zk.getData( "/$7_2_4/get_data", true, nult)
public void process(WatchedEvent event)
清单7-12是一个发起一次简单的获取节点数据内容请求的样例程序,读者可以到
book.chapter007.872_4包中查看完整的源代码。客户端调用 getData接口,实际上就
是向 ZooKeeper服务端发送了一个 GetDataReques请求使用 Wireshark获取到
其发送的网络TCP包,如图7-14所示。
注2: Wireshark(前称Ethereal)是一个网络封包分析软件,由 GeraldCombs在1997年开发,于1998
年开源至今,已经吸引数以千计的开发人员参与开发与改进。其官方网站是: www wireshark.oorg
7.2序列化与协议279

<==========================290end ==============================>
<==========================291start==============================>

[SEQ/A
[PDU size: 291s]
TCP segment data (4 by
Short Message Peer to
Length:29
operation:
[Expert Info (Error/Malfor
00864810809
0010004949a14000800600000a4
0038808
0040904.0990
050
图7-14. GetDataRequest请求完整协议十六进制表示
在图7-14中,我们获取到了 ZooKeeper客户端请求发出后,在TCP层数据传输的十六
进制表示,其中带下划线的部分就是对应的 GetDataRequest请求,即[0,00,1d,00
00,0001000004.00000,...532,5f342f67.65,74.5f6461,74.61,01”通过
比对图7-13中的GetDataRequest请求的完整协议定义,我们来分析下这个十六进制
字节数组的含义,如表7-5所示。
表7-5. Get DataRequest请求协议解析
十六进制位
协议部分
数值或字符串
00.00.00.10-3位是en,代表整个请求的数据包长度29
00.0000.014~7位是id,代表客户端请求的发起序号
0000000811位是type,代表客户端请求类型4(代表 OpCode. getData)
0000012-15位是en,代表节点路径的长度
16(代表节点路径长度转换成十六进
制是16位)
2f,24,37,5f
32,5f34,2
724/get data(通过比对ASCl码
6765,745f1631位是path,代表节点路径
表转换成十进制即可)
6461,74,61
01
32位 watch是,代表是否注册 Watcher1(代表注册 Watcher)
表7-5中分段解析了 ZooKeeper客户端的 GetDataRequest请求发送的数据,其他请
求也都类似,感兴趣的读者可以使用相同的方法自行分析。
协议解析:响应部分
上面我们已经对 ZooKeeper请求部分的协议进行了解析,接下来我们看看服务器端响应
的协议解析。我们首先来看响应协议的详细设计,图7-15定义了一个“获取节点数据”
响应的完整协议定义。
280第7章 ZooKeeper技术内幕

<==========================291end ==============================>
<==========================292start==============================>

Bit
4-19
20-
offset-34-7-1
4-78-1516-1920-231e位
48位
8位
Protocol len xid zxid err len data
Part
pzxid
8位8位8位8位4位4位4位8位4位
4位
ezxid xid etise atize version everiion averszon hexeradataLength mumchildren
图7-15. GetDataResponse响应完整协议定义
响应头:ReplyHeader
响应头中包含了每一个响应最基本的信息,包括xidzxid和err:
module org. apache. zookeeper. proto
class ReplyHeader
int xid;
Long zxid;
int err;
}
xid和上文中提到的请求头中的xid是一致的响应中只是将请求中的xid原值返回zxid
代表 ZooKeeper服务器上当前最新的事务err则是一个错误码,当请求处理过程
中出现异常情况时,会在这个错误码中标识出来,常见的包括处理成功(Code.ok:0)
节点不存在(Code. NONODE:101)和没有权限(Code. NOAUTH:102)等,所有这
些错误码都被定义在类org. apache. zookeeper. KeeperException.code中
响应体: Response
协议的响应体部分是指响应的主体内容部分包含了响应的所有返回数据。不同的响应
类型,其响应体部分的结构是不同的,下面我们以会话创建、获取节点数据和更新节点
数据这三个典型的响应体为例来对响应体进行详细分析。
ConnectResponse:会话创建
针对客户端的会话创建请求,服务端会返回客户端一个 ConnectResponse响应,
7.2序列化与协议281

<==========================292end ==============================>
<==========================293start==============================>

该响应体中包含了协议的版本号 protocolVersion会话的超时时间 timeout、
会话标识 sessionId和会话密码 passwd,其数据结构定义如下:
module org. apache. zookeeper. proto
class ConnectResponse
int protocolVersion;
int timeOut;
Long sessionId;
buffer passwd;
GetDataResponse:获取节点数据
针对客户端的获取节点数据请求,服务端会返回客户端一个 GetDataResponse
响应,该响应体中包含了数据节点的数据内容data和节点状态stat,其数据结
构定义如下:
module org. apache. zookeeper. proto
class GetDataResponse
buffer data;
org. apache. zookeeper. data. Stat stat;
SetDataResponse:更新节点数据
针对客户端的更新节点数据请求,服务端会返回客户端一个 SetDataResponse
响应,该响应体中包含了最新的节点状态stat,其数据结构定义如下
module org. apache. zookeeper. proto
class SetDataResponse
org.apache. zookeeper. data. Stat stat;
以上介绍了常见的三种典型响应体定义,针对不同的响应类型, ZooKeeper都会定义不
同的响应体,读者可以到org. apache. zookeeper. proto包下自行查看。
响应协议实例:获取节点数据
在上面的内容中,我们分别介绍了响应头和响应体的协议定义,现在我们再次通过上文
中提到的客户端“获取节点数据”的例子来对响应协议做一个实际分析。
这里的测试用例还是使用清单7-12中的示例程序,只是这次我们使用 Wireshark获取到
服务端响应客户端时的网络TCP包,如图7-16所示。
282第7章 ZooKeeper技术内幕

<==========================293end ==============================>
<==========================294start==============================>

rcP segment data (4 bytes)
Short Message Peer to peer
L
S
Service type: (Default)
d
0000
6f2e495
0010009b5ae540008006000004911690a7
0020c3508
0203f0ff
0030
000101080a009 9c ca aa 9dd
805
88927
00
008
00a0
图7-16. Get DataResponse完整协议十六进制表示
在图7-16中,我们获取到了 ZooKeeper服务端响应发出之后,在TCP层数据传输的十
六进制表示,其中带下划线的部分就是对应的 GetDataResponse响应,即“[000,00
63,00,00,0005,00,0000000000,00,00,00000b,9,27,6d,5f,63,6f,6e,74,65
6e,74,00.00,00,00,00,00,00,04,00,00,000,00,00,00,04,00,00,01,43,67,bd,0e,08,00,00,01,43,
67,bd,0e,08000,00,00,00,0000,0,0000000000000000000,00
0000000000.0.通过比对图7-5中 GetDataResponse的响应完整协
议定义,我们来分析下这个十六进制字节数组的含义,如表7-6所示。
表7-6. GetDataResponse响应协议解析
十六进制位
协议部分
数值或字符串
00,00,00,63
0~3位是en,代表整个响应的数据包长度99
4~7位是id,代表客户端请求发起的序号
5(代表本次请求是客户端会
00,00.00,05
话创建后的第5次请求发送)
0000.00.00
8~15位是zxid,代表当前服务端处理过的4
00.00,00,04最新的ZXID值
00000
16~19位是err,代表错误码
0(代表Code.0K)
0000,00b
(代表接下去11位是数据
20~23位是en,代表节点数据内容的长度内容的字节数组)
69,276d.5f
63,6f,6e,74.
2434位是data,代表节点的数据内容
i'm content
65,6e,74
00,000,00
00,00.00.04
35~42位是,代表创建该数据节点时的4
00
ZXID
00.00,00,00
00,00,00,04
43~50位 mzxid是,代表最后一次修改该数据4
节点时的XD
00,00,01,43
6
7,bd,0e,08
5158位是ctime,代表数据节点的创建时间《158C3890I4879752《(即
2014-01-0621:27:59)
7.2序列化与协议283

<==========================294end ==============================>
<==========================295start==============================>

续表
十六进制位
协议部分
数值或字符串
00,00,01,43,
59~66位 mtime是,代表数据节点最后一次变1389014879752(即:
67bd,0e.08
更的时间
2014-01-0621:27:59)
0000,0000
67~70位是 version,代表数据节点的内容
0
的版本号
00.00.0000
7~74位 cversion是,代表数据节点的子节
点版本号
0
00.0000
75-78位 aversion是,代表数据节点的ACL
变更版本号
0
0000000
79~86位ephemeralowner是,如果该数据
000000.00
节点是临时节点,那么就记录创建该临时节点的0(代表该节点是持久节点)
会话ID,如果是持久节点,则为0
00.00.00.0b
87~90位dataLength是,代表数据节点的数
据内容长度
00,00,0000
91-94位 numChildren是,代表数据节点的0
子节点个数
000000
95-102位 pzxid是,代表最后一次对子节点4
000000列PZXID表变更的
表7-6中分段解析了 ZooKeeper服务端的 GetDataRespor响应发送的数据,其他
响应也都类似,感兴趣的读者可以使用相同的方法自行分析。
7.3客户端
客户端是开发人员使用 ZooKeeper最主要的途径,因此我们有必要对 ZooKeeper客户端
的内部原理进行详细讲解。 ZooKeeper的客户端主要由以下几个核心组件组成。
● ZooKeeper实例:客户端的入口。
Client WatchManager:客户端 Watcher管理器。
●HostProvider:客户端地址列表管理器。
ClientCnxn:客户端核心线程,其内部又包含两个线程,即 SendThread和
EventThread前者是一个1线程,主要负责 ZooKeeper客户端和服务端之间
的网络IO通信;后者是一个事件线程,主要负责对服务端事件进行处理。
客户端整体结构如图7-17所示。
284第7章 ZooKeeper技术内幕

<==========================295end ==============================>
<==========================296start==============================>

ZooKeeper
ClientCnxn
-w atchManager ZK.
ZKWatchManager
aticHostProvider
HostProvider
图7-17.客户端整体结构
ZooKeeper客户端的初始化与启动环节,实际上就是 ZooKeeper对象的实例化过程,因
此我们首先来看下 ZooKeeper客户端的构造方法:
ZooKeeper(String connectString, int sessionTimeout, Watcher watcher);
ZooKeeper(String connectString, int sessionTimeout, Watcher watcher,
boolean canBeReadOnly)
ZooKeeper(String connectString, int sessionTimeout, Watcher watcher,
long sessionId, byte[] sessionPasswd)
ZooKeeper(String connectString, int sessionTimeout, Watcher watcher,
Long sessionId, byte[] sessionPasswd, boolean canBeReadOnly)
关于 ZooKeeper构造方法的参数说明,在5.3.1节中已经做了详细的解释,这里不再赘
述。客户端的整个初始化和启动过程大体可以分为以下3个步骤。
1.设置默认 Watcher
2.设置 ZooKeeper服务器地址列表。
3.创建 ClientCnxn
如果在 ZooKeeper的构造方法中传入一个 Watcher对象的话,那么 ZooKeeper就会将
这个 Watcher对象保存在 ZKWatchManager defaultWatcher中,作为整个客户
端会话期间的默认 Watcher关于 Watcher的更多详细讲解,已经在7.1.4节中做了详
细说明。
7.3客户端285

<==========================296end ==============================>
<==========================297start==============================>

7.3.1一次会话的创建过程
为了帮助读者更好地了解 ZooKeeper客户端的工作原理,我们首先从一次客户端会话的
创建过程讲起,从而先对 ZooKeeper的客户端及其几个重要组件之间的协作关系有一个
宏观上的了解,如图7-18所示是客户端一次会话创建的基本过程。在这个流程图中,
所有以白色作为底色的框图流程可以看作是第一阶段,我们称之为初始化阶段;以斜线
底纹表示的流程是第二阶段,称之为会话创建阶段;以点状底纹表示的则是客户端在接
收到服务端响应后的对应处理,称之为响应处理阶段。
初始化阶段
1.初始化 ZooKeeper对象
通过调用 ZooKeeper的构造方法来实例化一个 ZooKeeper对象,在初始化过程中,
会创建一个客户端的 Watcher管理器: ClientWat
2.设置会话默认 Watcher
如果在构造方法中传入了一个 Watcher对象,那么客户端会将这个对象作为默认
Watcher保存在 ClientWatchManager中
3.构造 ZooKeeper服务器地址列表管理器:HostProvider
对于构造方法中传入的服务器地址,客户端会将其存放在服务器地址列表管理器
HostProvider中
4.创建并初始化客户端网络连接器: ClientCnxn。
ZooKeeper客户端首先会创建一个网络连接器 ClientCnxn,用来管理客户端与
服务器的网络交互。另外,客户端在创建 ClientCnxn的同时,还会初始化客户
端两个核心队列 outgoingQueue和 pendingQu,分别作为客户端的请求
发送队列和服务端响应的等待队列。
在后面的章节中我们也会讲到, ClientCnxn连接器的底层O处理器是
ClientCnxnSocket,因此在这一步中,客户端还会同时创建 ClientCnxnSocket
处理器。
286第7章 ZooKeeper技术内幕

<==========================297end ==============================>
<==========================298start==============================>

SendThread Fventinread
实例
SendThread
实化d
构建
实倒
ZooKeepes
EventThread
启动
chentoroo
Event Thread
服务器地址
启动
SendThread
创
TCF连接
构
请求
放入
发送队列
请求
收
设置默认
处理
Watcher
Besponse
连接成功
生成事件
处理
Watcher
SyncCannected件
图7-18. ZooKeeper客户端一次会话的创建过程
5.初始化 SendThread和 EventThread
客户端会创建两个核心网络线程 SendThread和 EventThread,前者用于管理
客户端和服务端之间的所有网络IO,后者则用于进行客户端的事件处理。同时,
客户端还会将 Client CnxnSocket分配给 SendThread作为底层网络O处理
器,并初始化 EventThread的待处理事件队列 waitingEvents,用于存放所
有等待被客户端处理的事件。
会话创建阶段
6.启动 SendThread和 EventThread
SendThread首先会判断当前客户端的状态,进行一系列清理性工作,为客户端
发送“会话创建”请求做准备。
7.3客户端287

<==========================298end ==============================>
<==========================299start==============================>

7.获取一个服务器地址。
在开始创建TCP连接之前,€CP,SendThreadoKce€首先需要获取一个Zookeeper服务器的
目标地址,这通常是从 HostProvider中随机获取出一个地址,然后委托给
ClientCnxnSocket去创建与 ZooKeeper服务器之间的TCP连接。
8.创建TCP连接。
获取到一个服务器地址后, ClientCnxnSocket负责和服务器创建一个CP长
连接。
9.构造 ConnectRequest请求。
在TCP连接创建完毕后,可能有的读者会认为,这样是否就说明已经和 ZooKeeper
服务器完成连接了呢?其实不然,步骤8只是纯粹地从网络TCP层面完成了客户
端与服务端之间的 Socket连接,但远未完成 ZooKeeper客户端的会话创建。
SendTh read会负责根据当前客户端的实际设置,构造出一个 ConnectRequest
请求,该请求代表了客户端试图与服务器创建一个会话。同时, ZooKeeper客户
端还会进一步将该请求包装成网络IO层的 Packet对象,放入请求发送队列
outgoingQueue中去。
10.发送请求
当客户端请求准备完毕后,就可以开始向服务端发送请求了。 ClientCnxnSocket负
责从 outgoing Queue中取出一个待发送的 Packet对象,将其序列化成 ByteBuffer
后,向服务端进行发送。
响应处理阶段
1.接收服务端响应。
ClientCnxnSocket接收到服务端的响应后,会首先判断当前的客户端状态是
否是“已初始化”,如果尚未完成初始化,那么就认为该响应一定是会话创建请求
的响应,直接交由 readconnectResult方法来处理该响应。
12.处理 Response
ClientCnxnSocket会对接收到的服务端响应进行反序列化,得到 Connect
Response对象,并从中获取到 ZooKeeper服务端分配的会话 sessionId
288第7章 ZooKeeper技术内幕

<==========================299end ==============================>