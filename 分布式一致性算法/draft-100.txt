<==========================5start==============================>

问题的提出
在计算机科学领域,分布式一致性问题是一个相当重要,且被广泛探索与论证的问题,
通常存在于诸如分布式文件系统、缓存系统和数据库等大型分布式存储系统中。
什么是分布式一致性?分布式一致性分为哪些类型?分布式系统达到一致性后将会是
一个什么样的状态?如果失去了一致性约束,分布式系统是否还可以依赖?如果一味地
追求一致性,对系统的整体架构和性能又有多大影响?这一系列的问题,似乎都没有一
个严格意义上准确的定义和答案。
终端用户
IT技术的发展,让我们受益无穷,从日常生活的超市收银,到高端精细的火箭发射,现
代社会中几乎所有行业,都离不开计算机技术的支持。
尽管计算机工程师们创造出了很多高科技的计算机产品来解决我们日常碰到的问题,但
用户只会倾向于选择一些易用、好用的产品那些难以使用的计算机产品最终都会被淘
汰这种易用性,其实就是用户体验的一部分。
计算机产品的用户体验,可以分为便捷性、安全性和稳定性等方面。在本书中,我们主
要讨论的是用户在使用计算机产品过程中遇到的那些和一致性有关的问题。在此之前,
我们首先来看一下计算机产品的终端用户是谁,他们的需求又是什么。
火车站售票
假如说我们的终端用户是一位经常做火车的旅行家,通常他是去车站的售票处购买车票,
然后拿着车票去检票口,再坐上火车,开始一段美好的旅行一一切似乎都是那么和谐。
想象一下,如果他选择的目的地是杭州,而某一趟开往杭州的火车只剩下最后一张车票
了,可能在同一时刻,不同售票窗口的另一位乘客也购买了同一张车票。假如说售票系

<==========================5end ==============================>
<==========================6start==============================>

目录
第1章分布式架构
…
1.1从集中式到分布式
1.1.1集中式的特点
1.1.2分布式的特
..2
1.1.3分布式环境的各种问题
12从ACID到 CAP
5
1.2.1ACD.5
1.2.2分布式事
1.2.3CA和BASE理论
小结…
第2章一致性协
2.12PC3
2.1.12
.1.23
…
.21
2.2 Paxos..... ........24
2.2.1追本溯源
….25
2.2.2 Paxos理论的
26
2.2.3 Paxos算法详
27
目录

<==========================6end ==============================>
<==========================7start==============================>

小结
第3章 Paxos的工程实.39
.I Chubby....
3.1.1概述
3.1.2广场场40
3.1.3设计
3.1.4 Chubby技术架
3.1.5 Paxos协议实现5
3.2 Hypertable..... ............55
3.2.1概
3.2.2算法实现57
小结…
.5
第4章 ZooKeeper与Paos.5
4.1初识 ZooKeeper.5
4.1.1 ZooKeeper介绍
5
4.1.2 ZooKeeper从何而来
62
4.1.3 ZooKeeper的基本概念62
4.1.4为什ZooKeeper么选择6
4.42《ZooKccpcrZBww65《的AB协65
4.2.1ZAB协议
65
4.2.2协议介绍
.6
4.2.3深入ZAB协议
.7
4.2.4ZAB与 Paxos算法的联系与
小结
78
目录

<==========================7end ==============================>
<==========================8start==============================>

第5章使用 ZooKeeper
.79
5.1部署与运行
.79
…
5.1.1系统.7
5.1.2集群与单机
5.1.3运行服8
5.2客户端脚
5.2.1创建…
5.2.2读取
9
5.2.3新
5.2.4删除
5.3Java客户端API使用
5.3.1创建会话91
5.3.2创建节点9
5.3.3删除节点……
5.3.4读取数据0
5.3.5更新数
5.3.6检测节点是否在…11
5.3.7权限控制
5.4开源客端.2
5.4.1 ZkClient........... ...............120
5.4.2rt130
小结
16
第6章 ZooKeeper的典型应用场景
6.1典型应用场景及实现注…
163
6.1.1数据发布/订
6
6.1.2负载均.16
6.1.3命名服
6.1.4分布式协调通
173
6.1.5集群管理
179
目录

<==========================8end ==============================>
<==========================9start==============================>

6.1.6 Master选举
6.1.7分布式锁
188
.194
6.1.8分布式队
……197
6.62《ZooKecpcrp97《在大型分布式系统中的应用…19
6.2.1 Hadoop..
.19
6.2.2 HBase..
2
6.2.3 Kafka...
0
.3 ZooKeeper在阿里巴巴的实践与应用
.213
6.3.1案例一消息中间件: Metamorphosis
….21
6.3.2案例二RPC服务框架: Dubbo
…27
6.3.3案例三MySQL基于 Binlog的增量订阅和消费组件: Canal219
6.3.4案例四分布式数据库同步系统:t223
6.3.5案例五轻量级分布式通用搜索平台:终搜
.26
6.3.6案例六实时计算引擎: JStorm
.238
小结
…242
第7章 ZooKeeper技术内2
7.1系统模型.243
7.1.1数据模型
…24
7.1.2节点特性
24
7.1.3版本保证分布式数据原子性操作.246
7.1.4 Watcher数据变更通24
7.1.5ACL保障数据的安全.265
7.2序列化与协议…
2
7.2.1Jute介
7.2.2使用Jute进行序列化.273
7.2.3深入Jute
2
7.2.4通信协议27
7.3客端……
7.3.1一次会话的创建过程286
目录

<==========================9end ==============================>
<==========================10start==============================>

7.3.2服务器地址
7.3.3 ClientCnxn:网络2
7.4会话
298
7.4.1会话状态
298
7.4.2会话创建
.299
7.4.3会话管
304
7.4.4会话清
7.4.5重
7.5服务器启动3
7.5.1单机版服务器启动.312
7.5.2集群版服务器动.35
7.6 Leader选
.32
7.6.1 Leader选举概2
7.6.2 Leader选举的算法分析
323
7.6.3 Leader选举的实现细2
7.7各服务器角色介
7.7.1 Leader......
3
7.7.2 Follower.....
333
7.7.3 Observer..................
7.7.4集群间消息通3
7.8请求处理
342
7.8.1会话创建请求3
7.8.2 SetData请
7.8.3事务请求转发35
7.8.4 GetData请求355
7.9数据与存储
7.9.1内存数.356
7.9.2事务日志35
7.9.3 snapshot数据快照36
7.9.4初始化3
7.9.5数据同372
小结
目录Ⅻ

<==========================10end ==============================>
<==========================11start==============================>

第8章 ZooKeeper运.379
8.1配置详解
379
8.1.1基本配置
.379
8.1.2高级配置
.380
8.2四字命令
8.3 JMX ..
30
8.3.1开启远程JM390
8.3.通过 JConsole连接oe39
8.4监…39
8.4.1实时监39
8.4.2数据39
8.5构建一个高可用的集39
8.5.1集群组成398
8.5.2容.39
8.5.3扩容缩40
8.6日常运40
8.6.1数据与日志管理
.402
8.6.2 Too many connections....
04
8.6.3磁盘管理
40
小结
附录 Windows平台上部署 ZooKeeper..40
附录B从源代码开始构建
附录C各发行版本重大更新记录
..414
附录 ZooKeeper源代码阅读指引
.418
目录

<==========================11end ==============================>
<==========================12start==============================>

第1章
分布式架构
随着计算机系统规模变得越来越大,将所有的业务单元集中部署在一个或若干个大型机
上的体系结构,已经越来越不能满足当今计算机系统,尤其是大型互联网系统的快速发
展,各种灵活多变的系统架构模型层出不穷。同时,随着微型计算机的出现,越来越多
廉价的PC机成为了各大企业IT架构的首选,分布式的处理方式越来越受到业界的青
睐—计算机系统正在经历一场前所未有的从集中式向分布式架构的变革。
1.1从集中式到分布式
自20世纪60年代大型主机被发明出来以后,凭借其超强的计算和O处理能力以及在
稳定性和安全性方面的卓越表现,在很长一段时间内,大型主机引领了计算机行业以及
商业计算领域的发展。在大型主机的研发上最知名的当属IBM,其主导研发的革命性产
品 System/360系列大型主机,是计算机发展史上的一个里程碑,与波音707和福特T
型车齐名,被誉为20世纪最重要的三大商业成就,并一度成为了大型主机的代名词。
从那时起,IT界进入了大型主机时代。
伴随着大型主机时代的到来,集中式的计算机系统架构也成为了主流。在那个时候,由
于大型主机卓越的性能和良好的稳定性,其在单机处理能力方面的优势非常明显,使得
IT系统快速进入了集中式处理阶段,其对应的计算机系统称为集中式系统。但从20世
纪80年代以来,计算机系统向网络化和微型化的发展日趋明显,传统的集中式处理模
式越来越不能适应人们的需求。
首先,大型主机的人才培养成本非常之高通常一台大型主机汇集了大量精密的计算机
组件,操作非常复杂,这对一个运维人员掌握其技术细节提出了非常高的要求。
其次,大型主机也是非常昂贵的。通常一台配置较好的IM大型主机,其售价可能在
1

<==========================12end ==============================>
<==========================13start==============================>

上百万美元甚至更高,因此也只有像政府、金融和电信等企业才有能力采购大型主机。
另外,集中式系统具有明显的单点问题。大型主机虽然在性能和稳定性方面表现卓越,
但这并不代表其永远不会出现故障。一旦一台大型主机出现了故障,那么整个系统将处
于不可用状态,其后果相当严重。最后,随着业务的不断发展,用户访问量迅速提高,
计算机系统的规模也在不断扩大,在单一大型主机上进行系统的扩容往往比较困难。
而另一方面,随着PC机性能的不断提升和网络技术的快速普及,大型主机的市场份额
变得越来越小,很多企业开始放弃原来的大型主机,而改用小型机和普通PC服务器来
搭建分布式的计算机系统。
其中最为典型的就是阿里巴巴集团的“去IOE”运动。从2008年开始,阿里巴巴的各
项业务都进入了井喷式的发展阶段,这对于后台IT系统的计算与存储能力提出了非常
高的要求,一味地针对小型机和高端存储进行不断扩容,无疑会产生巨大的成本。同时,
集中式的系统架构体系也存在诸多单点问题,完全无法满足互联网应用爆炸式的发展需
求。因此,为了解决业务快速发展给IT系统带来的巨大挑战,从2009年开始,阿里集
团启动了“去IOE”计划,其电商系统开始正式迈入分布式系统时代。
1.1.1集中式的特点
所谓的集中式系统就是指由一台或多台主计算机组成中心节点,数据集中存储于这个中
心节点中,并且整个系统的所有业务单元都集中部署在这个中心节点上,系统的所有功
能均由其集中处理。也就是说,在集中式系统中,每个终端或客户端机器仅仅负责数据
的录入和输出,而数据的存储与控制处理完全交由主机来完成。
集中式系统最大的特点就是部署结构简单。由于集中式系统往往基于底层性能卓越的大
型主机,因此无须考虑如何对服务进行多个节点的部署,也就不用考虑多个节点之间的
分布式协作问题。
1.1.2分布式的特点
在《分布式系统概念与设计》1一书中,对分布式系统做了如下定义:
分布式系统是一个硬件或软件组件分布在不同的网络计算机上,彼此之间仅仅
注: Distributed Systems Concepts and Design是 Coulouris、 Jean Dollimore、 Tim Kindberg和
《G0rdonBIair《合著的一本分布式领域经典著作其中文版本《分布式系统概念与设计》由金蓓弘
和马应龙共同翻译。
2第1章分布式架构

<==========================13end ==============================>
<==========================14start==============================>

通过消息传递进行通信和协调的系统。
上面这个简单的定义涵盖了几乎所有有效地部署了网络化计算机的系统。严格地讲,同
一个分布式系统中的计算机在空间部署上是可以随意分布的,这些计算机可能被放在不
同的机柜上,也可能在不同的机房中,甚至分布在不同的城市。无论如何,一个标准的
分布式系统在没有任何特定业务逻辑约束的情况下,都会有如下几个特征。
分布性
分布式系统中的多台计算机都会在空间上随意分布,同时,机器的分布情况也会随
时变动。
对等性
分布式系统中的计算机没有主/从之分,既没有控制整个系统的主机,也没有被控
制的从机,组成分布式系统的所有计算机节点都是对等的。副本(Replica)是分布
式系统最常见的概念之一,指的是分布式系统对数据和服务提供的一种冗余方式。
在常见的分布式系统中,为了对外提供高可用的服务,我们往往会对数据和服务进
行副本处理。数据副本是指在不同的节点上持久化同一份数据,当某一个节点上存
储的数据丢失时,可以从副本上读取到该数据这是解决分布式系统数据丢失问题
最为有效的手段。另一类副本是服务副本,指多个节点提供同样的服务,每个节点
都有能力接收来自外部的请求并进行相应的处理。
并发性
在“问题的提出”部分,我们已经提到过与“更新的并发性”相关的内容。在一个
计算机网络中,程序运行过程中的并发性操作是非常常见的行为,例如同一个分布
式系统中的多个节点,可能会并发地操作一些共享的资源,诸如数据库或分布式存
储等,如何准确并高效地协调分布式并发操作也成为了分布式系统架构与设计中最
大的挑战之一。
缺乏全局时钟
在上面的讲解中,我们已经了解到,一个典型的分布式系统是由一系列在空间上随
意分布的多个进程组成的,具有明显的分布性,这些进程之间通过交换消息来进行
相互通信。因此,在分布式系统中,很难定义两个事件究竟谁先谁后,原因就是因
为分布式系统缺乏一个全局的时钟序列控制。关于分布式系统的时钟和事件顺序,
1.1从集中式到分布式3

<==========================14end ==============================>
<==========================15start==============================>

在 Leslie Lamport2的经典论文Time, Clocks, and the Ordering of Events in
Distributed System中已经做了非常深刻的讲解。
故障总是会发生
组成分布式系统的所有计算机,都有可能发生任何形式的故障。一个被大量工程实
践所检验过的黄金定理是:任何在设计阶段考虑到的异常情况,一定会在系统实际
运行中发生,并且,在系统实际运行过程中还会遇到很多在设计时未能考虑到的异
常故障。所以,除非需求指标允许,在系统设计时不能放过任何异常情况。
1.1.3分布式环境的各种问题
分布式系统体系结构从其出现之初就伴随着诸多的难题和挑战,本节将向读者简要的介
绍分布式环境中一些典型的问题。
通信异常
从集中式向分布式演变的过程中,必然引入了网络因素,而由于网络本身的不可靠性,
因此也引入了额外的问题。分布式系统需要在各个节点之间进行网络通信,因此每次网
络通信都会伴随着网络不可用的风险,网络光纤、路由器或是DNS等硬件设备或是系
统不可用都会导致最终分布式系统无法顺利完成一次网络通信。另外,即使分布式系统
各节点之间的网络通信能够正常进行,其延时也会远大于单机操作。通常我们认为在现
代计算机体系结构中,单机内存访问的延时在纳秒数量级(通常是10ns左右),而正常
的一次网络通信的延迟在0.1~1ms左右(相当于内存访问延时的105~106倍),如此巨
大的延时差别,也会影响消息的收发的过程,因此消息丢失和消息延迟变得非常普遍。
网络分区
当网络由于发生异常情况,导致分布式系统中部分节点之间的网络延时不断增大,最终
导致组成分布式系统的所有节点中,只有部分节点之间能够进行正常通信,而另一些节
点则不能一—我们将这个现象称为网络分区,就是俗称的“脑裂”。当网络分区出现时,
分布式系统会出现局部小集群,在极端情况下,这些局部小集群会独立完成原本需要整
个分布式系统才能完成的功能,包括对数据的事务处理,这就对分布式一致性提出了非
注2: Leslie Lamport同时也是 Paxos的作者,在2.2.2节中将对其进行详细介绍
注3:ime, Clocks, and the Ordering of Events in Distributed System是分布式领域非常重要的经典论文,
读者可以访问微软研究院的官网查看其全文 http: //research. microsofi. com/en-us/um/peoplel
lamport/pubs/time-clocks.pdf.
4第1章分布式架构

<==========================15end ==============================>
<==========================16start==============================>

常大的挑战。
三态
从上面的介绍中,我们已经了解到了在分布式环境下,网络可能会出现各式各样的问题,
因此分布式系统的每一次请求与响应,存在特有的“三态”概念,即成功、失败与超时。
在传统的单机系统中,应用程序在调用一个函数之后,能够得到一个非常明确的响应:
成功或失败。而在分布式系统中,由于网络是不可靠的,虽然在绝大部分情况下,网络
通信也能够接收到成功或失败的响应,但是当网络出现异常的情况下,就可能会出现超
时现象,通常有以下两种情况:
由于网络原因,该请求(消息)并没有被成功地发送到接收方,而是在发送过程
就发生了消息丢失现象。
·该请求(消息)成功的被接收方接收后,并进行了处理,但是在将响应反馈给发
送方的过程中,发生了消息丢失现象。
当出现这样的超时现象时,网络通信的发起方是无法确定当前请求是否被成功处理的。
节点故障
节点故障则是分布式环境下另一个比较常见的问题,指的是组成分布式系统的服务器节
点出现的宕机或“僵死”现象。通常根据经验来说,每个节点都有可能会出现故障,并
且每天都在发生。
1.2从ACID到CAP/BASE
在上文中,我们讲解了集中式系统和分布式系统各自的特点,同时也看到了在从集中式
系统架构向分布式系统架构变迁的过程中会碰到的一系列问题。接下来,我们再重点看
看在分布式系统事务处理与数据一致性上遇到的种种挑战。
1.2.1 ACID
事务(Transaction)是由一系列对系统中数据进行访问与更新的操作所组成的一个程序
执行逻辑单元(Unit),狭义上的事务特指数据库事务。一方面,当多个应用程序并发访
问数据库时,事务可以在这些应用程序之间提供一个隔离方法,以防止彼此的操作互相
干扰。另一方面,事务为数据库操作序列提供了一个从失败中恢复到正常状态的方法,
同时提供了数据库即使在异常状态下仍能保持数据一致性的方法。
1.2从Aci到P/BASE5

<==========================16end ==============================>
<==========================17start==============================>

事务具有四个特征,分别是原子性( Atomicity)致性( Consistency)隔离性(Isolation)
和持久性( Durability),简称为事务的ACID特性。
原子性
事务的原子性是指事务必须是一个原子的操作序列单元。事务中包含的各项操作在一次
执行过程中,只允许出现以下两种状态之一。
●全部成功执行。
全部不执行。
任何一项操作失败都将导致整个事务失败,同时其他已经被执行的操作都将被撤销并回
滚,只有所有的操作全部成功,整个事务才算是成功完成。
一致性
事务的一致性是指事务的执行不能破坏数据库数据的完整性和一致性,一个事务在执行
之前和执行之后,数据库都必须处于一致性状态。也就是说,事务执行的结果必须是使
数据库从一个一致性状态转变到另一个一致性状态,因此当数据库只包含成功事务提交
的结果时,就能说数据库处于一致性状态。而如果数据库系统在运行过程中发生故障,
有些事务尚未完成就被迫中断,这些未完成的事务对数据库所做的修改有一部分已写入
物理数据库,这时数据库就处于一种不正确的状态,或者说是不一致的状态。
隔离性
事务的隔离性是指在并发环境中,并发的事务是相互隔离的,一个事务的执行不能被其
他事务干扰。也就是说,不同的事务并发操纵相同的数据时,每个事务都有各自完整的
数据空间,即一个事务内部的操作及使用的数据对其他并发事务是隔离的,并发执行的
各个事务之间不能互相干扰。
在标准SQL规范中,定义了4个事务隔离级别不同的隔离级别对事务的处理不同,
如未授权读取、授权读取、可重复读取和串行化。
未授权读取
未授权读取也被称为读未提交(Read Uncommitted),该隔离级别允许脏读取,其
隔离级别最低。换句话说,如果一个事务正在处理某一数据,并对其进行了更新,
注4:关于标准SQL规范中对事务隔离级别的定义,读者可以查阅SQL92相关文档进行详细了解:
http: /www.contrib. andrew.cmu.edu/-shadow/sql/sql1992.txt.
6第1章分布式架构

<==========================17end ==============================>
<==========================18start==============================>

但同时尚未完成事务,因此还没有进行事务提交;而与此同时,允许另一个事务也
能够访问该数据。举个例子来说,事务A和事务B同时进行,事务A在整个执行
阶段,会将某数据项的值从1开始,做一系列加法操作(比如说加1操作)直到变
成10之后进行事务提交,此时,事务B能够看到这个数据项在事务A操作过程中
的所有中间值(如1变成2、2变成3等),而对这一系列的中间值的读取就是未授
权读取。
授权读取
授权读取也被称为读已提交(Read Committed),它和未授权读取非常相近,唯一
的区别就是授权读取只允许获取已经被提交的数据。同样以上面的例子来说,事务
A和事务B同时进行,事务A进行与上述同样的操作,此时,事务B无法看到这
个数据项在事务A操作过程中的所有中间值,只能看到最终的10。另外,如果说
有一个事务C,和事务A进行非常类似的操作,只是事务C是将数据项从10加到
20,此时事务B也同样可以读取到20,即授权读取允许不可重复读取。
可重复读取
可重复读取(Repeatable Read),简单地说,就是保证在事务处理过程中,多次读取
同一个数据时,其值都和事务开始时刻是一致的因此该事务级别禁止了不可重复
读取和脏读取,但是有可能出现幻影数据。所谓幻影数据,就是指同样的事务操作,
在前后两个时间段内执行对同一个数据项的读取,可能出现不一致的结果。在上面
的例子,可重复读取隔离级别能够保证事务B在第一次事务操作过程中,始终对数
据项读取到1,但是在下一次事务操作中,即使事务B(注意,事务名字虽然相同,
但是指的是另一次事务操作)采用同样的查询方式,就可能会读取到10或20
串行化
串行化(Serializable)是最严格的事务隔离级别。它要求所有事务都被串行执行,
即事务只能一个接一个地进行处理,不能并发执行。
图1-1展示了不同隔离级别下事务访问数据的差异。
1.2从acid到ap/BSE7

<==========================18end ==============================>
<==========================19start==============================>

未授权读取授权读取可重复读取串行化
可能读取到~20只可能读取到只能读取到1不可访问
中任一个值1.10和20
1…10
1112…1920
事务A
事务C
A, start
A.end
C.start
C. end
事务A将数据项从1更新到
多C将数据项从10更新到
10,存在多个中国状态值
20,存在多个中周状态值
图1-1.4种隔离级别示意图
以上4个隔离级别的隔离性依次增强,分别解决不同的问题,表1-1对这4个隔离级别
进行了一个简单的对比。
表1-1.隔离级别对比
隔离级别
脏读
可重复读幻读
未授权读取
存在
不可以
存在
授权读取
不存在
不可以
存在
可重复读取
不存在
可以
存在
串行化
不存在
可以
不存在
事务隔离级别越高,就越能保证数据的完整性和一致性,但同时对并发性能的影响也越
大。通常,对于绝大多数的应用程序来说,可以优先考虑将数据库系统的隔离级别设置
为授权读取,这能够在避免脏读取的同时保证较好的并发性能。尽管这种事务隔离级别
会导致不可重复读、虚读和第二类丢失更新等并发问题,但较为科学的做法是在可能出
现这类问题的个别场合中,由应用程序主动采用悲观锁或乐观锁来进行事务控制。
持久性
事务的持久性也被称为永久性,是指一个事务一旦提交,它对数据库中对应数据的状态
变更就应该是永久性的。换句话说,一旦某个事务成功结束,那么它对数据库所做的更
新就必须被永久保存下来即使发生系统崩溃或机器宕机等故障,只要数据库能够重
新启动,那么一定能够将其恢复到事务成功结束时的状态。
1.2.2分布式事务
随着分布式计算的发展,事务在分布式计算领域中也得到了广泛的应用。在单机数据库
8第1章分布式架构

<==========================19end ==============================>
<==========================20start==============================>

中,我们很容易能够实现一套满足ACID特性的事务处理系统,但在分布式数据库中,
数据分散在各台不同的机器上,如何对这些数据进行分布式的事务处理具有非常大的挑
战。在1.1.3节中,我们已经讲解了分布式环境中会碰到的种种问题,其中就包括机器
宕机和各种网络异常等。尽管存在这种种分布式问题,但是在分布式计算领域,为了保
证分布式应用程序的可靠性,分布式事务是无法回避的。
分布式事务是指事务的参与者、支持事务的服务器、资源服务器以及事务管理器分别位
于分布式系统的不同节点之上。通常一个分布式事务中会涉及对多个数据源或业务系统
的操作。
我们可以设想一个最典型的分布式事务场景:一个跨银行的转账操作涉及调用两个异地
的银行服务,其中一个是本地银行提供的取款服务,另一个则是目标银行提供的存款服
务,这两个服务本身是无状态并且是互相独立的,共同构成了一个完整的分布式事务。
如果从本地银行取款成功,但是因为某种原因存款服务失败了,那么就必须回滚到取款
前的状态,否则用户可能会发现自己的钱不翼而飞了。
从上面这个例子中,我们可以看到,一个分布式事务可以看作是由多个分布式的操作序
列组成的,例如上面例子中的取款服务和存款服务,通常可以把这一系列分布式的操作
序列称为子事务。因此,分布式事务也可以被定义为一种嵌套型的事务,同时也就具有
了ACID事务特性。但由于在分布式事务中,各个子事务的执行是分布式的,因此要实
现一种能够保证ACID特性的分布式事务处理系统就显得格外复杂。
1.2.3cap和BASE理论
对于本地事务处理或者是集中式的事务处理系统,很显然我们可以采用已经被实践证
明很成熟的ACID模型来保证数据的严格一致性。而在1.2.2节中,我们也已经看到,
随着分布式事务的出现,传统的单机事务模型已经无法胜任。尤其是对于一个高访问
量、高并发的互联网分布式系统来说,如果我们期望实现一套严格满足ACID特性的
分布式事务,很可能出现的情况就是在系统的可用性和严格一致性之间出现冲突
因为当我们要求分布式系统具有严格一致性时,很可能就需要牺牲掉系统的可用性。
但毋庸置疑的一点是,可用性又是一个所有消费者不允许我们讨价还价的系统属性,
比如说像淘宝网这样的在线购物网站,就要求它能够7×24小时不间断地对外提供服务,
而对于一致性,则更加是所有消费者对于一个软件系统的刚需。因此,在可用性和一
致性之间永远无法存在一个两全其美的方案,于是如何构建一个兼顾可用性和一致性
的分布式系统成为了无数工程师探讨的难题,出现了诸如AP和BASE这样的分布式
系统经典理论。
1.2从AC到CA/Ae9

<==========================20end ==============================>
<==========================21start==============================>

CAP定理
2000年7月,来自加州大学伯克利分校的 Eric Brewer教授在 ACM PODC(Principles
of Distributed Computing)会议上,首次提出了著名的CAP猜想2年后,来自麻省
理工学院的 Seth Gilbert和 Nancy Lynch从理论上证明了 Brewer教授CAP猜想的可行
性7,从此,CAP理论正式在学术上成为了分布式计算领域的公认定理,并深深地影响
了分布式计算的发展。
CAP理论告诉我们,一个分布式系统不可能同时满足一致性(C: Consistency)可用性
(A: Availability)和分区容错性(p: Partition tolerance)这三个基本需求,最多只能同
时满足其中的两项。
一致性
在分布式环境中,一致性是指数据在多个副本之间是否能够保持一致的特性。在一致性
的需求下,当一个系统在数据一致的状态下执行更新操作后,应该保证系统的数据仍然
处于一致的状态。
对于一个将数据副本分布在不同分布式节点上的系统来说,如果对第一个节点的数据进
行了更新操作并且更新成功后,却没有使得第二个节点上的数据得到相应的更新,于是
在对第二个节点的数据进行读取操作时,获取的依然是老数据(或称为脏数据),这就
是典型的分布式数据不一致情况。在分布式系统中,如果能够做到针对一个数据项的更
新操作执行成功后,所有的用户都可以读取到其最新的值,那么这样的系统就被认为具
有强一致性(或严格的一致性)。
可用性
可用性是指系统提供的服务必须一直处于可用的状态,对于用户的每一个操作请求总是
能够在有限的时间内返回结果。这里我们重点看下“有限的时间内”和“返回结果”
“有限的时间内”是指,对于用户的一个操作请求,系统必须能够在指定的时间(即
响应时间)内返回对应的处理结果,如果超过了这个时间范围,那么系统就被认为
注5: Eric. Brewer是加州大学伯克利分校的终身教授,2009年度ACm- -Infosys奖得主,读者可以
访问伯克利的官方网站了解Brewer的详细介绍:Brewe《wcsberkeleucdrcbveiverbrewer
注6:关于 6: Brewer PODC PPT http://www在poC上的演讲PPT,读者可以访问伯克利的官方网站进行查阅http:/w
cs.berkeley. edu/~brewer/cs 262b-2004/PODC-keynote.pdf.
注7:关于 Seth Gilbert和 Nancy Lynch对CAP的证明。读者可以访问以下网址详细查阅完整论文:
10第1章分布式架构

<==========================21end ==============================>
<==========================22start==============================>

是不可用的。另外,“有限的时间内”是一个在系统设计之初就设定好的系统运行指
标,通常不同的系统之间会有很大的不同。比如说,对于一个在线搜索引擎来说,
通常在0.5秒内需要给出用户搜索关键词对应的检索结果。以 Google为例,搜索“分
布式”这一关键词, Google能够在0.3秒左右的时间,返回大约上千万条检索结果。
而对于一个面向HIVE的海量数据查询平台来说,正常的一次数据检索时间可能在
20秒到30秒之间,而如果是一个时间跨度较大的数据内容查询,“有限的时间”有
时候甚至会长达几分钟。
从上面的例子中,我们可以看出,用户对于一个系统的请求响应时间的期望值不尽相同。
但是,无论系统之间的差异有多大,唯一相同的一点就是对于用户请求,系统必须存在
一个合理的响应时间,否则用户便会对系统感到失望。
“返回结果”是可用性的另一个非常重要的指标,它要求系统在完成对用户请求的处理
后,返回一个正常的响应结果。正常的响应结果通常能够明确地反映出对请求的处理结
果,即成功或失败,而不是一个让用户感到困惑的返回结果。
让我们再来看看上面提到的在线搜索引擎的例子,如果用户输入指定的搜索关键词后,
返回的结果是一个系统错误,通常类似于“ Error"或“System Has Crashed
等提示语,那么我们认为此时系统是不可用的。
分区容错性
分区容错性约束了一个分布式系统需要具有如下特性:分布式系统在遇到任何网络分区
故障的时候,仍然需要能够保证对外提供满足一致性和可用性的服务,除非是整个网络
环境都发生了故障。
网络分区是指在分布式系统中,不同的节点分布在不同的子网络(机房或异地网络等)
中,由于一些特殊的原因导致这些子网络之间出现网络不连通的状况,但各个子网络的
内部网络是正常的,从而导致整个系统的网络环境被切分成了若干个孤立的区域。需要
注意的是,组成一个分布式系统的每个节点的加入与退出都可以看作是一个特殊的网络分区。
以上就是对CAP定理中一致性、可用性和分区容错性的讲解,通常使用图1-2所示的示
意图来表示CAP定理。
既然在上文中我们提到,一个分布式系统无法同时满足上述三个需求,而只能满足其中
的两项,因此在进行对CAP定理的应用时,我们就需要抛弃其中的一项,表1-2所示是
抛弃CAP定理中任意一项特性的场景说明。
1.2从acd到cap/baE11

<==========================22end ==============================>
<==========================23start==============================>

分区容错性
Partition tolerance
N
一致性
可用性
Consistency
Availability
图1-2.CAP定理示意图
表1-2.CAP定理应用
放弃CAP定理
说明
如果希望能够避免系统出现分区容错性问题,一种较为简单的做法是将所有
的数据(或者仅仅是那些与事务相关的数据)都放在一个分布式节点上。这样
放弃P
的做法虽然无法100%地保证系统不会出错,但至少不会碰到由于网络分区带
来的负面影响。但同时需要注意的是、放弃P的同时也就意味着放弃了系统的
可扩展性
相对于放弃“分区容错性”来说,放弃可用性则正好相反,其做法是一旦系
放弃A
统遇到网络分区或其他故障时,那么受到影响的服务需要等待一定的时间,因
此在等待期间系统无法对外提供正常的服务,即不可用
这里所说的放弃一致性,并不是完全不需要数据一致性,如果真是这样的话
那么系统的数据都是没有意义的,整个系统也是没有价值的
放弃C
事实上,放弃一致性指的是放弃数据的强一致性,而保留数据的最终一致性。
这样的系统无法保证数据保持实时的一致性,但是能够承诺的是,数据最终会
达到一个一致的状态。这就引入了一个时间窗口的概念,具体多久能够达到数
据一致取决于系统的设计,主要包括数据副本在不同节点之间的复制时间长短
从CAP定理中我们可以看出,一个分布式系统不可能同时满足一致性、可用性和分区
容错性这三个需求。另一方面,需要明确的一点是,对于一个分布式系统而言,分区容
错性可以说是一个最基本的要求。为什么这样说,其实很简单,因为既然是一个分布式
系统,那么分布式系统中的组件必然需要被部署到不同的节点,否则也就无所谓分布式
系统了,因此必然出现子网络。而对于分布式系统而言,网络问题又是一个必定会出现
的异常情况,因此分区容错性也就成为了一个分布式系统必然需要面对和解决的问题。
因此系统架构设计师往往需要把精力花在如何根据业务特点在C(一致性)和A(可用
性)之间寻求平衡。
BASE理论
BASE是 Basically Available(基本可用) Soft state(软状态)和 Eventually consistent
(最终一致性)三个短语的简写,是由来自eBay的架构师 Dan Pritchett在其文章bae
12第1章分布式架构

<==========================23end ==============================>
<==========================24start==============================>

An Acid Alternative中第一次明确提出的。BAE是对CAP中一致性和可用性权衡的结
果,其来源于对大规模互联网系统分布式实践的总结,是基于CAP定理逐步演化而来
的,其核心思想是即使无法做到强一致性( consistency),但每个应用都可以根据
自身的业务特点,采用适当的方式来使系统达到最终一致性(Eventual consistency)接下
来我们着重对BASE中的三要素进行详细讲解。
基本可用
基本可用是指分布式系统在出现不可预知故障的时候,允许损失部分可用性但请注
意,这绝不等价于系统不可用。以下两个就是“基本可用”的典型例子。
·响应时间上的损失:正常情况下,一个在线搜索引擎需要在0.5秒之内返回给用户
相应的查询结果,但由于出现故障(比如系统部分机房发生断电或断网故障),查
询结果的响应时间增加到了12秒。
·功能上的损失:正常情况下,在一个电子商务网站上进行购物,消费者几乎能够
顺利地完成每一笔订单,但是在一些节日大促购物高峰的时候,由于消费者的购
物行为激增,为了保护购物系统的稳定性,部分消费者可能会被引导到一个降级
页面。
弱状态
弱状态也称为软状态,和硬状态相对,是指允许系统中的数据存在中间状态,并认为该
中间状态的存在不会影响系统的整体可用性即允许系统在不同节点的数据副本之间进
行数据同步的过程存在延时。
最终一致性
最终一致性强调的是系统中所有的数据副本,在经过一段时间的同步后,最终能够达到
一个一致的状态。因此,最终一致性的本质是需要系统保证最终数据能够达到一致,而
不需要实时保证系统数据的强一致性。
亚马逊首席技术官 Werner Vogels在于208年发表的一 Eventually篇经典文章 Consistent
注8:BAE: An Acid Alternative是 Dan Pritchett在aCM上发表的一篇正式介绍BASE理论的经典文
章,作者在文中讨论了BASE和ACID之间的差异,并着重介绍了如何基于BASE理论来进行
大规模可扩展的分布式系统的架构设计,读者可以访问以下网址阅读此文: http: //d.acm.org.acm.og
citation.cfm?id-1394128.
1.2从acid到cap/base13

<==========================24end ==============================>
<==========================25start==============================>

Revisited中,对最终一致性进行了非常详细的介绍。他认为最终一致性是一种特殊的
弱一致性:系统能够保证在没有其他新的更新操作的情况下,数据最终一定能够达到一
致的状态,因此所有客户端对系统的数据访问都能够获取到最新的值。同时,在没有发
生故障的前提下,数据达到一致状态的时间延迟,取决于网络延迟、系统负载和数据复
制方案设计等因素。
在实际工程实践中,最终一致性存在以下五类主要变种。
因果一致性( Causal consistency)
因果一致性是指,如果进程A在更新完某个数据项后通知了进程B,那么进程B
之后对该数据项的访问都应该能够获取到进程A更新后的最新值,并且如果进程B
要对该数据项进行更新操作的话,务必基于进程A更新后的最新值,即不能发生
丢失更新情况。与此同时,与进程A无因果关系的进程C的数据访问则没有这样
的限制。
读己之所写( Read your writes)
读己之所写是指,进程A更新一个数据项之后它自己总是能够访问到更新过的
最新值,而不会看到旧值。也就是说,对于单个数据获取者来说,其读取到的数据,
一定不会比自己上次写入的值旧。因此,读己之所写也可以看作是一种特殊的因果
一致性。
会话一致性( Session consistency)
会话一致性将对系统数据的访问过程框定在了一个会话当中:系统能保证在同一个
有效的会话中实现“读己之所写”的一致性,也就是说,执行更能操作之后,客户
端能够在同一个会话中始终读取到该数据项的最新值。
单调读一致性( Monotonic read consistency)
单调读一致性是指如果一个进程从系统中读取出一个数据项的某个值后,那么系统
对于该进程后续的任何数据访问都不应该返回更旧的值。
注9: Eventually Consistent- Revisited是 Werner Vogels写的第二篇关于最终一致性的文章,读者可以
访问以下网址查看全文htp: http://www.allthingsdistributed.ccom/20/2/ealyconsisten.html
其关于最终一致性的第一篇文章发表于2007年,详见:itp 07, : http: /www: allhingsdistributed.com/2007/2007
12/eventually_consistent. html.
14第1章分布式架构

<==========================25end ==============================>
<==========================26start==============================>

单调写一致性( Monotonic write consistency)
单调写一致性是指,一个系统需要能够保证来自同一个进程的写操作被顺序地执行。
以上就是最终一致性的五类常见的变种,在实际系统实践中,可以将其中的若干个变种
互相结合起来,以构建一个具有最终一致性特性的分布式系统。事实上,最终一致性并
不是只有那些大型分布式系统才涉及的特性,许多现代的关系型数据库都采用了最终一
致性模型。在现代关系型数据库中,大多都会采用同步和异步方式来实现主备数据复制
技术。在同步方式中,数据的复制过程通常是更新事务的一部分,因此在事务完成后,
主备数据库的数据就会达到一致。而在异步方式中,备库的更新往往会存在延时,这取
决于事务日志在主备数据库之间传输的时间长短,如果传输时间过长或者甚至在日志传
输过程中出现异常导致无法及时将事务应用到备库上,那么很显然,从备库中读取的数
据将是旧的,因此就出现了数据不一致的情况。当然,无论是采用多次重试还是人为数
据订正,关系型数据库还是能够保证最终数据达到一致这就是系统提供最终一致性
保证的经典案例。
总的来说,BASE理论面向的是大型高可用可扩展的分布式系统,和传统事务的ACD
特性是相反的,它完全不同于ACID的强一致性模型,而是提出通过牺牲强一致性来获
得可用性,并允许数据在一段时间内是不一致的,但最终达到一致状态。但同时,在实
际的分布式场景中,不同业务单元和组件对数据一致性的要求是不同的,因此在具体的
分布式系统架构设计过程中,ACID特性与BASE理论往往又会结合在一起使用。
小结
计算机系统从集中式向分布式的变革伴随着包括分布式网络、分布式事务和分布式数据
一致性等在内的一系列问题与挑战,同时也催生了一大批诸如ACID、CAP和BASE等
经典理论的快速发展。
本章由计算机系统从集中式向分布式发展的过程展开,围绕在分布式架构发展过程中碰
到的一系列问题,结合ACID、CAP和BASE等分布式事务与一致性方面的经典理论,
向读者介绍了分布式架构
小结15

<==========================26end ==============================>
<==========================27start==============================>


<==========================27end ==============================>
<==========================28start==============================>

第2章
一致性协议
在第1章内容的讲解中我们也已经提到在对一个分布式系统进行架构设计的过程中,
往往会在系统的可用性和数据一致性之间进行反复的权衡,于是就产生了一系列的一致
性协议。
为了解决分布式一致性问题,在长期的探索研究过程中,涌现出了一大批经典的一致性
协议和算法,其中最著名的就是二阶段提交协议三阶段提交协议和 Paxos算法了本
章将着重向读者介绍二阶段和三阶段提交协议的设计与算法实现流程,指出它们各自的
优缺点,同时重点介绍 Paxos算法。
2.12pc与3PC
在分布式系统中,每一个机器节点虽然都能够明确地知道自己在进行事务操作过程中的
结果是成功或失败,但却无法直接获取到其他分布式节点的操作结果。因此,当一个事
务操作需要跨越多个分布式节点的时候,为了保持事务处理的ACD特性,就需要引入
一个称为“协调者(Coordinator)”的组件来统一调度所有分布式节点的执行逻辑,这
些被调度的分布式节点则被称为“参与者”(Participant)协调者负责调度参与者的行
为,并最终决定这些参与者是否要把事务真正进行提交。基于这个思想,衍生出了二阶
段提交和三阶段提交两种协议,在本节中,我们将重点对这两种分布式事务中涉及的一
致性协议进行讲解。
2.1.12C
2c,是Two- -Phase Commit的缩写,即二阶段提交,是计算机网络尤其是在数据库领域
内,为了使基于分布式系统架构下的所有节点在进行事务处理过程中能够保持原子性和
17

<==========================28end ==============================>
<==========================29start==============================>

一致性而设计的一种算法。通常,二阶段提交协议也被认为是一种一致性协议,用来保
证分布式系统数据的一致性。目前,绝大部分的关系型数据库都是采用二阶段提交协议
来完成分布式事务处理的,利用该协议能够非常方便地完成所有分布式事务参与者的协
调,统一决定事务的提交或回滚,从而能够有效地保证分布式数据一致性,因此二阶段
提交协议被广泛地应用在许多分布式系统中。
协议说明
顾名思义,二阶段提交协议是将事务的提交过程分成了两个阶段来进行处理,其执行流
程如下。
阶段一:提交事务请求
1.事务询问。
协调者向所有的参与者发送事务内容,询问是否可以执行事务提交操作,并开始
等待各参与者的响应。
2.执行事务。
各参与者节点执行事务操作,并将Undo和Redo信息记入事务日志中
3.各参与者向协调者反馈事务询问的响应。
如果参与者成功执行了事务操作,那么就反馈给协调者Yes响应,表示事务可以
执行;如果参与者没有成功执行事务,那么就反馈给协调者No响应,表示事务不
可以执行。
由于上面讲述的内容在形式上近似是协调者组织各参与者对一次事务操作的投票表态
过程,因此二阶段提交协议的阶段一也被称为“投票阶段”,即各参与者投票表明是否
要继续执行接下去的事务提交操作。
阶段二:执行事务提交
在阶段二中,协调者会根据各参与者的反馈情况来决定最终是否可以进行事务提交操作,
正常情况下,包含以下两种可能。
执行事务提交
假如协调者从所有的参与者获得的反馈都是Yes响应,那么就会执行事务提交。
1.发送提交请求。
18第2章致性协议

<==========================29end ==============================>
<==========================30start==============================>

协调者向所有参与者节点发出 Commit请求。
2.事务提交。
参与者接收到 Commit请求后,会正式执行事务提交操作,并在完成提交之
后释放在整个事务执行期间占用的事务资源。
3.反馈事务提交结果。
参与者在完成事务提交之后,向协调者发送Ack消息。
4.完成事务。
协调者接收到所有参与者反馈的Ack消息后,完成事务。
中断事务
假如任何一个参与者向协调者反馈了No响应,或者在等待超时之后,协调者尚无
法接收到所有参与者的反馈响应,那么就会中断事务。
1.发送回滚请求。
协调者向所有参与者节点发出 Rollback请求
2.事务回滚。
参与者接收到 Rollback请求后,会利用其在阶段一中记录的Undo信息来执
行事务回滚操作,并在完成回滚之后释放在整个事务执行期间占用的资源。
3.反馈事务回滚结果。
参与者在完成事务回滚之后,向协调者发送Ack消息。
4.中断事务。
协调者接收到所有参与者反馈的Ack消息后,完成事务中断。
以上就是二阶段提交过程中,前后两个阶段分别进行的处理逻辑。简单地讲,二阶段提
交将一个事务的处理过程分为了投票和执行两个阶段,其核心是对每个事务都采用先尝
试后提交的处理方式,因此也可以将二阶段提交看作一个强一致性的算法,图2-1和图
2-2分别展示了二阶段提交过程中“事务提交”和“事务中断”两种场景下的交互流程。
2.12Pc与3PC19

<==========================30end ==============================>
<==========================31start==============================>

事务提交
阶段一
阶段二
Prepare请求
om
Yes
与者
Ack
APrepare
参与
协调
omm请
者
s与者
Ack
德与卷
图2-1.二阶段提交“事务提交”示意图
务中断
阶段一
阶段二
Rollback
Ack
与
ppe求
Rollback
协调者
Ack
Prepare与参与者
Prepare求
Rollback请求
←
Ack
与者
与者
图22.二阶段提交“事务中断”示意图
优缺点
二阶段提交协议的优点:原理简单,实现方便。
二阶段提交协议的缺点:同步阻塞、单点问题、脑裂、太过保守。
同步阻塞
二阶段提交协议存在的最明显也是最大的一个问题就是同步阻塞,这会极大地限制
分布式系统的性能。在二阶段提交的执行过程中,所有参与该事务操作的逻辑都处
于阻塞状态,也就是说,各个参与者在等待其他参与者响应的过程中,将无法进行
其他任何操作。
单点问题
在上面的讲解过程中,相信读者可以看出,协调者的角色在整个二阶段提交协议中
起到了非常重要的作用。一旦协调者出现问题,那么整个二阶段提交流程将无法运
转,更为严重的是,如果协调者是在阶段二中出现问题的话,那么其他参与者将会
一直处于锁定事务资源的状态中,而无法继续完成事务操作。
20第2章一致性协议

<==========================31end ==============================>
<==========================32start==============================>

数据不一致
在二阶段提交协议的阶段二,即执行事务提交的时候,当协调者向所有的参与者发
送 Commit请求之后,发生了局部网络异常或者是协调者在尚未发送完 Commit请求
之前自身发生了崩溃,导致最终只有部分参与者收到了 Commit请求。于是,这部分
收到了 Commit请求的参与者就会进行事务的提交,而其他没有收到Commit请求的
参与者则无法进行事务提交,于是整个分布式系统便出现了数据不一致性现象。
太过保守
如果在协调者指示参与者进行事务提交询问的过程中,参与者出现故障而导致协调
者始终无法获取到所有参与者的响应信息的话,时协调者只能依靠其自身的超时
机制来判断是否需要中断事务,这样的策略显得比较保守。换句话说,二阶段提交
协议没有设计较为完善的容错机制,任意一个节点的失败都会导致整个事务的失败。
2.1.23PC
在上文中,我们讲解了二阶段提交协议的设计和实现原理,并明确指出了其在实际运行
过程中可能存在的诸如同步阻塞、协调者的单点问题、脑裂和太过保守的容错机制等缺
陷,因此研究者在二阶段提交协议的基础上进行了改进,提出了三阶段提交协议。
协议说明
3PC,是 Three-Phase- Commit的缩写,即三阶段提交,是2PC的改进版,其将二阶段提
交协议的“提交事务请求”过程一分为二,形成了由 CanCommit PreCommit和 do Commit
三个阶段组成的事务处理协议,其协议设计如图2-3所示。
状态
者状态
选收集
不确定
提交授权
准备捷交
确定提交
结束
成交
图2-3.三阶段提交协议流程示意图
注1:图片来自维基百科对三阶段提交协议的介绍 1: : http: len. wikipedia.org/wiki/file: three-phase_.wikipedorgi/fileheephase
commit_diagram.png
2.12pc与3PC21

<==========================32end ==============================>
<==========================33start==============================>

阶段:CanCommit
1.事务询问。
协调者向所有的参与者发送一个包含事务内容的 canCommit请求,询问是否可以
执行事务提交操作,并开始等待各参与者的响应。
2.各参与者向协调者反馈事务询问的响应。
参与者在接收到来自协调者的 canCommit求后,正常情况下,如果其自身认为
可以顺利执行事务,那么会反馈Yes响应,并进入预备状态,否则反馈No响应。
阶段二: PreCommit
在阶段二中,协调者会根据各参与者的反馈情况来决定是否可以进行事务的 PreCommit
操作,正常情况下,包含两种可能。
执行事务预提交
假如协调者从所有的参与者获得的反馈都是Yes响应,那么就会执行事务预提交。
1.发送预提交请求。
协调者向所有参与者节点发出 preCommit的请,并进入 Prepared阶段
2.事务预提交。
参与者接收到 preCommit请求后,会执行事务操作,并将Undo和edo信息
记录到事务日志中。
3.各参与者向协调者反馈事务执行的响应。
如果参与者成功执行了事务操作,那么就会反馈给协调者Ack响应,同时等
待最终的指令:提交( commit)或中止(abort)
中断事务
假如任何一个参与者向协调者反馈了N响应,或者在等待超时之后,协调者尚无
法接收到所有参与者的反馈响应,那么就会中断事务。
1.发送中断请求。
协调者向所有参与者节点发出 abort请求。
2.中断事务。
22第2章致性协议

<==========================33end ==============================>
<==========================34start==============================>

无论是收到来自协调者的 abort请求,或者是在等待协调者请求过程中出现超
时,参与者都会中断事务。
阶段三: doCommit
该阶段将进行真正的事务提交,会存在以下两种可能的情况。
执行提交
1.发送提交请求。
进入这一阶段,假设协调者处于正常工作状态,并且它接收到了来自所有参与
者的Ack响应,那么它将从“预提交”状态转换到“提交”状态,并向所有的
参与者发送 doCommit请求。
2.事务提交。
参与者接收到 doCommit请求后,会正式执行事务提交操作,并在完成提交之
后释放在整个事务执行期间占用的事务资源。
3.反馈事务提交结果。
参与者在完成事务提交之后,向协调者发送Ack消息。
4.完成事务。
协调者接收到所有参与者反馈的Ack消息后,完成事务。
中断事务
进入这一阶段,假设协调者处于正常工作状态,并且有任意一个参与者向协调者反
馈了No响应,或者在等待超时之后,协调者尚无法接收到所有参与者的反馈响应,
那么就会中断事务。
1.发送中断请求。
协调者向所有的参与者节点发送 abort请求。
2.事务回滚。
参与者接收到 abort请求后,会利用其在阶段二中记录的Undo信息来执行事
务回滚操作,并在完成回滚之后释放在整个事务执行期间占用的资源。
2.12pc与3PC23

<==========================34end ==============================>
<==========================35start==============================>

3.反馈事务回滚结果。
参与者在完成事务回滚之后,向协调者发送Ack消息。
4.中断事务。
协调者接收到所有参与者反馈的Ack消息后,中断事务。
需要注意的是,一旦进入阶段三,可能会存在以下两种故障。
协调者出现问题。
协调者和参与者之间的网络出现故障。
无论出现哪种情况,最终都会导致参与者无法及时接收到来自协调者的 doCommit或是
abort请求,针对这样的异常情况,参与者都会在等待超时之后,继续进行事务提交。
优缺点
三阶段提交协议的优点:相较于二阶段提交协议,三阶段提交协议最大的优点就是降低
了参与者的阻塞范围,并且能够在出现单点故障后继续达成一致。
三阶段提交协议的缺点:三阶段提交协议在去除阻塞的同时也引入了新的问题,那就是
在参与者接收到 preCommit消息后,如果网络出现分区,此时协调者所在的节点和参与
者无法进行正常的网络通信,在这种情况下,该参与者依然会进行事务的提交,这必然
出现数据的不一致性。
2.2 Paxos算法
在2.1节中,我们已经对二阶段和三阶段提交协议进行了详细的讲解,并了解了它们各
自的特点以及解决的分布式问题。在本节中,我们将重点讲解另一种非常重要的分布式
一致性协议: Paxos Paxos算法是莱斯利·兰伯特(Leslie Lamport)于1990年提出
的一种基于消息传递且具有高度容错特性的一致性算法,是目前公认的解决分布式一致
性问题最有效的算法之一。
在第1章中我们已经提到,在常见的分布式系统中,总会发生诸如机器宕机或网络异常
等情况。 Paxos算法需要解决的问题就是如何在一个可能发生上述异常的分布式系统中,
注2:读者可以访问微软研究院的官方网站浏览 2: lamport : http: //research. microsoft.com/的完整介绍:h:research.microsof.com
en-us/news/features/lamport-031814.aspx.
24第2章一致性协议

<==========================35end ==============================>
<==========================36start==============================>

快速且正确地在集群内部对某个数据的值达成一致,并且保证不论发生以上任何异常,
都不会破坏整个系统的一致性。
2.2.1追本溯源
1982年, Lamport与另两人共同发表了论文 The Byzantine Generals Problem,提出了
一种计算机容错理论。在理论描述过程中,为了将所要描述的问题形象的表达出来,
Lamport设想出了下面这样一个场景:
拜占庭帝国有许多支军队,不同军队的将军之间必须制订一个统一的行
动计划,从而做出进攻或者撤退的决定,同时,各个将军在地理上都是
被分隔开来的,只能依靠军队的通讯员来进行通讯。然而,在所有的通
讯员中可能会存在叛徒,这些叛徒可以任意篡改消息,从而达到欺骗将
军的目的。
这就是著名的“拜占廷将军问题”。从理论上来说,在分布式计算领域,试图在异步系
统和不可靠的通道上来达到一致性状态是不可能的,因此在对一致性的研究过程中,都
往往假设信道是可靠的。而事实上,大多数系统都是部署在同一个局域网中的,因此消
息被篡改的情况非常罕见;另一方面,由于硬件和网络原因而造成的消息不完整问题,
只需一套简单的校验算法即可避免因此,在实际工程实践中,可以假设不存在拜占
庭问题,也即假设所有消息都是完整的,没有被篡改的。那么,在这种情况下需要什么
样的算法来保证一致性呢?
Lamport在1990年提出了一个理论上的一致性解决方案,同时给出了严格的数学证明。
鉴于之前采用故事类比的方式成功的阐述了“拜占廷将军问题”,因此这次 Lamport同
样用心良苦地设想出了一个场景来描述这种一致性算法需要解决的问题,及其具体的解
决过程:
在古希腊有一个叫做 Paxos的小岛,岛上采用议会的形式来通过法令,议
会中的议员通过信使进行消息的传递。值得注意的是,议员和信使都是兼
职的,他们随时有可能会离开议会厅,并且信使可能会重复的传递消息,
也可能一去不复返。因此,议会协议要保证在这种情况下法令仍然能够正
确的产生,并且不会出现冲突。
注3: The Byzantine Generals Problem论文完整的描述了拜占庭将军问题,由 Leslie Lamport、 Robert
Shostak和 Marshall Pease三位大牛共同撰写,读者可以到卡内基梅隆大学的官网上浏览论文全文
htp: //www.cs.cmu.edu/-15712/papers/lampori82.pdf.
2.2 Paxos算法25

<==========================36end ==============================>
<==========================37start==============================>

这就是论文 The Part-Time- Parliament中到的兼职议会,而 Paxos算法名称的由来也
是取自论文中提到的 Paxos小岛。在这个论文中, Lamport压根没有说 Paxos小岛是虚
构出来的,而是煞有介事的说是考古工作者发现了 Paxos议会事务的手稿,从这些手稿
猜测 Paxos人开展议会的方法。因此,在这个论文中, Lamport从问题的提出到算法的
推演论证,通篇贯穿了对 Paxos议会历史的描述。
2.2.2 Paxos理论的诞生
在讨论 Paxos理论的诞生之前,我们不得不首先来介绍下 Paxos算法的作者 Leslie
Lamport(莱斯利·兰伯特)及其对计算机科学尤其是分布式计算领域的杰出贡献。作
为2013年的新科图灵奖得主,现年73岁的 Lamport是计算机科学领域一位拥有杰出成
就的传奇人物,其先后多次荣获ACM和EEE以及其他各类计算机重大奖项。 Lamport
对时间时钟、面包店算法、拜占庭将军问题以及 Paxos算法的创造性研究,极大地推动
了计算机科学尤其是分布式计算的发展,全世界无数工程师得益于他的理论,其中 Paxos
算法的提出,正是 Lamport多年的研究成果。
说起 Paxos理论的发表,还有一段非常有趣的历史故事。 Lamport早在1990年就已经将
其对 Paxos算法的研究论文 The Part-Time- Parliament提交给 TOCS Jnl的评审委
员会了,但是由于 Lamport“创造性”地使用了故事的方式来进行算法的描述,导致当
时委员会的工作人员没有一个能够正确地理解他对算法的描述,时任主编要求 Lamport
使用严谨的数据证明方式来描述该算法,否则他们将不考虑接受这篇论文。遗憾的是,
Lamport并没有接收他们的建议,当然也就拒绝了对论文的修改,并撤销了对这篇论文
的提交。在后来的一个会议上, Lamport还对此事耿耿于怀:“为什么这些搞理论的人一
点幽默感也没有呢?
幸运的是,还是有人能够理解Lamport那公认的令人晦涩的算法描述的。1996年,来自
微软的 Butler Lampson在WDAG96上提出了重新审视这篇分布式论文的建议,在次年
的WDAG97上,麻省理工学院的 Nancy Lynch也公布了其根据 Lamport的原文重新修
改后的 Revisiting the Paxos Algorithm,帮助”Lamport用数学的形式化术语定义并证
注4:读者可以到微软研究院官网浏览论文全htp: 4: http: //research.microsoft. com/en-us/um/people /lamport/.mcosofcom/en-umpople/lamport
pubs/lamport-paxos.pdf.
注5:读者可以访问微软研究院官网浏论文全文:hp: 5: : http://research. microsofi. com/en-us/um/peoplel.microsof.com/en-us/um/people/
lamport/ pubs/lamport-paxos. pdf
注6:读者可以访问微软研究院官网浏览论文全文 6: http://research. microsoff. com/en-us/um/people/
blampson/60-PaxosAlgorithm/Acrobat.pdf.
26第2章一致性协议

<==========================37end ==============================>
<==========================38start==============================>

明了 Paxos算法。于是在1998年 ACM的 TOCS上,这篇延迟了9年的论文终于被接
受了,也标志着 Paxos算法正式被计算机科学接受并开始影响更多的工程师解决分布式
一致性问题。
后来在2001年, Lamport本人也做出了让步,这次他放弃了故事的描述方式,而是使用
了通俗易懂的语言重新讲述了原文,并发表了 Paxos Made Simple当然,Lamport
甚为固执地认为他自己的表述语言没有歧义并且也足够让人明白 Paxos算法,因此不
需要数学来协助描述,于是整篇文章还是没有任何数学符号。好在这篇文章已经能够被
更多的人理解,相信绝大多数的 Paxos爱好者也都是从这篇文章开始慢慢进入了 Paxos
的神秘世界。
由于 Lamport个人自负固执的性格,使得Paxs理论的诞生可谓一波三折。关于
理论的诞生过程,后来也成为了计算机科学领域被广泛流传的学术趣事。
2.2.3 Paxos算法详解
Paxos作为一种提高分布式系统容错性的一致性算法,一直以来总是被很多人抱怨其算
法理论太难理解,尤其是 The Part-Time- Parliament这篇以故事形式展开的论文,对于绝
大部分人来说太过于晦涩。因此在本节中,我们将围绕 Lamport的另一篇关于 Paxos的
论文 Paxos Made Simple,从一个一致性算法所必须满足的条件展开,来向读者讲解 Paxos
作为一种一致性算法的合理性。
Paxos算法的核心是一个一致性算法,也就是论文 The Part-Time- Parliament中提到的
“synod”算法,我们将从对一致性问题的描述开始来讲解该算法需要解决的实际需求
问题描述
假设有一组可以提出提案的进程集合,那么对于一个一致性算法来说需要保证以下几点:
在这些被提出的提案中,只有一个会被选定。
如果没有提案被提出,那么就不会有被选定的提案。
当一个提案被选定后,进程应该可以获取被选定的提案信息。
注7:tocs全 ACM称 Transactions on Computer Systems《美国计算机学会计算机系统汇刊》)由
ACM出版,是中国计算机学会推荐的A类国际学术期刊。
注8:读者可以访问微软研究院官网浏览论文全文:htp: 8: :  http: //research.m
lamport/pubs/paxos-simple. pdf.
2.2 Paxos算法27

<==========================38end ==============================>
<==========================39start==============================>

对于一致性来说,安全性(Safety)需求如下:
只有被提出的提案才能被选定(Chosen)。
只能有一个值被选定。
·如果某个进程认为某个提案被选定了,那么这个提案必须是真的被选定的那个。
在对 Paxos算法的讲解过程中,我们不去精确地定义其活性( Liveness)需求,从整体
上来说, Paxos算法的目标就是要保证最终有一个提案会被选定,当提案被选定后,进
程最终也能获取到被选定的提案。
在该一致性算法中,有三种参与角色,我们用 Proposer、 Acceptor和 Learner来表示。
在具体的实现中,一个进程可能充当不止一种角色,在这里我们并不关心进程如何映射
到各种角色。假设不同参与者之间可以通过收发消息来进行通信,那么:
●每个参与者以任意的速度执行,可能会因为出错而停止,也可能会重启。同时,即
使一个提案被选定后,所有的参与者也都有可能失败或重启,因此除非那些失败或
重启的参与者可以记录某些信息,否则将无法确定最终的值。
消息在传输过程中可能会出现不可预知的延迟,也可能会重复或丢失,但是消息不
会被损坏,即消息内容不会被篡改(拜占庭式的问题)。
提案的选定
要选定一个唯一提案的最简单方式莫过于只允许一个 Accpetor存在,这样的话,Proposer
只能发送提案给该 Accpetor, Acceptor会选择它接收到的第一个提案作为被选定的提案。
这种解决方式尽管实现起来非常简单,但是却很难让人满意,因为一旦这个 Accpetor
出现问题,那么整个系统就无法工作了。
因此,应该寻找一种更好的解决方式,例如可以使用多个 Accpetor来避免 Accpetor的
单点问题。现在我们就来看看,在存在多个 Acceptor的情况下,如何进行提案的选取:
Proposer向一个 Acceptor集合发送提案,同样,集合中的每个 Acceptor都可能会批准
(Accept)该提案,当有足够多的 Acceptor批准这个提案的时候,我们就可以认为该提案
注9:一个分布式算法有两个最重要的属性;安全性(Safety)和活性(Liveness)简单来说,Safety
是指那些需要保证永远都不会发生的事情, Liveness则是指那些最终一定会发生的事情。
注10:拜占庭将军问题(Byzantine Generals Problem)同样是Leslie Lamport提出的一个针对点对点通
信的基本问题,其认为在存在消息丢失的不可靠信道上试图通过消息传递的方式达到一致性是
不可能的。感兴趣的读者可以查看 The byzantine generals problem:  https://www.andrew.cmu
edu/course/15-749/READINGS/required/resilience/lampor182.pdf.
28第2章一致性协议

<==========================39end ==============================>
<==========================40start==============================>

被选定了。那么,什么是足够多呢?我们假定足够多的 Acceptor是整个 Acceptor集合
的一个子集,并且让这个集合大得可以包含 Acceptor集合中的大多数成员,因为任意两
个包含大多数 Acceptor的子集至少有一个公共成员。另外我们再规定,每一个Acceptor
最多只能批准一个提案,那么就能保证只有一个提案被选定了。
推导过程
在没有失败和消息丢失的情况下,如果我们希望即使在只有一个提案被提出的情况下,
仍然可以选出一个提案,这就暗示了如下的需求。
P1:一个 Acceptor必须批准它收到的第一个提案。
上面这个需求就引出了另外一个问题:如果有多个提案被不同的 Proposer同时提出,这
可能会导致虽然每个 Acceptor都批准了它收到的第一个提案,但是没有一个提案是由多
数人都批准的,如图2-4所示就是这样的场景。
Acceptor 1 Acceptor 2 Acceptor Acceptor 4
Proposer 1 Proposer Proposer 3 Proposer 4
图2-4.不同的 Proposer分别提出多个提案
图2-4所示就是不同的 Proposer分别提出了多个提案的场景,在这种场景下,是无法选
定一个提案的。另外,即使只有两个提案被提出,如果每个提案都被差不多一半的
Acceptor批准了,此时即使只有一个 Acceptor出错,都有可能导致无法确定该选定哪个
提案,如图2-5所示就是这样的场景。
Aeceptor Acceptor 2 Aecestor 3 Aecagtor Acceptor 5
Proposer
Prapose;
图2-5.任意一个 Acceptor出现问题
2.2 Paxos算法29

<==========================40end ==============================>
<==========================41start==============================>

图2-5所示就是一个典 Acceptor型的在任意一个出现问题的情况下,无法选定提案的情
况。在这个例子中,共有5个 Acceptor,其中2个批准了提案V1,另外3个批准了提案
V2,此时如果批准V2的3个 Acceptor中有一个(例如图2-5中的第5个Acceptor)出
错了,那么V1和V2的批准者都变成了2个,此时就无法选定最终的提案了。
因此,在P1的基础上,再加上一个提案被选定需要由半数以上的 Acceptor批准的需求
暗示着一个 Acceptor必须能够批准不止一个提案。在这里,我们使用一个全局的编号(这
种全局唯一编号的生成并不是 Paxos算法需要关注的地方,就算法本身而言,其假设当
前已经具备这样的外部组件能够生成一个全局唯一的编号)来唯一标识每一个被
Acceptor批准的提案,当一个具有某Vaue值的提案被半数以上的 Acceptor批准后,我
们就认为该 Value被选定了,此时我们也认为该提案被选定了。需要注意的是,此处讲
到的提案已经和 Value不是同一个概念了,提案变成了一个由编号和 Value组成的组合
本,因此我们以“[编号, Value]”来表示一个提案。
根据上面讲到的内容,我们虽然允许多个提案被选定,但同时必须要保证所有被选定的
提案都具有相同的 VeValue值这是一个关于提案的约定,结合提案的编号,该
约定可以定义如下:
P2:如果编号为Mo、 Value值为Vo的提案(即[MV)被选定了,那么所
有比编号Mo更高的,且被选定的提案,其 Value值必须也是vo
因为提案的编号是全序的,条件P2就保证了只有一个alue值被选定这一关键安全性
属性。同时,一个提案要被选定,其首先必须被至少一个 Acceptor批准,因此我们可以
通过满足如下条件来满足P2
P2a:如果编号为Mo、 Value值为Vo的提案(即[M,V)被选定了,那么所
有比编号M更高的,且被 Acceptor批准的提案,其 Value值必须也是Vo
至此,我们仍然需要P1来保证提案会被选定,但是因为通信是异步的,一个提案可能
会在某个 Acceptor还未收到任何提案时就被选定了,如图2-6所示。
M
图2-6.一个提案可能会在某个 Acceptor还未收到任何提案时就被选定了
30第2章一致性协议

<==========================41end ==============================>
<==========================42start==============================>

如图2-6所示,在 Acceptor1没有收到任何提案的情况下,其他4个 Acceptor已经批准
了来自 Proposer2的提案[Mo,vi,而此时, Proposer产生了一个具有其他 Value值的、
编号更高的提案[M1,V2,并发送给了 Acceptor1.根据P1,就需要 Acceptor1批准该
提案,但是这与P2a矛盾,因此如果要同时满足P1和P2a,需要对P2a进行如下强化
P2b:如果一个提案[M,Vo]被选定后,那么之后任何 Proposer产生的编号更
高的提案,其 Value值都为Vo
因为一个提案必须在被 Proposer提出后才能被 Acceptor批准,因此2b包含了p2a,进
而包含了P2。于是,接下去的重点就是论证P2成立即可:
假设某个提案[Mo,Vo]已经被选定了,证明任何编号Mn>Mo的提案,其 Value
值都是Vo
数学归纳法证明
我们可以通过对Mn进行第二数学归纳法来进行证明,也就是说需要证明以下结论:
假设编号在Mo到Mn-1之间的提案,其Vlue值都是Vo,证明编号为Mn的提
案的 Value值也为Vo
因为编号为M的提案已经被选定了,这就意味着肯定存在一个由半数以上的 Acceptor
组成的集合C,C中的 Acceptor每个都批准了该提案。再结合归纳假设,“编号为M
的提案被选定”意味着:
c中的 Acceptor每个都批准了一个编号在M到Mn范围内的提案,并且每个
编号在M到Mn-1范围内的被 Acceptor准的提案,其 Value值都为Vo
因为任何包含半数以上 Acceptor的集合S都至少包含C中的一个成员,因此我们可以
认为如果保持了下面P2c的不变性,那么编号为Mn的提案的 Value也为o
P2c:对于任意的Mn和Vn,如果提案[Mn,Vn]被提出,那么肯定存在一个由
半数以上的 Acceptor组成的集合S,满足以下两个条件中的任意一个
s中不存在任何批准过编号小于Mn的提Acceptor案的
选取S中所 Acceptor有批准的编号小于M的提案,其中编号最大的那个提
案其 Value值是Vn
至此,只需要通过保持P2c,我们就能够满足P2b了。
从上面的内容中,我们可以看到,从P1到P2c的过程其实是对一系列条件的逐步加强,
2.2paxs算法31

<==========================42end ==============================>
<==========================43start==============================>

如果需要证明这些条件可以保证一致性,那么就需要进行反向推导:P2c=>p2b=>p2a
=>P2,然后通过P2和P1来保证一致性。
我们再来看P2c,实际上P2c规定了每个 Proposer如何产生一个提案:对于产生的每个
提案[Mn,Vn],需要满足如下条件。
存在一个由超过半数的 Acceptor组成的集合S:
要么S中没 Acceptor有批准过编号小于Mn的任何提案。
要么S中的Acceptor所有批准的所有编号小于Mn的提案中,编号最大的那
个提案的 Value值为Vn
当每个 Proposer都按照这个规则来产生提案时,就可以保证满足P了,接下来我们就
使用第二数学归纳法来证明P2c。
首先假设提案[Mo,V]被选定了,设比该提案编号大的提案为[Mn,n,我们需要证明
的就是在P2c的前提下,对于所有的[Mn,Vn,存在Vn=Vo
1.当Mn=M+1时,如果有这样一个编号为Mn的提案,首先我们知道[Mo,V已经
被选定了,那么就一定存在一个 Acceptor的子集S,且S中的 Acceptor已经批准
了小于Mn的提案,于是,V只能是多数集S中编号小于M但为最大编号的那个
提案的值。而此时因为Mn=Mo+1,因此理论上编号小于M但为最大编号的那个
提案肯定是[Mo,Vo],同时由于S和通过[Mo,Vo]的 Acceptor集合都是多数集,
也就是说二者肯定有交集这样 Proposer在确定V取值的时候,就一定会选择Vo
值得注意的一点是, Paxos算法的证明过程使用的是第二数学归纳法,上面实际上
就是数学归纳法的第一步,验证了某个初始值成立。接下去,就需要假设编号在
M+1到Mn-1区间内时成立,并在此基础上推导出当编号为Mn时也成立。
2.根据假设,编号在M+1到Mn-1区间内的所有提案的 Value值为V,需要证明的
是编号为Mn的提案的 Value值也为Vo.根据P2c,首先同样一定存在一个 Acceptor
的子集S,且S中 Acceptor的已经批准了小于Mn的提案,那么编号为Mn的提案
的 Value值只能是这个多数集S中编号小于M但为最大编号的那个提案的值。如
果这个最大编号落在M+1到Mn-1区间内,那么 Value值肯定是V,如果不落在
M+1到Mn-1区间内,那么它的编号不可能比M再小了,肯定就是Mo,因为S
也肯定会与批准[Mo,Vo这个提案的 Acceptor集合S有交集,而如果编号是Mo,
那么它的 Value值也是Vo,由此得证。
32第2章一致性协议

<==========================43end ==============================>
<==========================44start==============================>

Proposer生成提案
现在我们来看看,在P2c的基础上如何进行提案的生成。对于一个 Proposer来说,获取
那些已经被通过的提案远比预测未来可能会被通过的提案来得简单。因此, Proposer在
产生一个编号为Mn的提案时,必须要知道当前某一个将要或已经被半数以上 Acceptor
批准的编号小于Mn但为最大编号的提案。并且, Proposer会要求所有的 Acceptor都不
要再批准任何编号小于Mn的提案这就引出了如下的提案生成算法。
1. Proposer选择一个新的提案编号Mn,然后向某个 Acceptor集合的成员发送请求,
要求该集合中的 Acceptor做出如下回应。
·向Proposer承诺,保证不再批准任何编号小于Mn的提案。
如果Acceptor已经批准过任何提案,那么其就向 Proposer反馈当前该
Acceptor已经批准的编号小于Mn但为最大编号的那个提案的值。
我们将该请求称为编号为Mn的提案的 Prepare请求。
2.如果 Proposer收到了来自半数以上的 Acceptor的响应结果,那么它就可以产生编
号为Mn、 Value值为Vn的提案,这里的Vn是所有响应中编 Value号最大的提案的
值。当然还存在另一种情况,就是半数以上的 Acceptor都没有批准过任何提案,
即响应中不包含任何的提案,那么此时V值就可以由 Proposer任意选择。
在确定提案之后, Proposer就会将该提案再次发送给某个 Acceptor集合,并期望获得它
们的批准,我们称此请求为 Accept请求。需要注意的一点是,此时接受 Accept请求的
Acceptor集合不一定是之前响应 Prepare请求的 Acceptor集合这点相信读者也能够
明白,任意两个半数以上的 Acceptor集合必定包含至少一个公共 Acceptor
Acceptor批准提案
在上文中,我们已经讲解了 Paxos算法中 Proposer的处理逻辑,下面我们来看看 Acceptor
是如何批谁提案的。
根据上面的内容,一个 Acceptor可能会收到来自 Proposer的两种请求,分别是 Prepare
请求和 Accept请求,对这两类请求做出响应的条件分别如下。
Prepare请求: Acceptor可以在任何时候响应一个 Prepare请求。
Accept请求:在不违背 Accept现有承诺的前提下,可以任意响应 Accept请求
因此,对 Acceptor逻辑处理的约束条件,大体可以定义如下。
2.2 Paxos算法33

<==========================44end ==============================>
<==========================45start==============================>

Pla:一个 Acceptor只要尚未响应过任何编号大于Mn的 Prepare请求,那么它
就可以接受这个编号为Mn的提案。
从上面这个约束条件中,我们可以看出,Pla包含了P1。同时,值得一提的是,Paxos
算法允许 Acceptor忽略任何请求而不用担心破坏其算法的安全性。
算法优化
在上面的内容中,我们分别从 Proposer和 Acceptor对提案的生成和批准两方面来讲解了
Paxos算法在提案选定过程中的算法细节,同时也在提案的编号全局唯一的前提下,获
得了一个满足安全性需求的提案选定算法,接下来我们再对这个初步算法做一个小优化。
尽可能地忽略 Prepare请求:
假设一个 Acceptor收到了一个编号为Mn的 Prepare请求,但此时该 Acceptor
已经对编号大于Mn的 Prepare请求做出了响应,因此它肯定不会再批准任何
新的编号为Mn的提案,那么很显然, Acceptor就没有必要对这个 Prepare请
求做出响应,于是 Acceptor可以选择忽略这样的 Prepare请求。同时,Acceptor
也可以忽略掉那些它已经批准过的提案的 Prepare请求。
通过这个优化,每个 Acceptor只需要记住它已经批准的提案的最大编号以及它已经做出
Prepare请求响应的提案的最大编号,以便在出现故障或节点重启的情况下,也能保证
P2c的不变性。而对于 Proposer来说,只要它可以保证不会产生具有相同编号的提案,
那么就可以丢弃任意的提案以及它所有的运行时状态信息。
算法陈述
综合前面讲解的内容,我们来对 Paxos算法的提案选定过程进行一个陈述。结合 Proposer
和 Acceptor对提案的处理逻辑,就可以得到如下类似于两阶段提交的算法执行过程。
阶段一
1. Proposer选择一个提案编号Mn,然后向 Acceptor的某个超过半数的子集成员发
送编号为Mn的 Prepare请求。
2.如果一个Acceptor收到一个编号为Mn的 Prepare请求,且编号Mn大于该 Acceptor
已经响应的所有 Prepare请求的编号,那么它就会将它已经批准过的最大编号的
提案作为响应反馈给 Proposer,同时该 Acceptor会承诺不会再批准任何编号小于
Mn的提案。
34第2章一致性协议

<==========================45end ==============================>
<==========================46start==============================>

举个例子来说,假定一个 Acceptor已经响应过的所有 Prepare请求对应的提案编
号分别为1、2、…、5和7,那么该 Acceptor在接收到一个编号为8的 Prepare
请求后,就会将编号为7的提案作为响应反馈给 Proposer
阶段二
1.如果 Proposer收到来自半数以上的 Acceptor对于其发出的编号为Mn的 Prepare
请求的响应,那么它就会发送一个针对[Mn,V提案的 Accept请求给 Acceptor
注意,Vn的值就是收到的响应中编号最大的提案的值,如果响应中不包含任何
提案,那么它就是任意值。
2.如果 Acceptor收到这个针对[Mn,n]提案的 Accept请求,只要该 Acceptor尚未
对编号大于Mn的 Prepare请求做出响应,它就可以通过这个提案。
当然,在实际运行过程中,每一个 Proposer都有可能会产生多个提案,但只要每个
Proposer都遵循如上所述的算法运行,就一定能够保证算法执行的正确性。值得一提的
是,每个 Proposer都可以在任意时刻丢弃一个提案,哪怕针对该提案的请求和响应在提
案被丢弃后会到达,但根据 Paxos算法的一系列规约,依然可以保证其在提案选定上的
正确性。事实上,如果某个 Proposer已经在试图生成编号更大的提案,那么丢弃一些旧
的提案未尝不是一个好的选择。因此,如果一个 Acceptor因为已经收到过更大编号的
Prepare请求而忽略某个编号更小的 Prepare或者 Accept请求,那么它也应当通知其对应
的 Proposer,以便该 Proposer也能够将该提案进行丢弃这和上面“算法优化”部分
中提到的提案丢弃是一致的。
提案的获取
在上文中,我们已经介绍了如何来选定一个提案,下面我们再来看看如何让 Learner获
取提案,大体可以有以下几种方案。
方案一
Learner获取一个已经被选定的提案的前提是,该提案已经被半数以上的Acceptor
批准。因此,最简单的做法就是一旦 Acceptor批准了一个提案,就将该提案发送给
所有的 Learner
很显然,这种做法虽然可以让 Learner尽快地获取被选定的提案,但是却需要让每
个 Acceptor与所有的 Learner逐个进行一次通信,信的次数至少为二者个数的乘积。
2.2 Paxos算法35

<==========================46end ==============================>
<==========================47start==============================>

方案二
另一种可行的方案是,我们可以让所有的 Acceptor将它们对提案的批准情况,统一
发送给一个特定的 Learner(下文中我们将这样的 Learner称为“主 Learner”),在
不考虑拜占庭将军问题的前提下,我们假定 Learner之间可以通过消息通信来互相
感知提案的选定情况。基于这样的前提,当主 Learner被通知一个提案已经被选定
时,它会负责通知其他的 Learner
在这种方案中, Acceptor首先会将得到批准的提案发送给主 Lcarner,再由其同
步给其他 Learner,因此较方案一而言,方案二虽然需要多一个步骤才能将提案
通知到所有的 Learner,但其通信次数却大大减少了,通常只是 Acceptor和
Learner的个数总和。但同时,该方案引入了一个新的不稳定因素:主 Learner
随时可能出现故障。
方案三
在讲解方案二的时候,我们提到,方案二最大的问题在于主 Learner存在单点问
题,即主Learner随时可能出现故障。因此对方案二进行改进,可以将主 Learner
的范围扩大,即 Acceptor可以将批的提案发送给一个特定的 Learner集合,
该集合中的每个 Learner都可以在一个提案被选定后通知所有其他的 Learner
这个 Learner集合中的 Learner个数越多,可靠性就越好,但同时网络通信的复
杂度也就越高。
通过选取主 Proposer保证算法的活性
根据前面的内容讲解,我们已经基本上了解了 Paxos算法的核心逻辑,下面我们再来看
看 Paxos算法在实际运作过程中的一些细节。假设存在这样一种极端情况,有两个
Proposer依次提出了一系列编号递增的议案但是最终都无法被选定,具体流程如下:
Proposer1提出了一个编号为M1的提案,并完成了上述阶段一的流程。但与
此同时,另外一个 Proposer P22提出了一个编号为M2(M2>M1)的提案,同样
也完成了阶段一的流程,于是 Acceptor已经承诺不再批准编号小于M2的提案
了。因此,当P1进入阶段二的时候,其发出的 Accept请求将被 Acceptor忽略,
于是P1再次进入阶段一并提出了一个编号为M3(M3>M2)的提案,而这又
导致P2在第二阶段的 Accept请求被忽略,以此类推,提案的选定过程将陷入
死循环,如图2-7所示。
36第2章一致性协议

<==========================47end ==============================>
<==========================48start==============================>

二新敦二
图2-7.两个 Proposer分别生成提案后陷入死循环
为了保证 Paxos算法流程的可持续性,以避免陷入上述提到的“死循环”,就必须选择
一个主 Proposer,并规定只有主 Proposer才能提出议案。这样一来,只要主 Proposer和
过半的 Acceptor能够正常进行网络通信,那么但凡主 Proposer提出一个编号更高的提案,
该提案终将会被批准。当然,如果 Proposer发现当前算法流程中已经有一个编号更大的
提案被提出或正在接受批准,那么它会丢弃当前这个编号较小的提案,并最终能够选出
一个编号足够大的提案。因此,如果系统中有足够多的组件(包括Proposer Acceptor
和其他网络通信组件)能够正常工作,那么通过选择一个主 Proposer,整套 Paxos算法
流程就能够保持活性。
小结
在本章中,我们主要从协议设计和原理实现角度详细讲解了二阶段提交协议、三阶段提
交协议和 Paxos这三种典型的一致性算法可以说,这三种一致性协议都是非常优秀的
分布式一致性协议,都从不同方面不同程度地解决了分布式数据一致性问题,使用范围
都非常广泛。其中二阶段提交协议解决了分布式事务的原子性问题,保证了分布式事务
的多个参与者要么都执行成功,要么都执行失败。但是,在二阶段解决部分分布式事务
问题的同时,依然存在一些难以解决的诸如同步阻塞、无限期等待和“脑裂”等问题。
三阶段提交协议则是在二阶段提交协议的基础上,添加了 PreCommit过程,从而避免了
二阶段提交协议中的无限期等待问题。而 Paxos算法引入了“过半”的理念,通俗地讲
就是少数服从多数的原则。同时, Paxos算法支持分布式节点角色之间的轮换,这极大
地避免了分布式单点的出现,因此 Paxos算法既解决了无限期等待问题,也解决了“脑
裂”问题,是目前来说最优秀的分布式一致性协议之一。
小结37

<==========================48end ==============================>
<==========================49start==============================>


<==========================49end ==============================>
<==========================50start==============================>

第3章
Paxos的工程实践
在第2章中,我们主要从理论上讲解了 Paxos算法,然而 Paxos算法在工程实现的过程
中,会遇到非常多的问题。 Paxos算法描述并没有涉及实际工程中需要注意的很多细节,
同时对于开发人员来说,如何在保证数据一致性的情况下兼顾稳定性和性能也是一个巨
大的挑战。从本章开始,我们将结合实际工程实践中的 Paxos实现,来讲解如何真正地
使用 Paxos算法来解决分布式一致性问题。
3.1 Chubby
Google Chubby是一个大名鼎鼎的分布式锁服务,GFS和 Big Table等大型系统都用它来
解决分布式协作、元数据存储和 Master选举等一系列与分布式锁服务相关的问题。
Chubby的底层一致性实现就是以 Paxos算法为基础的,这给 Paxos算法的学习者提供了
一个理论联系的范例,从而可以了解到Paos算法是如何在实际工程中得到应用的。在
本节中,我们将围绕 Google公开的 Chubby论文 The Chubby lock service for
loosely-coupled distributed systems来讲解paos算法在 Chubby中的应用
3.1.1概述
Chubby是一个面向松耦合分布式系统的锁服务通常用于为一个由大量小型计算机构
注1: The Chubby lock service for loosely-coupled distributed systems论文是 Google发表的关于 Chubby
的技术论文,是目前公开描述 Chubby http:// static.的最完整、最权威的资料之一,论文原文可见:hp:staic
googleusercontent.commedial research.gooe.com/en/archive/chubby-osdi06-pf论文作者
Michael Burrows是分布式系统和并行计算领域的专家,其还参与撰写 BigTable和 Dapper等系
统的技术论文。
39

<==========================50end ==============================>
<==========================51start==============================>

成的松耦合分布式系统提供高可用的分布式锁服务。一个分布式锁服务的目的是允许它
的客户端进程同步彼此的操作,并对当前所处环境的基本状态信息达成一致。针对这个
目的, Chubby提供了粗粒度的分布式锁服务,开发人员不需要使用复杂的同步协议,
而是直接调用 Chubby的锁服务接口即可实现分布式系统中多个进程之间粗粒度的同步
控制,从而保证分布式数据的一致性。
Chubby的客户端接口设计非常类似于UNI文件系统结构,应用程序通过 Chubby的客
户端接口,不仅能够对 Chubby服务器上的整个文件进行读写操作,还能够添加对文件
节点的锁控制,并且能够订阅 Chubby服务端发出的一系列文件变动的事件通知。
3.1.2应用场景
在 Chubby的众多应用场景中,最为典型的就是集群中服务器的 Master选举。例如在
Google文件系统( Google File System,GFs)中使用 Chubby锁服务来实现对 GFS Master
服务器的选举。而在 BigTable, Chubby同样被用于 Master选举,并且借助 Chubby
Master能够非常方便地感知到其所控制的那些服务器。同时,通过 Chubby, BigTable
的客户端还能够方便地定位到当前 Bigtable集群的 Master服务器此外,在gf和
Bigtable中,都使用 Chubby来进行系统运行时元数据的存储。
3.1.3设计目标
对于 Chubby的设计,有的开发人员觉得作为 Paxos算法的实现者, Chubby应该构建成
一个包含 Paxos算法的协议库,从而使应用程序能够便捷地使用 Pasox算法。但是,
Chubby的最初设计者并没有选择这样做,而是将 Chubby设计成一个需要访问中心化节
点的分布式锁服务。
Chubby之所以设计成这样一个完整的分布式锁服务,是因为锁服务具有以下4个传统
算法库所不具有的优点。
注2: Google文件系统,即GfS,是 Google开发的一种面向廉价服务器架构的大型分布式文件系统,
读者可以通过阅读 Google于2003年发表的论文 The Google File System了解更多关于这一分布
式文件系统的技术内幕,论文原文可见:hp: http: //  static. googleusercontent. com/media/research.googlesercontent.commediaresearch
google.com/en/archive/gfs-sosp2003.pdf.
注3: BigTable,是 Google开发的一种用于进行结构化数据存储与管理的大型分布式存储系统,读者可以
通过阅读 Google于2006年发表的论文 Big: Distributed Storage System for Structured Data了解
更多关于这一分布式存储系统的技术内幕论文原文可见:htp: http: static. googleusercontent..googleuserconten
com/media/research. google. com/en/larchive/bigtable-osdi06.pdf.
40第 Paxos3章的工程实践

<==========================51end ==============================>
<==========================52start==============================>

对上层应用程序的侵入性更小
对于应用程序,尤其是上层的业务系统来说,在系统开发初期,开发人员并没有为
系统的高可用性做好充分的考虑。事实上,绝大部分的系统一开始都是从一个只需
要支撑较小的负载,并且只需要保证大体可用的原型开始的,往往并没有在代码层
面为分布式一致性协议的实现留有余地。当系统提供的服务日趋成熟,并且得到一
定规模的用户认可之后,系统的可用性就会变得越来越重要了。于是,集群中副本
复制和 Master选举等一系列提高分布式系统可用性的措施,就会被加入到一个已
有的系统中去。
在这种情况下,尽管这些措施都可以通过一个封装了分布式一致性协议的客户端库
来完成,但相比之下,使用一个分布式锁服务的接口方式对上层应用程序的侵入性
会更小,并且更易于保持系统已有的程序结构和网络通信模式。
便于提供数据的发布与订阅
几乎在所有使用 Chubby进行 Master选举的应用场景中,都需要一种广播结果的机
制,用来向所有的客户端公布当前的 Master务器。这就意味着 Chubby应该允许
其客户端在服务器上进行少量数据的存储与读取也就是对小文件的读写操作。
虽然这个特性也能够通过一个分布式命名服务来实现,但是根据实际的经验来看,
分布式锁服务本身也非常适合提供这个功能这一方面能够大大减少客户端依赖的
外部服务,另一方面,数据的发布与订阅功能和锁服务在分布式一致性特性上是相
开发人员对基于锁的接口更为熟悉
对于绝大部分的开发人员来说,在平常的编程过程中,他们对基锁的接口都已经
非常熟悉了。因此, Chubby为其提供了一套近乎和单机锁机制一致的分布式锁服
务接口,这远比提供一个一致性协议的库来得更为友好。
更便捷地构建更可靠的服务
通常一个分布式一致性算法都需要使用 Quorum机制来进行数据项值的选定。
Quorum机制是分布式系统中实现数据一致性的一个比较特殊的策略,它指的是在
一个由若干个机器组成的集群中,在一个数据项值的选定过程中,要求集群中存在
过半的机器达成一致,因此 Quorum机制也被称作“过半机制”。在 Chubby中通常
使用5台服务器来组成一个集群单元(cell根据 Quorum机制,只要整个集群中
有3台服务器是正常运行的,那么整个集群就可以对外提供正常的服务。相反的,
3.1 Chubby 41

<==========================52end ==============================>
<==========================53start==============================>

如果仅提供一个分布式一致性协议的客户端库,那么这些高可用性的系统部署都将
交给开发人员自己来处理,这无疑提高了成本。
因此, Chubby被设计成一个需要访问中心化节点的分布式锁服务。同时,在 Chubby的
设计过程中,提出了以下几个设计目标。
提供一个完整的、独立的分布式锁服务,而非仅仅是一个一致性协议的客户端库
在上面的内容中我们已经讲到,提供一个独立的锁服务的最大好处在于,Chubby
对于使用它的应用程序的侵入性非常低,应用程序不需要修改已有程序的结构即可
使用分布式一致性特性。例如,对于“Master选举同时将 Master信息登记并广播”
的场景,应用程序只需要向 Chubby请求一个锁,并且在获得锁之后向相应的锁文
件写入 Master信息即可,其余的客户端就可以通过读取这个锁文件来获取 Master
信息。
提供粗粒度的锁服务
Chubby锁服务针对的应用场景是客户端获得锁之后会进行长时间持有(数小时或
数天),而非用于短暂获取锁的场景。针对这种应用场景,当锁服务短暂失效时(例
如服务器宕机), Chubby需要保持所有锁的持有状态,以避免持有锁的客户端出现
问题。这和细粒度锁的设计方式有很大的区别,细粒度锁通常设计为锁服务一旦失
效就释放所有锁,因为细粒度锁的持有时间很短,相比而言放弃锁带来的代价较小。
在提供锁服务的同时提供对小文件的读写功能
Chubby提供对小文件的读写服务,以使得被选举出来的 Master可以在不依赖额外
服务的情况下,非常方便地向所有客户端发布自己的状态信息。具体的,当一个客
户端成功获取到一个 Chubby文件锁而成为 Master之后,就可以继续向这个文件里
写入 Master信息,其他客户端就可以通过读取这个文件得知当前的 Master信息
高可用、高可靠
在 Chubby的架构设计中,允许运维人员通过部署多台机器(一般是5台机器)来
组成一个 Chubby集群,从而保证集群的高可用。基于对Paxos算法的实现,对于
一个由5台机器组成的 Chubby集群来说,只要保证存在3台正常运行的机器,整
个集群对外服务就能保持可用。
另外,由于 Chubby支持通过小文件读写服务的方式来进行 Master选举结果的发布
与订阅,因此在 Chubby的实际应用过程中,必须能够支撑成百上千个 Chubby客
42第3章 Paxos的工程实践

<==========================53end ==============================>
<==========================54start==============================>

户端对同一个文件进行监视和读取。
提供事件通知机制
在实际使用过程中, Chubby客户端需要实时地感知到 Master的变化情况,当然这
可以通过让客户端反复的轮询来实现,但是在客户端规模不断增大的情况下,客户
端主动轮询的实时性效果并不理想,且对服务器性能和网络带宽压力都非常大。因
此, Chubby需要有能力将服务端的数据变化情况(例如文件内容变更)以事件的
形式通知到所有订阅的客户端。
3.1.4 Chubby技术架构
接下来我们一起看看, Chubby是如何来实现一个高可用的分布式锁服务的。
系统结构
Chubby的整个系统结构主要由服务端和客户端两部分组成,客户端通过RPC调用与服
务端进行通信,如图3-1所示。
5 Servers of a Chubby cell
Chent
Chubby
Applicaton Library
RPCs
ApplcatiChubby
Client Processes
图3-1. Chubby服务端与客户端结构示意图
一个典型的 Chubby集群,或称为 Chubby cell,通常由5台服务器组成。这些副本服务
器采用 Paxos协议,通过投票的方式来选举产生一个获得过半投票的服务器作为 Master
一旦某台服务器成为了 Master, Chubby就会保证在一段时期内不会再有其他服务器成
为 MasterMaster这段时期被称为租期 Master lease)。在运行过程中, Master服务
器会通过不断续租的方式来延长 Master期,而如果Master服务器出现故障,那么余
下的服务器就会进行新一轮的 Master选举,最终产生新的Master服务器,开始新的
Master租期。
集群中的每个服务器都维护着一份服务端数据库的副本,但在实际运行过程中,只有
3.1 Chubby 43

<==========================54end ==============================>
<==========================55start==============================>

Master服务器才能对数据库进行写操作,而其他服务器都是使用 Paxos协议从 Master
服务器上同步数据库数据的更新。
现在,我们再来看下 Chubby的客户端是如何定位到 MasterChubby服务器的。客户端
通过向记录有 Chubby服务端机器列表的DNs来请求获取所有的服务器列表,
然后逐个发起请求询问该服务器是否是 Master在这个询问过程中,那些非 Master的
服务器,则会将当前 Master所在的服务器标识反馈给客户端,这样客户端就能够非常快
速地定位到 Master服务器了。
一旦客户端定位到 Master服务器之后,只要该 Master正常运行,那么客户端就会将所
有的请求都发送到该 Master服务器上。针对写请求, Chubby Master会采用一致性协议
将其广播给集群中所有的副本服务器,并且在过半的服务器接受了该写请求之后,再响
应给客户端正确的应答。而对于读请求,则不需要在集群内部进行广播处理,直接由
Master服务器单独处理即可。
在 Chubby运行过程中,服务器难免会发生故障。如果当前的 Master服务器崩溃了,那
么集群中的其他服务器会在 Master租期到期后,重新开启新一轮的 Master选举通常,
进行一次 Master选举大概需要花费几秒钟的时间。而如果是集群中任意一台非 Master
服务器崩溃,那么整个集群是不会停止工作的,这个崩溃的服务器会在恢复之后自动加
入到 ChyChubby集群中去。新加入的服务器首先需要同步最新的数据库数据,完
成数据同步之后,新的服务器就可以加入到正常的 Paxos运作流程中与其他服务器副本
一起协同工作。
如果集群中的一个服务器发生崩溃并在几小时后仍无法恢复正常,那么就需要加入新的
机器,并同时更新DNS列表。 Chubby服务器的更换方式非常简单,只需要启动 Chubby
服务端程序,然后更新DNS上的机器列表(即使用新机器的IP地址替换老机器的IP
地址)即可。在Chubby运行过程中, Master务器会周期性地轮询DNS列表,因此其
很快就会感知到服务器地址列表的变更,然后 Master就会将集群数据库中的地址列表做
相应的变更,集群内部的其他副本服务器通过复制方式就可以获取到最新的服务器地址
列表了。
目录与文件
Chubby对外提供了一套与Unix文件系统非常相近但是更简单的访问接口。 Chubby的
数据结构可以看作是一个由文件和目录组成的树,其中每一个节点都可以表示为一个使
用斜杠分割的字符串,典型的节点路径表示如下:
/ls/foo/wombat/pouch
44第3章 Paxos的工程实践

<==========================55end ==============================>
<==========================56start==============================>

其中,s是所有 Chubby节点所共有的前缀,代表着锁服务,是 Lock Service的缩写;foo
则指定了 Chubby集群的名字,从DNS可以查询到由一个或多个服务器组成该 Chubby
集群;剩余部分的路径wombat/pouch则是一个真正包含业务含义的节点名字,由 Chubby
服务器内部解析并定位到数据节点。
Chubby的命名空间,包括文件和目录,我们称之为节点( nodes,在本书后面的内容中,
我们以数据节点来泛指 Chubby的文件或目录)。在同一个 Chubby集群数据库中,每一
个节点都是全局唯一的。和Unix系统一样,每个目录都可以包含一系列的子文件和子
目录列表,而每个文件中则会包含文件内容当然, Chubby并非模拟一个完整的文件
系统,因此没有符号链接和硬连接的概念。
由于 Chubby的命名结构组成了一个近似标准文件系统的视图,因此 Chubby的客户端
应用程序也可以通过自定义的文件系统访问接口来访问 Chubby服务端数据,比如可以
使用GFS的文件系统访问接口,这就大大减少了用户使用 Chubby的成本
Chubby上的每个数据节点都分为持久节点和临时节点两大类,其中持久节点需要显式
地调用接口API来进行删除,而临时节点则会在其对应的客户端会话失效后被自动删除。
也就是说,临时节点的生命周期和客户端会话绑定,如果该临时节点对应的文件没有被
任何客户端打开的话,那么它就会被删除掉。因此,临时节点通常可以用来进行客户端
会话有效性的判断依据。
另外, Chubby上的每个数据节点都包含了少量的元数据信息,其中包括用于权限控制
的访问控制列表(ACL)信息。同时,每个节点的元数据中还包括4个单调递增的64
位编号,分别如下。
·实例编号:实例编号用于标识 Chubby创建该数据节点的顺序,节点的创建顺序不
同,其实例编号也不同,因此,通过实例编号,即使针对两个名字相同的数据节点,
客户端也能够非常方便地识别出是否是同一个数据节点一因为创建时间晚的数
据节点,其实例编号必定大于任意先前创建的同名节点。
·文件内容编号(只针对文件):文件内容编号用于标识文件内容的变化情况,该编
号会在文件内容被写人时增加。
锁编号:锁编号用于标识节点锁状态变更情况,该编号会在节点锁从自由(free)
状态转换到被持有(held)状态时增加。
ACL编号:ACL编号用于标识节点的L信息变更情况,该编号会在节点的ACL
配置信息被写入时增加。
3.1 Chubby 45

<==========================56end ==============================>
<==========================57start==============================>

同时, Chubby还会标识一个64位的文件内容校验码,以便客户端能够识别出文件是否
变更。
锁与锁序列器
在分布式系统中,锁是一个非常复杂的问题,由于网络通信的不确定性,导致在分布式
系统中锁机制变得非常复杂,消息的延迟或是乱序都有可能会引起锁的失效。一个典型
的分布式锁错乱案例是,一个客户端C1获取到了互斥锁L,并且在锁L的保护下发出
请求R,但请求R迟迟没有到达服务端(可能出现网络延时或反复重发等),这时应用
程序会认为该客户端进程已经失败,于是便会为另一个客户端C2分配锁L,然后再重新
发起之前的请求R,并成功地应用到了服务器上。此时,不幸的事情发生了,客户端
C1发起的请求R在经过一波三折之后也到达了服务端,此时,它有可能会在不受任何
锁控制的情况下被服务端处理,从而覆盖了客户端C2的操作,于是导致系统数据出现
不一致。当然,诸如此类消息接收顺序紊乱引起的数据不一致问题已经在人们对分布式
计算的长期研究过程中得到了很好的解决,典型的解决方案包括虚拟时间和虚拟同步。
这两个分布式系统中典型的解决方案并不是本书的重点,感兴趣的读者可以在互联网上
了解更多相关的参考资料4
在 Chubby中,任意一个数据节点都可以充当一个读写锁来使用:一种是单个客户端以
排他(写)模式持有这个锁,另一种则是任意数目的客户端以共享(读)模式持有这个
锁。同时,在 Chubby的锁机制中需要注意的一点是, Chubby舍弃了严格的强制锁,客
户端可以在没有获取任何锁的情况下访问 Chubby的文件,也就是说,持有锁F既不是
访问文件F的必要条件,也不会阻止其他客户端访问文件F。
在 Chubby中,主要采用锁延迟和锁序列器两种策略来解决上面我们提到的由于消息延
迟和重排序引起的分布式锁问题。其中锁延迟是一种比较简单的策略,使用 Chubby的
应用几乎不需要进行任何的代码修改。具体的,如果一个客户端以正常的方式主动释放
了一个锁,那么 Chubby服务端将会允许其他客户端能够立即获取到该锁。而如果一个
锁是因为客户端的异常情况(如客户端无响应)而被释放的话,那么 Chubby服务器会
为该锁保留一定的时间,我们称之为“延迟”(ock--delay),在这段时间内,其他客户
端无法获取这个锁。锁延迟措施能够很好地防止一些客户端由于网络闪断等原因而与服
务器暂时断开的场景出现。总的来说,该方案尽管不完美,但是锁延时能够有效地保护
在出现消息延时情况下发生的数据不一致现象。
注4:相关参考资料包括 DAVID R. JEFFERSON  virtual time: http: // masters. donntu.edu. ua/2012/的rtualime:htt:master.donntu.edu.ua/2012
fnt/ vorotnikovalibraryvirtual time.pdf,及 Kenneth. Birman与 Thomas. Joseph的 Exploiting
virtual synchrony in distributed systems: http://www.cs.cornell.edwh/rvr/sys/p123-birman.pdf.
46第 Paxos3章的工程实践

<==========================57end ==============================>
<==========================58start==============================>

Chubby提供的另一种方式是使用锁序列器,当然该策略需要 Chubby的上层应用配合在
代码中加入相应的修改逻辑。任何时候,锁的持有者都可以向 Chubby请求一个锁序列
器,其包括锁的名字、锁模式(排他或共享模式),以及锁序号。当客户端应用程序在
进行一些需要锁机制保护的操作时,可以将该锁序列器一并发送给服务端。 Chubby服
务端接收到这样的请求后,会首先检测该序列器是否有效,以及检查客户端是否处于恰
当的锁模式;如果没有通过检查,那么服务端就会拒绝该客户端请求。
Chubby中的事件通知机制
为了避免大量客户端轮询 Chubby服务端状态所带来的压力, Chubby提供了事件通知机
制。 Chubby的客户端可以向服务端注册事件通知,当触发这些事件的时候,服务端就
会向客户端发送对应的事件通知。在 Chubby的事件通知机制中,消息通知都是通过异
步的方式发送给客户端的,常见的 Chubby事件如下。
文件内容变更
例如, BigTable集群使用 Chubby锁来确定集群中的哪台 Big Table机器是 Master
获得锁的 BigTable Master会将自身信息写入 Chubby上对应的文件中。 BigTable集
群中的其他客户端可以通过监视这个 Chubby文件的变化来确定新的 BigTable
Master机器。
节点删除
当 Chubby上指定节点被删除的时候,会产生“节点删除”事件,这通常在临时节点
中比较常见,可以利用该特性来间接判断该临时节点对应的客户端会话是否有效。
子节点新增、删除
当 Chubby上指定节点的子节点新增或是减少时,会产生“子节点新增、删除”事
件。
Master服务器转移
当 Chubby服务器发生 Master转移时,会以事件的形式通知客户端。
Chubby中的缓存
为了提高 Chubby的性能,同时也是为了减少客户端和服务端之间频繁的读请求对服务
端的压力, Chubby除了提供事件通知机制之外,还在客户端中实现了缓存,会在客户
端对文件内容和元数据信息进行缓存。使用缓存机制在提高系统整体性能的同时,也为
3.1 Chubby 47

<==========================58end ==============================>
<==========================59start==============================>

系统带来了一定的复杂性,其中最主要的问题就是应该如何保证缓存的一致性。在
Chubby中,通过租期机制来保证缓存的一致性。
Chubby缓存的生命周期和 Master租期机制紧密相关, Master会维护每个客户端的数据
缓存情况,并通过向客户端发送过期信息的方式来保证客户端数据的一致性。在这种机
制下, Chubby就能够保证客户端要么能够从缓存中访问到一致的数据,要么访问出错,
而一定不会访问到不一致的数据。具体的,每个客户端的缓存都有一个租期,一旦该租
期到期,客户端就需要向服务端续订租期以继续维持缓存的有效性。当文件数据或元数
据信息被修改时, Chubby服务端首先会阻塞该修改操作,然后由 Master向所有可能缓
存了该数据的客户端发送缓存过期信号,以使其缓存失效,等到 Master在接收到所有相
关客户端针对该过期信号的应答(应答包括两类,一类是客户端明确要求更新缓存,另
一类则是客户端允许缓存租期过期)后,再继续进行之前的修改操作。
通过上面这个缓存机制的介绍,相信读者都已经明白了, Chubby的缓存数据保证了强
一致性。尽管要保证严格的数据一致性对于性能的开销和系统的吞吐影响很大,但由于
弱一致性模型在实际使用过程中极容易出现问题,因此 Chubby在设计之初就决定了选
择强一致性模型。
会话和会话激活(KeepAlive)
Chubby客户端和服务端之间通过创建一个tC连接来进行所有的网络通信操作,我们
将这一连接称为会话(Session)。会话是有生命周期的,存在一个超时时间,在超时时
间内, Chubby客户端和服务端之间可以通过心跳检测来保持会话的活性,以使会话周
期得到延续,我们将这个过程称为 KeepAlive(会话激活)如果能够成功地通过
KeepAlive过程将 Chubby会话一直延续下去,那么客户端创建的句柄、锁和缓存数据等
都依然有效。
KeepAlive请求
下面我们就重点来看看 Chubby Master是如何处理客户端的 KeepAlive请求的。Master
在接收到客户端的 KeepAlive请求时,首先会将该请求阻塞住,并等到该客户端的当前
会话租期即将过期时,才为其续租该客户端的会话租期,之后再向客户端响应这个
KeepAlive请求,并同时将最新的会话租期超时时间反馈给客户端。 Master对于会话续
租时间的设置,默认是12秒,但这不是一个固定的值, Chubby会根据实际的运行情况,
自行调节该周期的长短。举个例子来说,如果当 Master前处于高负载运行状态的话,那
么 Master会适当地延长会话租期的长度,以减少客户端 KeepAlive请求的发送频率。客
户端在接收到来自 Master的续租响应后,会立即发起一个新的 KeepAlive请求,再由
48第3章 Paxos的工程实践

<==========================59end ==============================>
<==========================60start==============================>

Master进行阻塞。因此我们可以看出,在正常运行过程中,每一个 Chubby客户端总是
会有一个 KeepAlive请求阻塞在 Master服务器上
除了为客户端进行会话续租外, Master还将通过 KeepAlive响应来传递 Chubby事件通
知和缓存过期通知给客户端。具体的,如果 Master发现服务端已经触发了针对该客户端
的事件通知或缓存过期通知,那么会提前将 KeepAlive响应反馈给客户端。
会话超时
谈到会话租期, Chubby的客户端也会维持一个和 Master端近似相同的会话租期。为什
么是近似相同呢?这是因为客户端必须考虑两方面的因素:一方面, KeepAlive响应在
网络传输过程中会花费一定的时间;另一方面, Master服务端和 Chubby客户端存在时
钟不一致性现象。因此在Chubby会话中,存在 Master端会话租期和客户端本地会话租
期。
如果 Chubby客户端在运行过程中,按照本地的会话租期超时时间,检测到其会话租期
已经过期却尚未接收到 Master的 KeepAlive响应,那么这个时候,它将无法确定 Master
服务端是否已经中止了当前会话,我们称这个时候客户端处于“危险状态”。此时,Chubby
客户端会清空其本地缓存,并将其标记为不可用。同时,客户端还会等待一个被称作“宽
限期”的时间周期,这个宽限期默认是4秒。如果在宽限期到期前,客户端和服务端
之间成功地进行了 KeepAlive,那么客户端就会再次开启本地缓存,否则,客户端就会
认为当前会话已经过期了,从而中止本次会话。
我们再着重来看看上面提到的“危险状态”。当客户端进入上述提到的危险状态时,
Chubby的客户端库会通过一个“jeopardy”事件来通知上层应用程序。如果恢复正常,
客户端同样会以一个“safe”事件来通知应用程序可以继续正常运行了。但如果客户端
最终没能从危险状态中恢复过来,那么客户端会以一个“expired”事件来通知应用程序
当前 Chubby会话已经超时。 Chubby通过这些不同的事件类型通知,能够很好地辅助上
层应用程序在不明确 Chubby会话状态的情况下,根据不同的事件类型来做出不同的处
理:等待或重启。有了这样的机制保证之后,对于那些在短时间内 Chubby服务不可用
的场景下,客户端应用程序可以选择等待,而不是重启,这对于那些重启整个应用程序
需要花费较大代价的系统来说非常有帮助。
Chubby Master故障恢复
Chubby的 Master服务器上会运行着会话租期计时器,用来管理所有会话的生命周期。
如果在运行过程中 Master出现了故障,那么该计时器会停止,直到新的 Master选举产
生后,计时器才会继续计时,也就是说,从旧的 Master崩溃到新的 Master选举产生所
3.1 Chubby I49

<==========================60end ==============================>
<==========================61start==============================>

花费的时间将不计入会话超时的计算中,这等价于延长了客户端的会话租期。如果新的
Master在短时间内就选举产生了,那么客户端就可以在本地会话租期过期前与其创建连
接。而如 Master果的选举花费了较长的时间,就会导致客户端只能清空本地的缓存,并
进入宽限期进行等待。从这里我们可以看出,由于宽限期的存在,使得会话能够很好地
在服务端 Master转换的过程中得到维持。整个 Chubby Master故障恢复过程中服务端和
客户端的交互情况如图3-2所示。
ld mater des no master
lease M
Old Masterea
lease
Clent
Tease.
图3-2. Chubby Master故障恢复过程中服务端和客户端的交互示意图5
图3-2展示了一个完整的Chubby服务端 Master故障恢复过程中所触发的所有事件序列。
在这整个故障恢复过程中,客户端必须使用宽限期来保证在 Master转换过程完成之后,
其会话依然有效。在图3-2中,从左向右代表了时间轴的变化,使用横向粗箭头代表会
话租期,并且在图中通过“M”和“C”来分别标记 Master和客户端上的视图,例如“lease
M”和“lease”斜向上的箭头代表了客户端向 Master发出的 KcepAlive请求,而
斜向下的箭头则代表了 Master反馈的 KeepAlive响应。
从图3-2中我们可以看出,一开始在旧的 Master服务器上维持了会话租期“lease MI”,
在客户端上维持了对应的“lease CI”,同时客户端的 KeepAlive请求1一直被 Master阻
塞着。在一段时间之后, Master向客户端反馈了 KeepAlive响应2,同时开始了新的会
话租期“lease M2”,而客户端在接收到该 KeepAlive响应之后,立即发送了新的 KeepAlive
请求3,并同时也开始了新的会话租期“ lease C22”至此,客户端和服务端Master之间
的所有交互都是正常的。但是随后, Master发生了故障,从而无法反馈客户端的
KeepAlive请求3。在这个过程中,客户端检测到会话租期“lease C2”已经过期,它会
清空本地缓存,并进入宽限期。在这段时间内,客户端无法确定 Master上的会话周期是
否也已经过期,因此它不会销毁它的本地会话,而是将所有应用程序对它的AP调用都
阻塞住,以避免在这个期间进行的API调用导致数据不一致现象。同时,在客户端宽限
期开始时, Chubby客户端会向上层应用程序发送一个“jeopardy”事件。一段时间之后,
注5:本图片引自 Chubby官方论文 The Chubby lock service for loosely-coupled- distributed systems
50第 Paxos3章的工程实践

<==========================61end ==============================>
<==========================62start==============================>

Chubby服务端选举产生了新的 Master,并为该客户端初始化了新的会话租期“lease M3”。
当客户端向新的 Master发送 KeepAlive请求4时, Master检测到该客户端的 Master周
期号(Master epoch number)已经过期,因此会在 KeepAlive响应5中拒绝这个客户端
请求,并将最新的 MrMaster周期号发送给客户端。关于周期,将在后面的内容中
做详细讲解。之后,客户端会携带上新的 Master周期号,再次发送 KeepAlive请求6给
Master,最终,整个客户端和服务端之间的会话就会再次恢复正常。
通过上面的详细介绍,不难看出,在 Master转换的这段时间内,只要客户端的宽限期足
够长,那么客户端应用程序就可以在没有任何察觉的情况下,实现 Chubby的故障恢复,
但如果客户端的宽限期设置得比较短,那么 Chubby客户端就会丢弃当前会话,并将这
个异常情况通知给上层应用程序。
一旦客户端与新的 Master建立上连接之后,客户端和 Master之间会通过互相配合来实
现对故障的平滑恢复新的 Master会设法将上一个 Master服务器的内存状态构造出来。
具体的,由于本地数据库记录了每个客户端的会话信息,以及其持有的锁和临时文件等
信息,因此 Chubby会通过读取本地磁盘上的数据来恢复一部分状态。总的来讲,一个
新的 Chubby Master服务器选举产生之后,会进行如下几个主要处理。
1.新的 Master选举产生后,首先需要确定 Master周期 Master周期用来唯一标识一
个 Chubby集群的 Master统治轮次,以便区分不同的 Master一旦新的 Master周
期确定下来之后, Master就会拒绝所有携带其他 Master周期编号的客户端请求,
同时告知其最新的 Master周期编号,例如上述提到的 KeepAlive请求4需要注
意的一点是,只要发生Master重新选举就一定会产生新的 Master周期,即使是
在选举前后 Master都是同一台机器的情况下也是如此。
2.选举产生的新Master能够立即对客户端的 Master寻址请求进行响应,但是不会立
即开始处理客户端会话相关的请求操作。
3. Master根据本地数据库中存储的会话和锁信息,来构建服务器的内存状态。
4.到现在为止, Master已经能够处理客户端的 KeepAlive请求了,但依然无法处理
其他会话相关的操作。
5. Master会发送一个“Master故障切换事件给每一个会话,客户端接收到这个事
件后,会清空它的本地缓存,并警告上层应用程序可能已经丢失了别的事件,之
后再向 Master反馈应答。
6.此时, Master会一直等待客户端的应答,直到每一个会话都应答了这个切换事件。
3.1 Chubby 51

<==========================62end ==============================>
<==========================63start==============================>

7.在 Master接收到了所有客户端的应答之后,就能够开始处理所有的请求操作了。
8.如果客户端使用了一个在故障切换之前创建的句柄, Master会重新为其创建这个
句柄的内存对象,并执行调用。而如果该句柄在之前的 Master周期中已经被关闭
了,那么它就不能在这个Master周期内再次被重建了一这一机制就确保了即使
由于网络原因使得 Master接收到那些延迟或重发的网络数据包,也不会错误地重
建一个已经关闭的句柄。
3.1.5 Paxos协议实现
Chubby服务端的基本架构大致分为三层:
·最底层是容错日志系统(FaultTolerant- Log),通过Paxos算法能够保证集群中所
有机器上的日志完全一致,同时具备较好的容错性。
·日志层之上是Key--Value类型的容错数据库(Fault-Tolerant- DB)其通过下层的日
志来保证一致性和容错性。
存储层之上就是Chubby对外提供的分布式锁服务和小文件存储服务。
其整体架构如图3-3所示。
Chubby客户端
c用( Chubbyt协议
--------
Chubby服务端集群
文件传
Chubby服务
快照转( Fle trans
(Snapshot exchange
数据库
Fautt-Tolerant DB
容日志系统
Paxos协议
Fault- Tolerant Log
本地文件系统
事务日志数据快照
图3-3. Chubby单机整体架构图
Paxos算法的作用就在于保证集群内各个副本节点的日志能够保持一致 Chubby事务日
志中的每一个 Value对应 Paxos算法中的一个 Instance,由于 Chubby需要对外提供不间
断的服务,因此事务日志会无限增长,于是在整个 Chubby运行过程中,会存在多个 Paxos
Instance同时, Chubby会为每一个 Paxos Instance都按序分配一个全局唯一的 Instance
编号,并将其顺序写入到事务日志中去。
在多 Paxos Instance的模式下,为了提升算法执行的性能,就必须选举出一个副本节点
52第3章 Paxos的工程实践

<==========================63end ==============================>
<==========================64start==============================>

作为Paxos算法的主节点(以下简称Master或Paxos(MasteCoordiaory《PaxoS),以避免因为每一个axos
Instance都提出提案而陷入多个 Paxos Round并存的情况。同时, Paxos会保证在 Master
重启或出现故障而进行切换的时候,允许出现短暂的多个 Master共存却不影响副本之间
的一致性。
在 Paxos中,每一个 Paxos Instance都需要进行一轮或多轮“Prepare-Promise-Propose→→
→Accept”这样完整的二阶段请求过程来完成对一个提案值的选定,而多个 Instance之
间是完全独立的,每个 Instance可以自己决定每一个 Round的序号,仅仅只需要保证在
Instance内部不会出现序号重复即可。为了在保证正确性的前提下尽可能地提高算法运
行性能,可以让多个 Instance共用一套序号分配机制,并将“Prepare-Promise→”合并为
一个阶段,具体做法如下。
·当某个副本节点通过选举成为Master后就会使用新分配的编号N来广播一个
Prepare消息,该 Prepare消息会被所有未达成一致的 Instance和目前还未开始的
Instance共用。
当Acceptor接收到 Prepare消息后,必须对多个 Instance同时做出回应,这通常可
以通过将反馈信息封装在一个数据包中来实现假设最多允许K个 Instance同时进
行提案值的选定,那么:
当前至多存在K个未达成一致的 Instance,将这些未决的 Instance各自最后接
受的提案值(若该提案尚未接受任何值,则使用null来代替)封装进一个数据
包,并作为 Promise消息返回。
同时,判断N是否大 Acceptor于当前的 highestPromisedNum值(当前已经接
受的最大提案编号值),如果大于该值的话,那么就标记这些未决 Instance和所
有未来的 Instance的 highestPromisedNum值为N这样, Instance这些未决
和所有未来 Instance都不能再接受任何编号小于N的提案。
然后Master就可以对所有未决 Instance和所有未来 Instance分别执行“Propose→
Accept”阶段的处理。值得注意的是,如果当 Master前能够一直稳定运行的话,那
么在接下来的算法运行过程中,就不再需要进行“Prepare-Promise→”的处理了。
但是,一但 Master发现 Acceptor返回了一个 Reject消息,说明集群中存在另一个
Master,并且试图使用更大的提案编号(如M,其M>N)发送了Prepare消息
碰到这种情况,当前Master就需要重新分配新的提案编号(必须比M更大)并再次
进行“Prepare-Promise→”阶段的逻辑处理。
利用上述改进的 Paxos算法,在 Master稳定运行的情况下,只需要使用同一个编号来依
次执行每一个 Instance的“Promise-Accept→”阶段逻辑处理。在每个Instance的运行过
3.1 Chubby 53

<==========================64end ==============================>
<==========================65start==============================>

程中,一旦接收到多数派的Accept反馈后,就可以将对应的提案值写入本地事务日志
并广播 COMMIT消息给集群中的其他副本节点,其他副本节点在接收到这个 COMMIT
消息之后也会将提案值写入到事务日志中去。如果某个副本节点因为宕机或者网络原因
没有接收到 COMMIT消息,可以主动向集群中的其他副本节点进行查询。因此,我们
可以看到,在 Chubby的 Paxos算法的实现中,只要维持集群中存在多数派的机器能够
正常运行,即使其他机器在任意时刻发生宕机也能保证已经提交的提案的安全性。
至此,我们已经实现了一套满足一致性的日志副本,在此基础上就可以在上层实现一个
一致的状态机副本,这就是图3-3中的容错数据库(Fault-Tolerant- DB)层。在最初版本
的 Chubby中,使用了具有数据复制特性的 Berkeley DB(下文中我们简称“BDB”)
来作为它的容错数据库。BDB使用分布式一致性协议来进行集群中不同服务器之间数据
库日志的复制。BDB的底层实现采用了经典的B树数据结构,我们可以将其看作是一
个能够存储大量数据的 HashMap(映射)。在 Chubby的使用中,将每一个数据节点的节
点路径名作为键,同时按照节点路径名进行排序,这样就能够使得兄弟节点在排序顺序
中相邻,方便对数据节点的检索。因此 Chubby的设计变得非常简单,只需要在此基础
上添加上 Master租期特性即可。
但是在后来的开发维护过程中, Chubby的开发人员觉得使用BDB就会引入其他额外的
风险和依赖,因此自己实现了一套更为简单的、基于日志预写和数据快照技术的底层数
据复制组件,这样就大大简化了整个 Chubby的系统架构和实现逻辑。在本书中,对于
该容错数据库层在内存中的数据结构不做展开讨论,这里只对“数据快照和事务日志回
放”机制的实现做一个简单讲解。
集群中的某台机器在宕机重启以后,为了恢复状态机的状态,最简单的方法就是将已经
记录的所有事务日志重新执行一遍。但这会有一个明显的问题,就是如果机器上的事务
日志已经积累了很多,那么恢复的时间就会非常长,因此需要定期对状态机数据做一个
数据快照并将其存入磁盘,然后就可以将数据快照点之前的事务日志清除。
通常副本节点在进行宕机后的恢复过程中,会出现磁盘未损坏和损坏两种情况。前者最
为常见,一般通过磁盘上保存的数据库快照和事务日志就可以恢复到之前某个时间点的
状态,之后再向集群中其他正常运行的副本节点索取宕机后缺失的部分数据变更记录,
这样即可实现宕机后的数据恢复。另外一种则是磁盘损坏,无法直接从本地数据恢复的
情况。针对这种异常情况,就需要从其他副本节点上索取全部的状态数据。
注6: Berkeley DB是一个历史非常悠久的嵌入式数据库系统,于2006年被 Oracle收购,其官方主页是:
http://www.oracle. com/technetwork/database/database-te/berkeleydb/overview/index. html.
54第 Paxos3章的工程实践

<==========================65end ==============================>
<==========================66start==============================>

副本节点在完成宕机重启之后,为了安全起见,不会立即参与 Paxos Instance流程,而
是需要等待检测到K(K是允许并发的最大数目)个 Paxos Instance流程成功完
成之后才能开始参与这样就能够保证新分配的提案编号不会和自己以前发过的重
复。
最后,为了提高整个集群的性能,还有一个改进之处在于:得益于 Paxos算法的容错机
制,只要任意时刻保证多数派的机器能够正常运行,那么在宕机瞬间未能真正写入到磁
盘上(只有当真正调用操作系统Flush接口后,数据才能被真正写入物理磁盘中)的那
一小部分事务日志也可以通过从其他正常运行的副本上复制来进行获取,因此不需要实
时地进行事务日志的 Flush操作,这可以极大地提高事务写入的效率。
通过本小节的介绍,相信读者对 Chubby这一分布式锁服务已经有了一个比较全面的了
解。 Chubby并非是一个分布式一致性的学术研究,而是一个满足第2章中我们提到的
各种一致性需求的工程实践,感兴趣的读者可以在其他资料中对其进行进一步的了解。
3.2Hypertable
Hypertable是一个使用C++语言开发的开源、高性能、可伸缩的数据库,其以 Google
的 BigTable相关论文为基础指导,采用与 HBase非常相似的分布式模型,其目的是要
构建一个针对分布式海量数据的高并发数据库。
3.2.1概述
目前 Hypertable只支持最基本的添、删、改、查功能,对于事务处理和关联查询等关系
型数据库的高级特性都尚未支持。同时,就少量数据记录的查询性能和吞吐量而言,
Hypertable可能也不如传统的关系型数据库。和传统关系型数据库相比, Hypertable最
大的优势在于以下几点。
支持对大量并发请求的处理。
支持对海量数据的管理。
扩展性良好,在保证可用性的前提下,能够通过随意添加集群中的机器来实现水平
扩容。
●可用性极高,具有非常好的容错性,任何节点的失效,既不会造成系统瘫痪也不会
影响数据的完整性。
3.2 Hypertable

<==========================66end ==============================>
<==========================67start==============================>

Hypertable的整体架构如图3-3所示。
Master
(standbyl
RangeServer Rangese
RangeServer
DFS Broker DFS Broker DFS Broker DFS Broker
DFS Broker
分布式文件系统
HDFS, MapR. Ceph, locat, .
图3-3. Hypertable整体架构图
Hypertable的核心组件包括 Hyperspace、 RangeServer Master和 DFS Broker四部分其
中 Hyperspace是 Hypertable中最重要的组件之一,其提供了对分布式锁服务的支持以及
对元数据的管理,是保证 Hypertable数据一致性的核心。 Hyperspace类似于 Google
BigTable系统中的 Chubby,在这里我们可以认为它是一个文件存储系统,主要用来存
储一些元数据信息,同时提供分布式锁服务,另外还负责提供高效、可靠的主机选举服
务。
RangeServer是实际对外提供服务的组件单元负责数据的读取和写入。在Hypertable
的设计中,对每一张表都按照主键进行切分,形成多个 Range(类似于关系型数据库中
的分表),每个 Range由一个 RangeServer(RangeServer调用 DFSBroker来进行数据的
读写)负责管理。在 Hypertable中,通常会部署多个 RangeServer,每个 RangeServer都
负责管理部分数据,由 Master来负责进行 angeServer的集群管理
Master是元数据管理中心,管理包括创建表删除表或是其他表空间变更在内的所有元
数据操作,同时负责检测RangeServer的工作状态,一旦某一个 RangeServer宕机或是
重启,能够自动进行 Range的重新分配,从而实现对 RangeServer集群的管理和负载均
衡
。
DFS Broker则是底层分布式文件系统的抽象层,用于衔接上层 Hypertable和底层文件存
储。所有对文件系统的读写操作,都是通过 DFS Broker来完成的。目前已经可以接入
56第 Paxos3章的工程实践

<==========================67end ==============================>
<==========================68start==============================>

Hypertable中的分布式文件系统包括ds、 MapR7Ceph和KF等,针对任何
其他新的文件系统,只需要实现一个对应的 DFS Broker,就可以将其快速接入到整个
Hypertable系统中。
3.2.2算法实现
从上面的讲解中我们了解到, HyperspaceHypertable是整个中最为核心的部分之基于
对底BDB的封装,通过对 Paxos算法的实现, Hyperspace能够很好地保证 Hypertable中
元数据的分布式一致性。接下来我们就看看 Hyperspace是如何实现分布式数据一致性的。
Active Server
Hyperspace通常以一个服务器集群的形式部署一般由5~11台服务器组成,在运行过
程中,会从集群中选举产生一个服务器作为 Active Server,其余的服务器则是 Standby
Server,同时, Active Server和 Standby Server之间会进行数据和状态的实时同步。
在 Hypertable启动初始化阶段, Master模块会连接上 Hyperspace集群中的任意一台服务
器,如果这台 Hyperspace服务器恰好处于 Active状态,那么便完成了初始化连接;如
果连接上的 Hyperspace服务器处于 Standby状态,那么该 Hyperspace服务器会在此次连
接创建后,将当前处于 Active状态的 Hyperspace服务器地址发送给模块, Master
模块会重新与该 Active Hyperspace服务器建立连接,并且之后对 Hyperspace的所有操
作请求都会发送给这个 Hyperspace服务器换句话说,只有 Active Hyperspace才能真
正地对外提供服务。
事务请求处理
在 Hyperspace集群中,还有一个非常重要的组件,就是 BDB BDB服务也是采用集群
部署的,也存在 Master的角色,是Hyperspace底层实现分布式数据一致性的精华所在。
在 Hyperspace对外提供服务时,任何对于元数据的操作, Master模块都会将其对应的事
务请求发送给 Hyperspace服务器。在接收到该事务请求后, Hyperspace服务器就会向
BDB集群中的 Master服务器发起事务操作。BDB服务器在接收到该事务请求后,会在
集群内部发起一轮事务请求投票流程,一旦BDB集群内部过半的服务器成功应用了该
注7:MapR是 pR Technologies公司开发并开源的一款分布式计算和存储平台,其官方主页是:
http: //www.mapr.com/
注8:Ceph是一个 8: CephLinux PB http: //ceph.com/级的分布式文件系统,其官方主页是:htp:ceph.com
注 9: KFS (KosmosFS)C+ http:/code. google.是一个使用C++实现的分布式文件系统,其官方主页是:http:code.google
com/p/kosmosfs/
3.2 Hypertable 57

<==========================68end ==============================>
<==========================69start==============================>

事务操作,就会反馈Hyperspace服务器更新已经成功,再由 Hyperspace响应上层的
Master模块。
举个例子来说,假设有一个由5台服务器组成的 Hyperspace集群,其 Active Hyperspace
在处理一个建表请求时,需要获得至少3台BDB服务器的同意才能够完成写入虽然
这样的事务更新策略显然会严重影响其对写操作的响应速度,但由于其存入的元数据更
新并不特别频繁,因此对写性能的影响还在可接受的范围内毕竟数据的可靠一致才
是最重要的。
Active Hyperspace选举
当某台处于 Active状态的 Hyperspace服务器出现故障时,集群中剩余的服务器会自动
重新选举出新的 Active Hyperspace,这一过程称为 Hyperspace集群的 Active选举Active
选举过程的核心逻辑就是根据所有服务器上事务日志的更新时间来确定哪个服务器的
数据最新——事务日志更新时间越新,那么这台服务器被选举为 Active Hyperspace的可
能性就越大,因为只有这样,才能避免集群中数据不一致情况的发生。完成 Active
Hyperspace选举之后,余下所有的服务器就需要和 Active Hyperspace服务器进行数据同
步,即所有 Hyperspace服务器对应的BDB数据库的数据都需要和 Master BDB保持一
致。
从上面的讲解中我们可以看出,在整个 Hyperspace的设计中,为了使整个集群能够正常
地对外提供服务,那么就必须要求 Hyperspace集群中至少需要有超过一半的机器能够正
常运行。另外,在 Hyperspace集群正常对外提供服务的过程中,只有 Active Hyperspace
才能接受来自外部的请求,并且交由底层的BDB事务来保证一致性这样就能够保
证在存在大量并发操作的情况下,依然能够确保数据的一致性和系统的可靠性。
小结
对于不少工程师来说, Paxos算法本身晦涩难懂的算法描述,使得学习成本非常高,但
Paxos算法超强的容错能力和对分布式数据一致性的可靠保证,使其在工业界得到了广
泛的应用。本章通过对 Google Chubby和 Hypertable这两款经典的分布式产品中 Paxos
算法应用的介绍,向读者阐述了 Paxos算法在实际工业实践中的应用,为读者更好地理
解 Paxos算法提供了帮助。
58第3章 Paxos的工程实践

<==========================69end ==============================>
<==========================70start==============================>

第4章
ZooKeeper与 Paxos
Apache ZooKeeper是由 Apache Hadoop的子项目发展而来,于2010年11月正式成为了
Apache的顶级项目。 ZooKeeper为分布式应用提供了高效且可靠的分布式协调服务,提
供了诸如统一命名服务、配置管理和分布式锁等分布式的基础服务。在解决分布式数据
一致性方面, ZooKeeper并没有直接采用 Paxos算法,而是采用了一种被称为B
(ZooKeeper Atomic Broadcast)的一致性协议。
在本章中,我们将首先对 ZooKeeper进行一个整体上的介绍,包括 ZooKeeper的设计目
标、由来以及它的基本概念,然后将会重点介绍ZAB这 ZooKeeper中非常重要的一
致性协议。
4.1初ZooKeeper识
在本节中,我们会对 ZooKeeper进行一个初步的介绍,从 ZooKeeper是什么、 ZooKeeper
的由来及其基本概念展开,同时会向读者介绍使用 ZooKeeper来解决分布式一致性问题
的优势。
4.4.1.《ZooKeeper介绍
ZooKeeper是一个开放源代码的分布式协调服务,由知名互联网公司雅虎创建,是 Google
Chubby的开源实现 ZooKeeper的设计目标是将那些复杂且容易出错的分布式一致性服
务封装起来,构成一个高效可靠的原语集,并以一系列简单易用的接口提供给用户使用。
ZooKeeper是什么
ZooKeeper是一个典型的分布式数据一致性的解决方案,分布式应用程序可以基于它实
59

<==========================70end ==============================>
<==========================71start==============================>

现诸如数据发布/订阅、负载均衡、命名服务、分布式协调通知、集群管理、 Master选
举、分布式锁和分布式队列等功能。 ZooKeeper可以保证如下分布式一致性特性。
顺序一致性
从同一个客户端发起的事务请求,最终将会严格地按照其发起顺序被应用到
ZooKeeper中去
原子性
所有事务请求的处理结果在整个集群中所有机器上的应用情况是一致的,也就是说,
要么整个集群所有机器都成功应用了某一个事务,要么都没有应用,一定不会出现
集群中部分机器应用了该事务,而另外一部分没有应用的情况。
单一视图( Single System Image)
无论客户端连接的是哪个 ZooKeeper服务器,其看到的服务端数据模型都是一致的。
可靠性
一旦服务端成功地应用了一个事务,并完成对客户端的响应,那么该事务所引起的
服务端状态变更将会被一直保留下来,除非有另一个事务又对其进行了变更。
实时性
通常人们看到实时性的第一反应是,一旦一个事务被成功应用,那么客户端能够立
即从服务端上读取到这个事务变更后的最新数据状态。这里需要注意的是,
ZooKeeper仅仅保证在一定的时间段内,客户端最终一定能够从服务端上读取到最
新的数据状态。
ZooKeeper的设计目标
ZooKeeper致力于提供一个高性能、高可用,且具有严格的顺序访问控制能力(主要是
写操作的严格顺序性)的分布式协调服务。高性能使得 ZooKeeper能够应用于那些对系
统吞吐有明确要求的大型分布式系统中,高可用使得分布式的单点问题得到了很好的解
决,而严格的顺序访问控制使得客户端能够基于 ZooKeeper实现一些复杂的同步原语。
下面我们来具体看一下 ZooKeeper的四个设计目标。
目标一:简单的数据模型
ZooKeeper使得分布式程序能够通过一个共享的树型结构的名字空间来进行相互协调。
60第4章 ZooKeeper与 Paxos

<==========================71end ==============================>
<==========================72start==============================>

这里所说的树型结构的名字空间,是指 ZooKeeper服务器内存中的一个数据模型,其由
一系列被称为 ZNode的数据节点组成,总的来说,其数据模型类似于一个文件系统,而
ZNode之间的层级关系,就像文件系统的目录结构一样不过和传统的磁盘文件系统不
同的是, ZooKeeper将全量数据存储在内存中,以此来实现提高服务器吞吐、减少延迟
的目的。关于 ZooKeeper的数据模型,将会在7.1.1节中做详细阐述。
目标二:可以构建集群
一个 ZooKeeper集群通常由一组机器组成,一般3~5台机器就可以组成一个可用的
ZooKeeper集群了,如图4-1所示。
Client Clent Client
ClientClient ClientClient
图4-1. ZooKeeper的集群模式
组成 ZooKeeper集群的每台机器都会在内存中维护当前的服务器状态,并且每台机器之
间都互相保持着通信。值得一提的是,只要集群中存在超过一半的机器能够正常工作,
那么整个集群就能够正常对外服务。
ZooKeeper的客户端程序会选择和集群中任意一台机器共同来创建一个TCP连接,而一
旦客户端和某台 ZooKeeper服务器之间的连接断开后,客户端会自动连接到集群中的其
他机器。关于 ZooKeeper客户端的工作原理,将会在7.3节中做详细阐述。
目标三:顺序访问
对于来自客户端的每个更新请求, ZooKeeper都会分配一个全局唯一的递增编号,这个
编号反映了所有事务操作的先后顺序,应用程序可以使用 ZooKeeper的这个特性来实现
更高层次的同步原语。关于 ZooKeeper的事务请求处理和事务ID的生成,将会在7.8
节中做详细阐述。
目标四:高性能
由于 ZooKeeper将全量数据存储在内存中,并直接服务于客户端的所有非事务请求,因
注1:图片摘自 Zookeeper在 Allche的项目官网
4.1初识 ZooKeeper61

<==========================72end ==============================>
<==========================73start==============================>

此它尤其适用于以读操作为主的应用场景。作者曾经以3台3.4.3版本的 ZooKeeper服
务器组成集群进行性能压测,100%读请求的场景下压测结果是12~13w的QS
4.412《ZooKeeper从何而来
ZooKeeper最早起源于雅虎研究院的一个研究小组。在当时,研究人员发现,在雅虎内
部很多大型系统基本都需要依赖一个类似的系统来进行分布式协调,但是这些系统往往
都存在分布式单点问题。所以雅虎的开发人员就试图开发一个通用的无单点问题的分布
式协调框架,以便让开发人员将精力集中在处理业务逻辑上。
关于“ZooKeeper”这个项目的名字,其实也有一段趣闻。在立项初期,考虑到之前内
部很多项目都是使用动物的名字来命名的(例如著名的Pig项目),雅虎的工程师希望
给这个项目也取一个动物的名字。时任研究院的首席科学家 Raghu Ramakrishnan开玩笑
地说:“在这样下去,我们这儿就变成动物园了!”此话一出,大家纷纷表示就叫动物园
管理员吧因为各个以动物命名的分布式组件放在一起,雅虎的整个分布式系统看上
去就像一个大型的动物园了,而 ZooKeeper正好要用来进行分布式环境的协调于是,
ZooKeeper的名字也就由此诞生了。
4.1.3 ZooKeeper的基本概念
本节将介绍 ZooKeeper的几个核心概念。这些概念贯穿于本书之后对 ZooKeeper更深
的讲解,因此有必要预先了解这些概念。
集群角色
通常在分布式系统中,构成一个集群的每一台机器都有自己的角色,最典型的集群模式
就是 Master/Slave模式(主备模式)。在这种模式中,我们把能够处理所有写操作的机
器称为 Master机器,把所有通过异步复制方式获取最新数据,并提供读服务的机器称为
Save机器。
而在 ZooKeeper中,这些概念被颠覆了。它没有沿用传统的 MasterSlave概念,而是引
入了 Leader、 Follower和 Observer三种角色。 ZooKeeper集群中的所有机器通过一个
Leader选举过程来选定一台被称为“Leader”的机器, Leader服务器为客户端提供读和
写服务。除 LeaderFollower外,其他机器包括和 Observer Follower和 Observer都能
够提供读服务,唯一的区别在于, Observer机器不参与 Leader选举过程,也不参与写操
作的“过半写成功”策略,因此 Observer可以在不影响写性能的情况下提升集群的读性
能。关于 ZooKeeper的集群结构和各角色的工作原理,将会在7.7节中做详细阐述。
62第4章 ZooKeeper与 Paxos

<==========================73end ==============================>
<==========================74start==============================>

会话(Session)
Session是指客户端会话,在讲解会话之前我们首先来了解一下客户端连接。在
ZooKeeper中,一个客户端连接是指客户端和服务器之间的一个TCP长连接。ZooKeeper
对外的服务端口默认是2181,客户端启动的时候,首先会与服务器建立一个TCP连接,
从第一次连接建立开始,客户端会话的生命周期也开始了,通过这个连接,客户端能够
通过心跳检测与服务器保持有效的会话,也能够向 ZooKeeper服务器发送请求并接受响
应,同时还能够通过该连接接收来自服务器的 Watch事件通知 Session的 sessionTimeout
值用来设置一个客户端会话的超时时间。当由于服务器压力太大、网络故障或是客户端
主动断开连接等各种原因导致客户端连接断开时,只要在 session Timeout规定的时间内
能够重新连接上集群中任意一台服务器,那么之前创建的会话仍然有效。关于 ZooKeeper
客户端会话,将会在7.4节中做详细阐述。
数据节点(Znode)
在谈到分布式的时候,我们通常说的“节点”是指组成集群的每一台机器。然而,在
ZooKeeper中,“节点”分为两类,第一类同样是指构成集群的机器,我们称之为机器节
点;第二类则是指数据模型中的数据单元,我们称之为数据节点 ZNode ZooKeeper
将所有数据存储在内存中,数据模型是一棵树(ZNode Tree),由斜杠()进行分割的
路径,就是一个 Znode,例如foo/path1每个ZNode上都会保存自己的数据内容,同时
还会保存一系列属性信息。
在 ZooKeeper中, ZNode可以分为持久节点和临时节点两类。所谓持久节点是指一旦这
个 ZNode被创建了,除非主动进行 ZNode的移除操作,否则这个 ZNode将一直保存在
ZooKeeper上。而临时节点就不一样了,它的生命周期和客户端会话绑定,一旦客户端
会话失效,那么这个客户端创建的所有临时节点都会被移除。另外, ZooKeeper还允许
用户为每个节点添加一个特殊的属性: SEQUENTIA一旦节点被标记上这个属性,那
么在这个节点被创建的时候, ZooKeeper会自动在其节点名后面追加上一个整型数字,
这个整型数字是一个由父节点维护的自增数字。
关于 ZooKeeper的节点特性以及完整的数据模型,将会在7.1节中做详细阐述。
版本
在前面我们已经提到, ZooKeeper的每个 ZNode上都会存储数据,对应于每个 zNode,
ZooKeeper都会为其维护一个叫作Stat的数据结构,Stat中记录了这个 ZNode的三个数
据版本,分别是 version(当前 ZNode的版本)、 cversion(当前 ZNode子节点的版本)
和 aversion(当前 ZNode的ACL版本)关于 FZooKeeper数据模型中的版本,将会在7.1.3
4.1初识 ZooKeeper63

<==========================74end ==============================>
<==========================75start==============================>

节中做详细阐述。
Watcher
Watcher(事件监听器),是 ZooKeeper中的一个很重要的特性。 ZooKeeper允许用户在
指定节点上注册一些 Watcher,并且在一些特定事件触发的时候, ZooKeeper服务端会
将事件通知到感兴趣的客户端上去,该机制是 ZooKeeper实现分布式协调服务的重要特
性。关于 ZooKeeper Watcher机制的特性和使用,将会在7.1.4节中做详细阐述。
ACL
ZooKeeper采用C(Access Control Lists)策略来进行权限控制,类似于UN文件
系统的权限控制。 ZooKeeper定义了如下5种权限。
CREATE:创建子节点的权限。
READ:获取节点数据和子节点列表的权限。
WRITE:更新节点数据的权限。
DELETE:删除子节点的权限。
ADMIN:设置节点ACL的权限。
其中尤其需要注意的是, CREATE和 DELETE这两种权限都是针对子节点的权限控制。
关于 ZooKeeper权限控制的原理和使用方式,将会在7.1.5节中做详细阐述。
4.1.4为什ZooKeeper么选择
在本书引言部分,我们已经讲到,随着分布式架构的出现,越来越多的分布式应用会面
临数据一致性问题。很遗憾的是,在解决分布式数据一致性上,除了 ZooKeeper之外,
目前还没有一个成熟稳定且被大规模应用的解决方案。 ZooKeeper无论从性能、易用性
还是稳定性上来说,都已经达到了一个工业级产品的标准。
其次, ZooKeeper是开放源代码的,所有人都在关注它的发展,都有权利来贡献自己的力
量,你可以和全世界成千上万的 ZooKeeper开发者们一起交流使用经验,共同解决问题。
另外, ZooKeeper是免费的,你无须为它支付任何费用。这点对于一个小型公司,尤其
是初创团队来说,无疑是非常重要的。
最后, ZooKeeper已经得到了广泛的应用。诸如Hadoop、 HBase、 Storm和solr等越来
64第4章 ZooKeeper与 Paxos

<==========================75end ==============================>
<==========================76start==============================>

越多的大型分布式项目都已经将 ZooKeeper作为其核心组件,用于分布式协调。
4.4.2《ZooKeeperZAB的ZAB协议
在第2、3两章中,我们已经详细地讲解了其他的分布式一致性协议,在本小节中,我
们将围绕 ZooKeeper官方的几篇论文资料来看看 ZooKeeper中的一致性协议。
4.2.1ZAB协议
在深入了解 ZooKeeper之前,相信很多读者都会认为 ZooKeeper就是 Paxos算法的一个
实现。但事实上, ZooKeeper并没有完全采用axos算法,而是使用了一种称为 ZooKeeper
Atomic Broadcast(zab, ZooKeeper原子消息广播协议)的协议作为其数据一致性的核
心算法。
ZAB协议是为分布式协调服务 ZooKeeper专门设计的一种支持崩溃恢复的原子广播协
议。ZAB协议的开发设计人员在协议设计之初并没有要求其具有很好的扩展性,最初只
是为雅虎公司内部那些高吞吐量、低延迟、健壮、简单的分布式系统场景设计的。在
ZooKeeper的官方文档中也指出,ZAB协议并不像 Paxos算法那样,是一种通用的分布
式一致性算法,它是一种特别为 ZooKeeper计的崩溃可恢复的原子消息广播算法。
在 ZooKeeper中,主要依赖ZAB协议来实现分布式数据一致性,基于该协议,ZooKeeper
实现了一种主备模式的系统架构来保持集群中各副本之间数据的一致性。具体的,
ZooKeeper使用一个单一的主进程来接收并处理客户端的所有事务请求,并采用ZAB的
原子广播协议,将服务器数据的状态变更以事务 Proposal的形式广播到所有的副本进程
上去。ZAB协议的这个主备模型架构保证了同一时刻集群中只能够有一个主进程来广播
服务器的状态变更,因此能够很好地处理客户端大量的并发请求。另一方面,考虑到在
分布式环境中,顺序执行的一些状态变更其前后会存在一定的依赖关系,有些状态变更
必须依赖于比它早生成的那些状态变更,例如变更C需要依赖变更A和变更B这样的
依赖关系也对ZAB协议提出了一个要求ZAB协议必须能够保证一个全局的变更序列
被顺序应用,也就是说,ZAB协议需要保证如果一个状态变更已经被处理了,那么所有
其依赖的状态变更都应该已经被提前处理掉了。最后,考虑到主进程在任何时候都有可
注2:本书中涉及的ZAB协议相关的论文主要包括雅虎研究院的 Benjamin Reed、 Flavio. Junqueira
和 Marco Serafini写的 simple totally ordered broadcast protocol以及Zab:high- performance
broadcast for primary-backup systems, http://dl.acm.org/citation.cfm?id=-backupsstems,读者可以分别访问htpla.orgcitation.cfmid
1529978htp和 /al acmorgcitationcfimid2056409阅读论文全文。
4.1初ZooKeeper识65

<==========================76end ==============================>
<==========================77start==============================>

能出现崩溃退出或重启现象,因此,ZAB协议还需要做到在当前主进程出现上述异常情
况的时候,依旧能够正常工作。
ZAB协议的核心是定义了对于那些会改变 ZooKeeper服务器数据状态的事务请求的处
理方式,即:
所有事务请求必须由一个全局唯一的服务器来协调处理,这样的服务器被称为 Leader
服务器,而余下的其他服务器则成为 Follower服务器 Leader服务器负责将一个客户
端事务请求转换成一个事务 Proposal(提议),并将该 Proposal分发给集群中所有的
Follower服务器。之后 Leader服务器需要等待所有 Follower服务器的反馈,一旦超过
半数的 Follower服务器进行了正确的反馈后,那么 Leader就会再次向所有的 Follower
服务器分发 Commit消息,要求其将前一个 Proposal进行提交
4.2.2协议介绍
从上面的介绍中,我们已经了解了ZAB协议的核心,现在我们就来详细地讲解下AB
协议的具体内容。ZAB协议包括两种基本的模式,分别是崩溃恢复和消息广播。当整个
服务框架在启动过程中,或是当 Leader服务器出现网络中断、崩溃退出与重启等异常情
况时,ZAB协议就会进入恢复模式并选举产生新的 Leader服务器。当选举产生了新的
Leader服务器,同时集群中已经有过半的机器与该 Leader服务器完成了状态同步之后,
ZAB协议就会退出恢复模式。其中,所谓的状态同步是指数据同步,用来保证集群中存
在过半的机器能够和 Leader服务器的数据状态保持一致。
当集群中已经有过半的 Follower服务器完成了和 Leader服务器的状态同步,那么整个
服务框架就可以进入消息广播模式了。当一台同样遵守ZAB协议的服务器启动后加入
到集群中时,如果此时集群中已经存在一个 Leader服务器在负责进行消息广播,那么新
加入的服务器就会自觉地进入数据恢复模式:找到 Leader所在的服务器,并与其进行数
据同步,然后一起参与到消息广播流程中去正如上文介绍中所说的, ZooKeeper设计
成只允许唯一的一个 Leader服务器来进行事务请求的处理。 Leader服务器在接收到客
户端的事务请求后,会生成对应的事务提案并发起一轮广播协议;而如果集群中的其他
机器接收到客户端的事务请求,那么这些非 Leader服务器会首先将这个事务请求转发给
Leader服务器。
当 Leader服务器出现崩溃退出或机器重启,亦或是集群中已经不存在过半的服务器与该
Leader服务器保持正常通信时,那么在重新开始新一轮的原子广播事务操作之前,所有
进程首先会使用崩溃恢复协议来使彼此达到一个一致的状态,于是整个ZAB流程就会
从消息广播模式进入到崩溃恢复模式。
66第4章 ZooKeeper与 Paxos

<==========================77end ==============================>
<==========================78start==============================>

一个机器要成为新的 Leader,必须获得过半进程的支持,同时由于每个进程都有可能会
崩溃,因此,在ZAB协议运行过程中,前后会出现多个 Leader,并且每个进程也有可
能会多次成为 Leader。进入崩溃恢复模式后,只要集群中存在过半的服务器能够彼此进
行正常通信,那么就可以产生一个新的 Leader并再次进入消息广播模式举个例子来说,
一个由3台机器组成的ZAB服务,通常由1个 Leader、2个 Follower服务器组成。某
一个时刻,假如其中一个Follower服务器挂了,整个ZAB集群是不会中断服务的,这
是因为 Leader服务器依然能够获得过半机器(包括Leader自己)的支持
接下来我们就重点讲解一下ZAB协议的消息广播和崩溃恢复过程。
消息广播
ZAB协议的消息广播过程使用的是一个原子广播协议,类似于一个二阶段提交过程。针
对客户端的事务请求, Leader服务器会为其生成对应的事务 Proposal,并将其发送给集
群中其余所有的机器,然后再分别收集各自的选票,最后进行事务提交,如图4-2所示
就是ZAB协议消息广播流程的示意图。
Request
Propose
Propose
Ack
Ack
Commit
Commit
Lead
Follower
图4-2.ZAB协议消息广播流程示意图
在第2章中,我们已经详细讲解了关于二阶段提交协议的内容,而此处ZAB协议中涉
及的二阶段提交过程则与其略有不同。在ZAB协议的二阶段提交过程中,移除了中断
逻辑,所有的 Follower服务器要么正常反馈 Leader提出的事务 Proposal,要么就抛弃
Leader服务器。同时,ZAB协议将二阶段提交中的中断逻辑移除意味着我们可以在过半
的 Follower服务器已经反馈Ack之后就开始提交事务 Proposal了,而不需要等待集群
中所有的 Follower服务器都反馈响应。当然,在这种简化了的二阶段提交模型下,是无
法处理 Leader服务器崩溃退出而带来的数据不一致问题的,因此在ZAB协议中添加了
另一个模式,即采用崩溃恢复模式来解决这个问题。另外,整个消息广播协议是基于具
有FFO特性的TCP协议来进行网络通信的,因此能够很容易地保证消息广播过程中消
息接收与发送的顺序性。
4.1初识 ZooKeeper67

<==========================78end ==============================>
<==========================79start==============================>

在整个消息广播过程中, Leader服务器会为每个事务请求生成对应的 Proposal来进行广
播,并且在广播事务 Proposal之前, Leader服务器会首先为这个事务 Proposal分配一个
全局单调递增的唯一ID,我们称之为事务D(即ZD)由于ZAB协议需要保证每一
个消息严格的因果关系,因此必须将每一个事务 Proposal按照其ZXID的先后顺序来进
行排序与处理。
具体的,在消息广播过程中, Leader服务器会为每一个 Follower服务器都各自分配一个
单独的队列,然后将需要广播的事务 Proposal依次放入这些队列中去,并且根据fi
策略进行消息发送。每一个 Follower服务器在接收到这个事务 Proposal之后,都会首先
将其以事务日志的形式写入到本地磁盘中去,并且在成功写入后反馈给 Leader服务器一
个Ack响应 Leader。当服务器接收到超过半数 Follower的ack响应后,就会广播一个
Commit消息给所有的 Follower服务器以通知其进行事务提交,同时 Leader自身也会完
成对事务的提交,而每一个 Follower服务器在接收到 Commit消息后,也会完成对事务
的提交。
崩溃恢复
上面我们主要讲解了ZAB协议中的消息广播过程。ZAB协议的这个基于原子广播协议
的消息广播过程,在正常情况下运行非常良好,但是一旦 Leader服务器出现崩溃,或者
说由于网络原因导致 Leader服务器失去了与过半 Follower的联系,那么就会进入崩溃
恢复模式。在ZAB协议中,为了保证程序的正确运行,整个恢复过程结束后需要选举
出一个新的 Leader服务器。因此,ZAB协议需要一个高效且可靠的 Leader选举算法,
从而确保能够快速地选举出新的 Leader同时, LeerLeader选举算法不仅仅需要让
自己知道其自身已经被选举为 Leader,同时还需要让集群中的所有其他机器也能够快速
地感知到选举产生的新的 Leader服务器。
基本特性
根据上面的内容,我们了解到,ZAB协议规定了如果一个事务 Proposal在一台机器上被
处理成功,那么应该在所有的机器上都被处理成功,哪怕机器出现故障崩溃。接下来我
们看看在崩溃恢复过程中,可能会出现的两个数据不一致性的隐患及针对这些情况AB
协议所需要保证的特性。
ZAB协议需要确保那些已经在 Leader服务器上提交的事务最终被所有服务器都提交
假设一个事务在 Leader服务器上被提交了,并且已经得到过半 Follower服务器的
Ack反馈,但是在它将 Commit消息发送给所有 Follower机器之前, Leader服务器
挂了,如图4-3所示。
68第4章 ZooKeeper与 Paxos

<==========================79end ==============================>
<==========================80start==============================>

P1P2C1P3C2
Server2
Server3
P1P2C1
P1P2
图4-3.崩溃恢复过程需要确保已经被 Leader提交的 Proposal也能够被所有的 Follower提交
图4-3中的消息C2就是一个典型的例子:在集群正常运行过程中的某一个时刻,
Serverl1是 Leader服务器,其先后广播了消息P1、P2、C1、P3和C2,其中,当
Leader服务器将消息C2(C2是Commit Of Proposal22的缩写,即提交事务 Proposal22)
发出后就立即崩溃退出了。针对这种情况ZAB协议就需要确保事务 Proposal22最
终能够在所有的服务器上都被提交成功,否则将出现不一致。
ZAB协议需要确保 Leader丢弃那些只在服务器上被提出的事务
相反,如果在崩溃恢复过程中出现一个需要被丢弃的提案,那么在崩溃恢复结束后
需要跳过该事务 Proposal,如图4-4所示。
Server
P1P2C1P3C2
Senver2
Server3
P1 P2
P1P2
1C2P10.01P10.02C10.01
C1C210.01P10.0210.01
图4-4.崩溃恢复过程需要跳过那些已经被丢弃的事务 Proposal
在图4-4所示的集群中,假设初始的 Leader服务器 Serverl在提出了一个事务
Proposal33之后就崩溃退出了,从而导致集群中的其他服务器都没有收到这个事务
Proposal.于是,当 Serverl恢复过来再次加入到集群中的时候,ZAB协议需要确
保丢弃 Proposal33这个事务。
结合上面提到的这两个崩溃恢复过程中需要处理的特殊情况,就决定了ZAB协议必须
设计这样一个 Leader选举算法:能够确保提交已经被 Leader提交的事务 Proposal,同
时丢弃已经被跳过的事务 Proposal。针对这个要求,如果让 Leader选举算法能够保证新
4.1初识 ZooKeeper69

<==========================80end ==============================>
<==========================81start==============================>

选举出来的 Leader服务器拥有集群中所有机器最高编号(即ID最大)的事务 Proposal,
那么就可以保证这个新选举出来的 Leader一定具有所有已经提交的提案。更为重要的是,
如果让具有最高编号事务 Proposal的机器来成为 Leader,就可以省去 Leader服务器检
查 Proposal的提交和丢弃工作的这一步操作了。
数据同步
完成 Leader选举之后,在正式开始工作(即接收客户端的事务请求,然后提出新的提案)
之前, Leader服务器会首先确认事务日志中的所有 Proposal是否都已经被集群中过半的
机器提交了,即是否完成数据同步。下面我们就来看看ZAB协议的数据同步过程。
所有正常运行的服务器,要么成为 Leader,要么成为 Follower并和 Leader保持同步
Leader服务器需要确保所有的 Follower服器能够接收到每一条事务 Proposal,并且能
够正确地将所有已经提交了的事务 Proposal应用到内存数据库中去。具体的, Leader服
务器会为每一个 Follower服务器都准备一个队列,并将那些没有被各 Follower服务器同
步的事务以 Proposal消息的形式逐个发送给 Follower服务器,并在每一个 Proposal消息
后面紧接着再发送一个 Commit消息,以表示该事务已经被提交。等到 Follower服务器
将所有其尚未同步的事务 Proposal都从 Leader服务器上同步过来并成功应用到本地数
据库中后, Leader服务器就会将该 Follower服务器加入到真正的可用 Follower列表中,
并开始之后的其他流程。
上面讲到的是正常情况下的数据同步逻辑下面来看ZAB协议是如何处理那些需要被
丢弃的事务 Proposal的。在ZAB协议的事务编号ZXID设计中,ZXD是一个64位的
数字,其中低32位可以看作是一个简单的单调递增的计数器,针对客户端的每一个事
务请求, Leader服务器在产生一个新的事务 Proposal的时候,都会对该计数器进行加1
操作;而高32位则代表了 Leader周期 epoch的编号,每当选举产生一个新的 Leader服
务器,就会从这个 Leader服务器上取出其本地日志中最大事务 Proposal的ZXID,并从
该ZXD中解析出对应的 epoch值,然后再对其进行加1操作,之后就会以此编号作为
新的 epoch,并将低32位置0来开始生成新的ZXIDZAB协议中的这一通过 epoch编
号来区分 Leader周期变化的策略,能够有效地避免不同的 Leader服务器错误地使用相
同的ZXD编号提出不一样的事务 Proposal的异常情况,这对于识别在 Leader崩溃恢复
前后生成的 Proposal非常有帮助,大大简化和提升了数据恢复流程。
基于这样的策略,当一个包含了上一个Leader周期中尚未提交过的事务 Proposal的服
务器启动时,其肯定无法成为 Leader原因很简单,因为当前集群中一定包含一个
Quorum集合,该集合中的机器一定包含了更高 epoch的事务 Proposal,因此这台机器的
事务 Proposal肯定不是最高,也就无法成为 Leader了。当这台机器加入到集群中,以
70第4章 ZooKeeper与 Paxos

<==========================81end ==============================>
<==========================82start==============================>

Follower角色连接上 Leader服务器之后, Leader服务器会根据自己服务器上最后被提交
的 Proposal来和 Follower服务器的 Proposal进行比对,比对的结果当然是Leader会要
求 Follower进行一个回退操作—回退到一个确实已经被集群中过半机器提交的最新
的事务 Proposal。举个例子来说,在图4-4中当 Serverl连接上 Leader后, Leader会
要求 Serverl去除P3。
4.2.3深入ZAB协议
在4.2.2节中,我们已经基本介绍了ZAB协议的大体内容以及在实际运行过程中消息广
播和崩溃恢复这两个基本的模式,下面将从系统模型、问题描述、算法描述和运行分析
四方面来深入了解ZAB协议。
系统模型
在深入讲解ZAB协议之前,我们先来抽象地描述下ZAB协议需要构建的分布式系统模
型。通常在一个由一组进程∏={PP2Pn}组成的分布式系统中,其每一个进程都
具有各自的存储设备,各进程之间通过相互通信来实现消息的传递。一般的,在这样的
一个分布式系统中,每一个进程都随时有可能会出现一次或多次的崩溃退出,当然,这
些进程会在恢复之后再次加入到进程组中去。如果一个进程正常工作,那么我们称该
进程处于UP状态;如果一个进程崩溃了,那么我们称其处于DOWN状态。事实上,
当集群中存在过半的处于UP状态的进程组成一个进程子集之后,就可以进行正常的消
息广播了。我们将这样的一个进程子集称为 Quorum(下文中使用“Q来表示),并假
设这样的Q已经存在,其满足:
vQ,Q
q和Q2,QQ2≠0
上述集合关系式表示,存在这样的一个进程子集Q,其必定是进程组∏的子集;同时,
存在任意两个进程子集Q1和Q2,其交集必定非空。
我们使用P1和P来分别表示进程组∏中的两个不同进程,使用C来表示进程P和P之
间的网络通信通道,其满足如下两个基本特性。
完整性( Integrity)
进程P如果收到来自进程P的消息m,那么进程P一定确实发送了消息m。
4.1初识 ZooKeeper71

<==========================82end ==============================>
<==========================83start==============================>

前置性(Prefix)
如果进程P收到了消息m,那么存在这样的消息m:如果消息m是消息m的前置
消息,那么P务必先接收到消息m',然后再接收到消息m。我们将存在这种前置
性关系的两个消息表示为:m<m。前置性是整个协议设计中最关键的一点,由于
每一个消息都有可能是基干之前的消息来进行的,因此所有的消息都必须按照严格
的先后顺序来进行处理。
问题描述
在了解了ZAB协议所针对应用的系统模型后,我们再来看看其所要解决的实际分布式
问题。在前文的介绍中,我们已经了解到 ZooKeeper是一个高可用的分布式协调服务,
在雅虎的很多大型系统上得到应用。这类应用有个共同的特点,即通常都存在大量的客
户端进程,并且都依赖 ZooKeeper来完成一系列诸如可靠的配置存储和运行时状态记录
等分布式协调工作。鉴于这些大型应用对 ZooKeeper的依赖,因此 Zookeeper必须具备
高吞吐和低延迟的特性,并且能够很好地在高并发情况下完成分布式数据的一致性处理,
同时能够优雅地处理运行时故障,并具备快速地从故障中恢复过来的能力。
ZAB协议是整个 ZooKeeper框架的核心所在,其规定了任何时候都需要保证只有一个主
进程负责进行消息广播,而如果主进程崩溃了,就需要选举出一个新的主进程。主进程
的选举机制和消息广播机制是紧密相关的随着时间的推移,会出现无限多个主进程并
构成一个主进程序列:P1,P2,…,Pe-1,Pe其中P,e表示主进程序列号,也被称作
主进程周期。对于这个主进程序列上的任意两个主进程来说,如果e小于e',那么我们
就说P是P之前的主进程,通常使用P来表示需要注意的是,由于各个进程都
会发生崩溃然后再次恢复,因此会出现这样的情况:存在这样的P和P,它们本质上
是同一个进程,只是处于不同的周期中而已。
主进程周期
为了保证主进程每次广播出来的事务消息都是一致的,我们必须确保ZA协议只有在
充分完成崩溃恢复阶段之后,新的主进程才可以开始生成新的事务消息广播。为了实现
这个目的,我们假设各个进程都实现了类似于 readye)这样的一个函数调用,在运行过
程中,ZAB协议能够非常明确地告知上层系统(指主进程和其他副本进程)是否可以开
始进行事务消息的广播,同时,在调用 readye)函数之后,ZAB还需要为当前主进程设
置一个实例值。实例值用于唯一标识当前主进程的周期,在进行消息广播的时候,主进
程使用该实例值来设置事务标识中的 epoch字段当然,ZAB需要保证实例值在不同
的主进程周期中是全局唯一的。如果一个主进程周期e早于另一个主进程周期e,那么
将其表示为。
72第4章 ZooKeeper与 Paxos

<==========================83end ==============================>
<==========================84start==============================>

事务
我们假设各个进程都存在一个类似于 transactions(v,z)这样的函数调用,来实现主进程对
状态变更的广播。主进程每次对 transaction,z)函数的调用都包含了两个字段:事务内
容v和事务标识z,而每一个事务标识z=<e,也包含两个组成部分,前者是主进程周
期e,后者是当前主进程周期内的事务计数c我们使用 epoch(z)来表示一个事务标识中
的主进程周期 epoch,使用 counter(z)来表示事务标识中的事务计数。
针对每一个新的事务,主进程都会首先将事务计数c递增。在实际运行过程中,如果一
个事务标识z优先于另一个事务标识z',那么就有两种情况:一种情况是主进程周期不
同,即 epoch(z)< epoch(z);另一种情况则是主进程周期一致,但是事务计数不同,
即 epoch(z)=epoch(z)且 counterz)< counter(z),无论哪种情况,均使用z来表
示。
算法描述
下面我们将从算法描述角度来深入讲解ZAB协议的内部原理。整个ZAB协议主要包括
消息广播和崩溃恢复两个过程,进一步可以细分为三个阶段,分别是发现(Discovery)
同步(Synchronization)和广播(Broadcast)段。组成ZAB协议的每一个分布式进程,
会循环地执行这三个阶段,我们将这样一个循环称为一个主进程周期。
为了更好地对ZAB协议各阶段的算法流程进行描述,我们首先定义一些专有标识和术
语,如表4-1所示。
表4-1.ZAB协议算法表述术语介绍
术语名
说明
F
Follower fProposal处理过的最后一个事务
F. zxid
Follower fProposal处理过的历史事务中最后一个事务 Proposal的事务标识zxi
每一个 Follower通常都已经处理(接受)了不少事务 Proposal并且会有一个针
h
对已经处理过的事务的集合,将其表示为hf表示 Follower fe已经处理过的事务
序列
Ie
初始化历史记录,在某一个主进程周期 epoch中,当准 Leader完成阶段一之后,
此时它的hr就被标记为1.关于ZAB协议的阶段一过程,将在下文中做详细讲解
下面我们就从发现、同步和广播这三个阶段展开来讲解ZAB协议的内部原理。
阶段一:发现
阶段一主要就是 Leader选举过程,用于在多个分布式进程中选举出主进程,准 Leader
和 Follower的工作流程分别如下。
4.1初识 ZooKeeper73

<==========================84end ==============================>
<==========================85start==============================>

步骤F.1.1 Follower将自己最后接受的事务 Proposal的 epoch值 CEPOCH()发送
给准 Leader
步骤L.1.1当接收到来自过半 Follower CEPOCHfp)消息后,准 Leader会生成
NEWEPOCH(e')消息给这些过半的 Follower
关于这个 epoch值e',准 Leader会从所有接收到的 CEPOCH)消息中
选取出最大的 epoch值,然后对其进行加1操作,即为e'
步骤F.1.2当 Follower接收到来自准 Leader NEWEPOCHe)消息后,如果其检
测到当前的 CEPOCH()值小于e',那么就会将 CEPOCH(-)赋值为e',
同时向这个准 Leader反馈Ack消息。在这个反馈消息(ack-efh)
中,包含了当前该 Follower的 epoch CEPOCH(p),以及该 Follower的历
史事务 Proposal集合:ht
当 Leader接收到来自过半 Follower的确认消息Ack之后, Leader就会从这过半服
务器中选取出一个 Follower,并使用其作为初始化事务集合
关于这个 Follower的选取,对于 Quorum中其他任意一个 Follower F',F需要满足以
下两个条件中的一个:
CEPOCH(.)< CEPOCH (F.)
(CEPOCH (.) =CEPOCH (F. ).xid F.zxid F'. zxid =F.zxid)
至此,ZAB协议完成阶段一的工作流程。
阶段二:同步
在完成发现流程之后,就进入了同步阶段。在这一阶段中, Leader和 Follower的工
作流程分别如下。
步骤L.2.1 Leader会将e和le'以 NEWLEADERe,)消息的形式发送给所有 Quorum
中的 Follower
步骤F.2.1当 Follower接收到来自 Leader的 NEWLEADERe'消息后,如果
Follower发现 CEPOCH(F)≠e',那么直接进入下一轮循环,因为此时
Follower发现自己还在上一轮,或者更上轮,无法参与本轮的同步。
如果 CEPOCH(F)=e',那么 Follower就会执行事务应用操作。具体的,
对于每一个事务 Proposal:<v,z>el, Follower都会接受<e,<vz
74第4章 ZooKeeper与 Paxos

<==========================85end ==============================>
<==========================86start==============================>

最后, Follower会反馈给 Leader,表明自己已经接受并处理了所有中的
事务 Proposal
步骤L.2.2当 Leader接收到来自过半 Follower针对 NEWLEADER(e)的反馈消息后,
就会向所有的 Follower发送 CommitLeader消息。至此完成阶段二
步骤F.2.2当 Follower收到来自 Leader的 Commit消息后,就会依次处理并提交所有
在中未处理的事务。至此 Follower完成阶段二。
阶段三:广播
完成同步阶段之后,ZAB协议就可以正式开始接收客户端新的事务请求,并进行消息广
播流程。
步骤L.3.1 Leader接收到客户端新的事务请求后,会生成对应的事务 Proposal,并
根据ZXID的顺序向所有 Follower发送提案e,<v,z>,其中 epoch(z)
e
步骤F.3.1 Follower根据消息接收的先后次序来处理这些来自 Leader的事务 Proposal
并将他们追加到h中去,之后再反馈给 Leader
步骤L.3.1当 Leader接收到来自过半 Follower针对事务 Proposal<e,<v,z>的Ack
消息后,就会发送 Commit<e,<v,z>>消息给所有的 Follower,要求它们
进行事务的提交。
步骤F.3.2当 Follower接收到来自 Leader的 Commit<e,<vz>消息后,就会开始
提交事务 Proposal<e,<v,z>。需要注意的是,此时该 Follower必定已
经提交了事务 Proposal<v,z>,其中<v,z>∈hr,z=Z。
以上就是整个ZAB协议的三个核心工作流程,如图45所示是在整个过程中各进程之
间的消息收发情况,各消息说明依次如下:
CEPOCH: Follower进程向准 Leader发送自己处理过的最后一个事务 Proposal的 epoch值。
NEWEPOCH:准 Leader进程根据接收的各进程的 epoch,来生成新一轮周期的 epoch值。
ack-e: Follower进程反馈谁 Leader进程发来的 NEWEPOCH消息。
NEWLEADER:准 Leader进程确立自己的领导位,并发送 NEWLEADER消息给各进程。
ack-ld: Follower进程反馈 Leader进程发来的 NEWLEADER消息。
4.1初识 ZooKeeper75

<==========================86end ==============================>
<==========================87start==============================>

COMMIT--ld:要求 Follower进程提交相应的历史事务 Proposal
PROPOSE: Leader进程生成一个针对客户端事务请求的 Proposal
ACK: Follower进程反馈 Leader进程发来的 PROPOSAL消息。
COMMIT: Leader发送 COMMIT消息,要求所有进程提交事务 PROPOSE。
在正常运行过程中,ZAB协议会一直运行于阶段三来反复地进行消息广播流程。如果出
现 ELeader崩溃或其他原因导致 Leader缺失,那么此时ZAB协议会再次进入阶段一,重
新选举新的 Leader
发现
同步
广播
Follower
CEPOCHNEWEPOCH
LACK-LD
PROPOSECOMMIT
ACK-E
CK
NEWLEADER
Leader
ACK-E
CEPOCH
NEWLEADER/ COMSNT-LD
COMMIT
NEWEPOCH
ACK-LD
PROPOSE AEK
Follower
图4-5.ZAB协议算法描述示意图
运行分析
在ZAB协议的设计中,每一个进程都有可能处于以下三种状态之一
LOOKING: Leader选举阶段
● FOLLOWING: Follower服务器和 Leader保持同步状态
LEADING: Leader服务器作为主进程领导状态
组成ZAB协议的所有进程启动的时候,其初始化状态都是 LOOKING状态,此时进程
组中不存在 Leader。所有处于这种状态的进程,都会试图去选举出一个新的 Leader随
后,如果进程发现已经选举出新的 Leader了,那么它就会马上切换到 FOLLOWING状
态,并开始和Leader保持同步。这里,我们将处于 FOLLOWING状态的进程称为 Follower,
将处于 LEADING状态的进程称为 Leader考虑到 Leader进程随时会挂掉,当检测出
76第4章 ZooKeeper与 Paxos

<==========================87end ==============================>
<==========================88start==============================>

Leader已经崩溃或者是放弃了领导地位时,其余的 Follower进程就会转换到 LOOKING
状态,并开始进行新一轮的 Leader选举。因此在ZAB协议运行过程中,每个进程都会
LEADING、 FOLLOWING和 LOOKING状态之间不断地转换。
Leader的选举过程发生在前面两个阶段图4-5展示了在一次 Leader选举过程中,各进
程之间的消息发送与接收情况。需要注意的是,只有在完成了阶段二,即完成各进程之
间的数据同步之后,准 Leader进程才能真正成为新的主进程周期中的 Leader具体的,
我们将一个可用的 Leader定义如下:
如果一个准 Leader Le接收到来自过半的 Follower进程针对Le的
(e,)反馈消息,那么L就成为了周期e的 Leader
完成 Leader选举以及数据同步之后,ZAB协议就进入了原子广播阶段。在这一阶段中,
Leader会以队列的形式为每一个与自己保持同步的 Follower创建一个操作队列。同一时
刻,一个 Follower只能和一个 Leader保持同步, Leader进程与所有的 Follower进程之
间都通过心跳检测机制来感知彼此的情况。如果 Leader能够在超时时间内正常收到心跳
检测,那么 Follower就会一直与该 Leader保持连接。而如果在指定的超时时间内 Leader
无法从过半的 Follower进程那里接收到心跳检测,或者是TCP连接本身断开了,那么
Leader就会终止对当前周期的领导,并转换到 LOOKING状态,所有的 Follower也会选
择放弃这个 Leader,同时转换到 LOOKING状态。之后,所有进程就会开始新一轮的
Leader选举,并在选举产生新的 Leader之后开始新一轮的主进程周期。
4.2.4AB与 Paxos算法的联系与区别
AB协议并 Paxos不是算法的一个典型实现,在讲解ZAB和 Paxos之间的区别之前,
我们首先来看下两者的联系。
两者都存在一个类似于 Leader进程的角色由其负责协调多个 Follower进程的运
行。
Leader进程都会等待超过半数的 Follower做出正确的反馈后,才会将一个提案进行
提交。
·在ZAB协议中,每个 Proposal中都包含了一个 epoch值,用来代表当前的 Leader
周期,在 Paxos算法中,同样存在这样的一个标识,只是名字变成了 Ballot
在 Paxos算法中,一个新选举产生的主进程会进行两个阶段的工作。第一阶段被称为读
阶段,在这个阶段中,这个新的主进程会通过和所有其他进程进行通信的方式来收集上
一个主进程提出的提案,并将它们提交。第二阶段被称为写阶段,在这个阶段,当前主
4.1初识 ZooKeeper77

<==========================88end ==============================>
<==========================89start==============================>

进程开始提出它自己的提案。在 Paxos算法设计的基础上,ZAB协议额外添加了一个同
步阶段。在同步阶段之前,ZAB协议也存在一个和 Paxos算法中的读阶段非常类似的过
程,称为发现(Discovery)阶段。在同步阶段中新的 Leader会确保存在过半的 Follower
已经提交了之前 Leader周期中的所有事务 Proposal这一同步阶段的引入,能够有效地
保证 Leader在新的周期中提出事务 Proposal前,所有的进程都已经完成了对之前所
有事务 Proposal的提交。一旦完成同步阶段后,那么ZAB就会执行和 Paxos算法类似
的写阶段。
总的来讲,ZAB协议和 Paxos算法的本质区别在于,两者的设计目标不太一样。ZAB
协议主要用于构建一个高可用的分布式数据主备系统,例如 ZooKeeper,而 Paxos算法
则是用于构建一个分布式的一致性状态机系统。
小结
本章从介绍 ZooKeeper开始,向读者初步讲解了 ZooKeeper的设计目标、由来以及基本
概念。同时,本章也详细地介绍了 ZooKeeper的一性协议B,并将其与 Paxos
算法进行了对比。对于 ZooKeeper中AB协议的具体实现,将会在78节中进行更为详
细的讲解。
78第4章 ZooKeeper与 Paxos

<==========================89end ==============================>
<==========================90start==============================>

第5章
使用 ZooKeeper
好了,到现在为止,在学习了前面几章的内容后,相信你对 ZooKeeper已经有了一个基
本的认识了,那么,还等什么,让我们开始 ZooKeeper之旅吧。
5.1部署与运行
本节将着重介绍如何部署一个 ZooKeeper集群,以及如何将其顺利地运行起来。注意,
截止作者完成本章写作的时候, ZooKeeper的官方最新稳定版本是3.4.3,因此本章的讲
解都是针对这个版本进行的。尽管如此,由于 ZooKeeper各个版本之间在部署和运行方
式上的变化不大,因此本章的很多内容都适用于 ZooKeeper的其他版本。
5.1.1系统环境
对于大部分Java开源产品而言,在部署与运行之前,总是需要搭建一个合适的环境,通
常包括操作系统和Java环境两方面本节将 Zoo介绍部署与运行需要的系统环境,
同样包括操作系统和Java环境两部分。
操作系统
首先,你需要选择一个合适的操作系统。幸运的是, ZooKeeper对于不同平台的支持都
很好,在现在绝大多数主流的操作系统上都能够正常运行,例如nu U/Linux Sun Solaris
Win32以及 MacOSX等。需要注意的是, ZooKeeper官方文档中特别强调,由于 FreeBSD
系统的JM对Java的 NIO Selector支持得不是很好,所以不建议在该系统上部署生产
环境的 ZooKeeper服务器。
79

<==========================90end ==============================>
<==========================91start==============================>

Java环境
ZooKeeper使用Java语言编写,因此它的运行环境需要Java环境的支持,可下载1.6或
以上版本的 Java Oracle Java http: //www. java.com/建议下载Oracle官方发布的Jav,下载地址是:htp:wjava.com
download/).
5.1.2集群与单机
ZooKeeper有两种运行模式:集群模式和单机模式。本节将分别对两种运行模式的安装
和配置进行简要讲解,具体的服务器配置将在8.1节中讲解。另外,如果没有特殊说明,
本节涉及的部署与配置操作都是针对GNU/Linux系统的。
集群模式
现在,我们开始讲解如何使用三台机器来搭建一个 ZooKeeper集群首先,我们假设已
经准备好三台互相联网的 Linux机器,它们的P地址分别为IP1、IP2和IP3
1.准备Java运行环境。确保你已经安装了Java1.6或更高版本的JDK1
2.下载 ZooKeeper安装包。
下载地址为:nttp: http: //zookeeper. apache. org/releases.html.,apache.org/eleases.html注意,用户可以选择
稳定版本( stable)进行下载,下载完成后会得到一个文件名类似于
zookeeper--x.x.x.targz的文件,解压到一个目录,例如opt tzookeeper--3.4.3/目
录下,同时我们约定,在下文中使用%ZK HOM%代表该目录,目录结构如
图5-1所示。
7582122012bu11d.x
2012 CHANEES. txt
cont
ib
:3s dist maven
docs.
1vysettings.
11:3816
LICEHSE.
12N
BEADME packaging. txt
recipes
12 zo0keeper
lar
jar.
图5-1. ZooKeeper目录结构
注:在作者所在公司,生产环境大规模使用的是Java1.6版本,因此还是建议读者尽量使用1.6版本
的JDK
80第5 ZooKeeper章使用

<==========================91end ==============================>
<==========================92start==============================>

3.配置文件z00.cfg
初次使用 ZooKeeper,需要将%ZKHOME%/onf目录下的 zoosample.cfg文件重
命名为zoo.cg,并且按照如下代码进行简单配置即可:
tickTime=2000
dataDir=/var/lib/zookeeper/
clientPort=2181
initLimit=5
syncLimit=2
server.1=IP1: 2888: 3888
server.2=ip2:2888:3888
server.3=ip3:2888:3888
关于 ZooKeeper的参数配置,将在8.1节中做详细讲解,这里只是简单地说几点。
·在集群模式下,集群中的每台机器都需要感知到整个集群是由哪几台机器组
成的,在配置文件中,可以按照这样的格式进行配置,每一行都代表一个机器
配置:
server.id=host:port:port
其中,id被称为 Server ID,用来标识该机器在集群中的机器序号。同时,在
每台 ZooKeeper机器上,我们都需要在数据目录(即dataDir参数指定的那
个目录)下创建一个myid文件,该文件只有一行内容,并且是一个数字,即
对应于每台机器的 Server ID数字。
在ZooKeeper的设计中,集群中所有机器上 zoocfg文件的内容都应该是一致
的。因此最好使用SVN或是GIT把此文件管理起来,确保每个机器都能共享
到一份相同的配置。
上面也提到了,myid文件中只有一个数,即一个 Server ID.例如, server.1
的myid文件内容就是“1”。注意,请确保每个服务器的myid文件中的数字
不同,并且和自己所在机器的z0cg中 server.idhost:port:port的
id值一致。另外,id的范围是1~255
4.创建myid文件。
在 dataDir所配置的目录下,创建一个名为myid的文件,在该文件的第一行写
上一个数字,和zoo.cg中当前机器的编号对应上。
5.按照相同的步骤,为其他机器都配置上zo.cg和myid文件。
6.启动服务器。
1部署与运行81

<==========================92end ==============================>
<==========================93start==============================>

至此,所有的选项都已经基本配置完毕,可以使用%ZKHOME%bin目录下的
zkServersh脚本进行服务器的启动,如下:
$sh zkServer. sh start
JMX enabled by default
Using config: /opt/zookeeper-3. 4.3/bin/../conf/zoo.cfg
Starting zookeeper .. START
ED
7.验证服务器。
启动完成后,可以使用如下命令来检查服务器启动是否正常:
$ telnet127.0.0.12181
Trying127.0.0.1
Connected to localhost. localdomain (127.0.0.1).
Escape character is '^]'
stat
Zookeeper version: 3.4.3-1240972 built on 02/06/2012 10: 48 GMT
clients:
/127.0.0.1:50257[0](queued=0, recved=l,sent=0)
Latency min/avg/max: 0/1/4489
Received:844689
sent:993100
Outstanding:0
Zxid:0x600084344
Mode: leader
Node count:37
上面就是通过 Telnet方式,使用stat命令进行服务器启动的验证,如果出现和
上面类似的输出信息,就说明服务器已经正常启动了。
单机模式
在上文的集群模式中,我们已经完成了一个ZooKeeper集群的搭建了一般情况下,在
开发测试环境,我们没有那么多机器资源,而且平时的开发调试并不需要极好的稳定性。
幸运的是, ZooKeeper支持单机部署,只要启动一台 ZooKeeper机器,就可以提供正常
服务了。
其实,单机模式只是一种特殊的集群模式而已一只有一台机器的集群,认识到这点后,
对下文的理解就会轻松不少了。单机模式的部署步骤和集群模式的部署步骤基本一致,
只是在z.cg文件的配置上有些差异。由于现在我们是单机模式,整个 ZooKeeper集
群中只有一台机器,所以需要对zoo.cfg做如下修改:
tickTime=2000
82第5章使用 ZooKeeper

<==========================93end ==============================>
<==========================94start==============================>

dataDir=/var/lib/zookeeper/
clientPort=2181
initlimit=5
syncLimit=2
server.1=ip1:2888:3888
和集群模式唯一的区别就在机器列表上在单机模式的zoo.cg文件中,只有 server.1
这一项。修改完这个文件后,就可以启动服务器了。同样,验证服务器启动情况,然后
得到如下的输出信息:
telnet127.0.0.12181
Trying127.0.0.1
Connected to 127.0.0.1.
Escape character is ']
stat
Zookeeper version: 3.4.3-1240972, built on 02/06/2012 10: 48 GMT
Clients:
/127.0.0.1:44801[0](queued=0, recved=,sent=0)
Latency min/avg/max: 0/0/0
Received:2
Sent:1
Outstanding: 0
Zxid:0x0
Mode:standalone
Node count:4
Connection closed by foreign host.
细心的读者会发现,集群模式和单机模式下输出的服务器验证信息基本一致,只有Mode
属性不一样。在集群模式中,Mode显示的是 leader,其实还有可能是 follower对于
Leader和 Follower角色的概念,在4.1.3节中已经做了详细介绍,它们用来标识当前服
务器在集群中的角色。而在单机模式中,Mode显示的是 standalone,相信读者也不难理
解这个标识,在这里就不再赘述了。
伪集群模式
在上文中,在集群和单机两种模式下,我们基本完成了分别针对生产环境和开发环境
ZooKeeper服务的搭建,已经可以满足绝大多数场景了。
现在我们再来看看另外一种情况,如果你手上有且只有一台比较好的机器,那么这个
时候,如果作为单机模式进行部署,资源明显有点浪费;而如果想要按照集群模式来部
署的话,那么就需要借助硬件上的虚拟化技术,把一台物理机器转换成几台虚拟机,不
注2:通常,我们认为比较好的机器大体是CPU核数大于10,内存大于等于8GB
5.1部署与运行83

<==========================94end ==============================>
<==========================95start==============================>

过这样操作成本太高。所幸,和其他分布式系统(如 Hadoop)一样, ZooKeeper也允许
你在一台机器上完成一个伪集群的搭建。
所谓的伪集群,用一句话说就是,集群所有的机器都在一台机器上,但是还是以集群的
特性来对外提供服务。这种模式和集群模式非常类似,只是把zoo.cfg做了如下修改:
tickTime=2000
dataDir=/var/lib/zookeeper/
clientPort=2181
initLimit=5
syncLimit=2
server.1=ip1:2888:3888
server.2=ip1:2889:3889
server.3=ip1:2890:3890
zoo.cfg配置中,每一行的机器列表配置都是同一个地址:I,但是后面的端口配
置都已经不一样了。这其实不难理解,在同一台机器上启动多个进程,就必须绑定不同
的端口。关于这几个端口的具体说明,将在8.1节中做详细讲解。
5.1.3运行服务
在5.1.2节中,我们主要讲解了如何搭建各种运行模式的 ZooKeeper集群。本节将会重
点讲解如何启动与停止 ZooKeeper服务,同时也会向读者介绍如何解决在 ZooKeeper服
务启动阶段出现的一些常见的异常问题。
启动服务
首先我们来看下如何启动 ZooKeeper服务。常见的启动方式有两种。
Java命令行
这是Java语言中通常使用的方式。使用Jaa命令来运行JAR包,具体方法是在
ZooKeeper3.4.3发行版本的%ZKHOME目录下执行如下命令:
sjava-cp zookeeper -3.4.3. jar: lib slf4j-api-1.6. 1. jar: lib/slf4j-Log4j12-
1.6.1.jar: lib/log4j-1.2.15.jar: conf org. apache. zookeeper. server. quorum.
QuorumPeerMain conf/zoo.cfg
通过运行上面这个命令, ZooKeeper的主 QuorumPeerMain口类就会启动
ZooKeeperZooKeeper服务器,同时,随着服务的启动,其内部的JMX也会被启
动,方便管理员在JMX管理控制台上进行一些对 ZooKeeper的监控与操作。关于
ZooKeeper的JMX管理,将在8.3节中做详细讲解。
84第5章使用 ZooKeeper

<==========================95end ==============================>
<==========================96start==============================>

注意,不同的ZooKeeper发行版本,依赖的lg4j和slf4j版本是不一样的
请读者务必看清楚自己的版本后,再执行上面这个命令。
使用 ZooKeeper自带的启动脚本来启动 Zookeeper
在 ZooKeeper的%ZKHOME%bin目录下有几个有用的脚本,如图5-2所示,可以
用这些脚本来启动与停止 ZooKeeper服务。这个目录下的所有文件都有两种文件格
式:sh和.cmd,分别适用于UNX系统和 Windows系统。
2382月62012 REACME.txt
190926 2012 kclearwp.sh
1022012kci.sh
2599262012 zkErV.sh
1084 26 2012 zkserver. a
5301 26 2012 zkServer. sh
图5-2. ZooKeeper bin目录下文件列表
表5-1中列举了这些脚本文件及其简单说明。注意,表5-1的“脚本”一列中,并没有
包含每个脚本的文件后缀(例如,表5-1中注明了 zkCleanup而不是 zkCleanup.sh)因
为尽管后缀不同,但是它们在各自的操作系统上的作用与用法是一致的。
表5-1. ZooKeeper可执行脚本
脚本
说明
zkCleanup
清理 ZooKeeper历史数据,包括事务日志文件和快照数据文件
zkCli
ZooKeeper的一个简易客户端
zkEmv
设置 ZooKeeper的环境变量
zkServer
ZooKeeper服务器的启动停止和重启脚本
停止服务
停止 ZooKeeper服务最常用的方法就是使用上面介绍的 zkServer脚本的stop命令来完
成,如下:
$ sh zkServer.sh stop
JMX enabled by default
Using config: /opt/zookeeper-3.4./bin/../conf/zoo.cfg
Stopping zookeeper.. STOPPED
执行上面的脚本,就能够停止 ZooKeeper服务了。
注3:在 ZooKeeper3 3.3.5和3.3.6两个发行版本中,只依赖了logj-1.2.15而在 ZooKeeper3 3.4.2、3.4.3
3.4.4、3.4.5版本中,依赖了log4j-1.2.15slfj-api-1.6.1和slf4j-log4j12-1.6.1
5.1部署与运行85

<==========================96end ==============================>
<==========================97start==============================>

常见异常
在启动的时候,通常会碰到一些异常,下面将对这些常见的异常进行讲解。
端口被占用
在启动 ZooKeeper的时候,可能出现如下“端口被占用”的异常,导致服务器无法
正常启动:
java. net. BindException: Address already in use
at sun. nio. ch. Net. bind(Native Method)
at sun. nio. ch. ServerSocketChannelImpl bind(ServerSocket ChannelImpl. java: 126)
at sun. nio. ch. ServerSocketAdaptor. bind (ServerSocketAdaptor.java: 59)
at sun. nio. ch. ServerSocketAdaptor. bind(ServerSocketAdaptor.java: 52)
at o.a... NIOServerCnxnFactory. configure(NIOServerCnxnFactory. java: 111)
ato.az.s. ZooKeeperServerMain. runFromConf( ZooKeeperServerMain.java:110)
serverMain initializeAndRun (ZooKeeperServerMain.java: 86)
at o.a.z.s. ZooKeeperServerMai
at o.a. z.s. ZooKeeperServerMain. main(ZooKeeperServerMain. java: 52)
at o.a.z. s. quorum. QuorumPeerMain. initializeAndRun(QuorumPeerMain. java: 116)
at
org. apache. zookeeper. server. quorum. QuorumPeerl.main(QuorumPeerMain.java:7
8)
java.net. BindException: Address already in use这个异常是java
程序员最熟悉的异常之一,导致这个异常的原因通常是因为2181端口已经被其他
进程占用了。通常的做法就是检查当前机器上哪个进程正在占用这个端口,确认其
端口占用的必要性,将该进程停止后,再一次启动 ZooKeeper即可。也可以编
辑%ZKHOME%confzoo.cfg文件,更换 ZooKeeper的 clientPort配置,例如,
可以将其设置为2080:
dataDir=/var/lib/zookeeper/
clientPort=2080
initLimit=5
磁盘没有剩余空间
无论是在 ZooKeeper启动还是正常运行过程中,都有可能出现如下“磁盘没有剩余
空间”的异常,一旦遇到这个异常, ZooKeeper会立即执行 Failover策略,从而退
出进程:
java. io. IOException: No space left on device
at java.io. FileOutputStream. writeBytes (Native Method)
at java. io. FileOutputstream. write(FileOutputStream. java: 260)
86第5 ZooKeeper章使用

<==========================97end ==============================>
<==========================98start==============================>

at java. io. BufferedOutputStream. flushBuffer(Buf feredOutputstream.java: 65)
at java. io. Buf feredOutputStream. flush(BufferedoutputStream.java: 123)
at
org. apache. zookeeper. server. persistence. FileTxnLog. commit(FileTxnLog. java: 30
9)
at o.a.z. s. persistence. FileTxnSnapLog commit(FileTxnSnapLog. java: 306)
at o.a. Z.. ZKDatabase. commit(ZKDatabase. java: 484)
at o.a.z. s. SyncRequestProcessor. flush(SyncRequestProcessor. java: 162)
ato.a.z.s. . SyncRequestProcessor.run(SyncRequestPro.java:101)
遇到这个问题,通常的做法就是清理磁盘当然,为了避免以后再次遇到此类磁盘
空间满的问题,需要加上对 ZooKeeper机器的磁盘使用量监控和 ZooKeeper日志的
自动清理。关于 ZooKeeper日志清理,将在8.6.1节中做详细讲解。
无法找到myid文件
在5.1.2节讲解如何部署一个ZooKeeper集群的基本步骤时,我们在步骤4中提到,
需要在数据目录下创建一个myid文件。这里说的“无法找到myid文件”就是因为
没有找到这个配置文件而导致的如下异常:
ERROR [main: QuorumPeerMain@85]- Invalid config, exiting abnormally
o.a.z.s. . quorum. QuorumPeerConfigsConfigEx: Error processing/home/
ttpproxy/yinshi. nc/zookeeper-34.3/bin/../conf/zoo. cfg
at o.a.z. s. quorum. QuorumPeerConfig. parse(QuorumPeerConfig. java: 121)
at o.a.Z.s. quorum. QuorumPeerMain. initializeAn(QuorumPeerMain. java: 101)
at o.a.z.s. quorum. QuorumPeerMain. main (QuorumPeerMain.java: 78)
Caused by: java. lang. IllegalArgumentException: /tmp/zookeeper/myid file is
missing
at o.a... quorum. QuorumPeerConfig. parseProperties(QuorumPeerConfig. java: 344)
at o.a. z.s. quorum. QuorumPeerConfig. parse(QuorumPeerConfig.java: 117)
.. 2 more
对于这个问题,只需在数据目录下创建好一个myid文件即可。
集群中其他机器 Leader选举端口未开
在集群模式部署下服务器逐台启动的过程中,会碰到类似于下面这样的异常:
WARN [QuorumPeer [myid=1]/: 0: 0: 0: 0:0:0:0: 2181:QuorumCnxManager@368]
Cannot open channel to 2 at election address/122.228.242.241:3888
java. net. SocketTimeoutException: connect timed out
at java. net. PlainSocketImpl. socketConnect(Native Method)
ectu
at java. net. PlainSocket Impl. doConnectPlainSocket Impl. java: 351)
at java.net. PlainSocket Impl. connectToAd(PlainSocketImpl.java:213)
at java. net. PlainSocketImpl. connect(PlainSocketImpl. java: 200)
at java. net. Socket. connect(Socket. java: 529) cKet
at java.net. SocksSocket Impl. connect(SocksSocket.java:366)
ava.
ct(s
5.1部署与运行87

<==========================98end ==============================>
<==========================99start==============================>

ato.a.z.s .. quorum. QuorumCnxManager. connectOneQuorumCnxManager.java:354)
at o.a.. s. quorum. QuorumCnxManager. connectAll(QuorumCnxManager.java: 388)
at o.a.z. s. quorum. FastLeaderElection. LookForLeader(FastLeaderElection.java:
755)
at o.a. z.s. quorum. QuorumPeer. run(QuorumPeer.java: 716)
INFO [QuorumPeer [myid=1]/0: 0: 0: 0: 00: 0: 0: 2181: FastLeaderElection@764]-
Notification time out:400
这是由于在启动过程中,虽然当前机器启动了,但其他机器还没有启动完成,因此
无法和其他机器在相应端口上进行连接。对于这个问题,只要快速启动集群中的其
他机器即可。另外,上面的异常中标明了是388这个端口无法创建连接,这是因
为 ZooKeeper默认使用3888端口进行 Leader选举过程中的投票通信,关于
ZooKeeper的 Leader选举,将在7.6节中做详细讲解。
5.2客户端脚本
现在,你已经搭建起了一个能够正常运行的 ZooKeeper集群了,接下来,我们开始来学
习如何使用客户端对 ZooKeeper进行操作。在表5-1中,我们已经列出了 ZooKeeper自
带的一些命令行工具,在本节,我们重点要看下 zkCli这个脚本。进入 ZooKeeper的bin
目录之后,直接执行如下命令:
s sh zkCli.sh
当看到如下输出信息时,表示已经成功连接上本地的 ZooKeeper服务器了:
WatchedEvent state: SyncConnected type: None path: null
[zk: Localhost: 2181(CONNECTED)0]
注意,上面的命令没有显式地指定 ZooKeeper服务器地址,那么默认是连接本地的
ZooKeeperZooKeepe服务器。如果希望连接指定的服务器,可以通过如下方式实现:
$ sh zkcli.sh-server ip:port
5.2.1创建
使用 create命令,可以创建一个ZooKeeper节点。用法如下:
create [-s] [-e] path data acl
其中,-s或-e分别指定节点特性:顺序或临时节点。默认情况下,即不添加-s或-e
参数的,创建的是持久节点。
执行如下命令:
88第5 ZooKeeper章使用

<==========================99end ==============================>